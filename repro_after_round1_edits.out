nohup: ignoring input
problem          : flying_inv_pend
reg_weight       : 150.0
reg_n_samples    : 250
critic_n_samples : 50
critic_max_n_steps : 20
test_N_volume_samples : 2500
test_N_boundary_samples : 2500
learner_stopping_condition : n_steps
learner_early_stopping_patience : 100
learner_n_steps  : 3000
learner_lr       : 0.001
random_seed      : 1
affix            : repro_after_round1_edits
log_root         : log
model_root       : checkpoint
n_checkpoint_step : 5
n_test_loss_step : 25
gpu              : 0
log_folder       : log/flying_inv_pend_repro_after_round1_edits
model_folder     : checkpoint/flying_inv_pend_repro_after_round1_edits
Reg grad norm: 0.423
total grad norm: 6.321

==================== evaluation at iteration: 0 ====================
train total loss: 95.123%
train max loss: 27.705%, reg loss: 75.574%
time spent training so far: 0:00:26.928259
train attack total time: 26.588s
train attack init time: 2.897s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.656s


train attack loss increase over inner max: 14.579
OOM debug. Mem allocated and reserved: 727552.000000, 2097152.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.680% of volume
percentage infeasible at boundary: 31.40%
mean, std amount infeasible at boundary: 2.63 +/- 5.34
max amount infeasible at boundary: 36.79

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.531
total grad norm: 7.073

==================== evaluation at iteration: 1 ====================
train total loss: 97.868%
train max loss: 35.178%, reg loss: 75.420%
time spent training so far: 0:03:18.968423
train attack total time: 16.707s
train attack init time: 3.016s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.354s


train attack loss increase over inner max: 4.080
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.546
total grad norm: 7.395

==================== evaluation at iteration: 2 ====================
train total loss: 96.530%
train max loss: 30.227%, reg loss: 75.553%
time spent training so far: 0:03:40.838101
train attack total time: 21.471s
train attack init time: 3.132s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.506s


train attack loss increase over inner max: -1.363
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.586
total grad norm: 5.284

==================== evaluation at iteration: 3 ====================
train total loss: 91.956%
train max loss: 26.282%, reg loss: 75.499%
time spent training so far: 0:04:03.256081
train attack total time: 22.060s
train attack init time: 2.991s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.530s


train attack loss increase over inner max: -0.551
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.590
total grad norm: 10.047

==================== evaluation at iteration: 4 ====================
train total loss: 89.201%
train max loss: 24.792%, reg loss: 75.138%
time spent training so far: 0:04:19.773115
train attack total time: 16.200s
train attack init time: 2.849s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.343s


train attack loss increase over inner max: -144.454
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.583
total grad norm: 6.461

==================== evaluation at iteration: 5 ====================
train total loss: 91.001%
train max loss: 24.095%, reg loss: 75.658%
time spent training so far: 0:04:38.180424
train attack total time: 18.067s
train attack init time: 2.770s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.407s


train attack loss increase over inner max: 3.498
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.602
total grad norm: 5.412

==================== evaluation at iteration: 6 ====================
train total loss: 88.917%
train max loss: 21.087%, reg loss: 75.400%
time spent training so far: 0:05:00.371978
train attack total time: 21.847s
train attack init time: 3.023s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.521s


train attack loss increase over inner max: -1.473
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.694
total grad norm: 5.529

==================== evaluation at iteration: 7 ====================
train total loss: 88.021%
train max loss: 22.650%, reg loss: 75.273%
time spent training so far: 0:05:14.647275
train attack total time: 13.884s
train attack init time: 2.611s
train attack avg grad step time: 0.081s
train attack avg reproj time: 0.280s


train attack loss increase over inner max: 3.877
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.645
total grad norm: 5.673

==================== evaluation at iteration: 8 ====================
train total loss: 86.448%
train max loss: 20.647%, reg loss: 75.251%
time spent training so far: 0:05:30.209513
train attack total time: 15.200s
train attack init time: 2.968s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.321s


train attack loss increase over inner max: -0.393
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.706
total grad norm: 4.038

==================== evaluation at iteration: 9 ====================
train total loss: 88.860%
train max loss: 21.069%, reg loss: 75.413%
time spent training so far: 0:05:44.022066
train attack total time: 13.429s
train attack init time: 2.835s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.265s


train attack loss increase over inner max: 0.978
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.723
total grad norm: 5.688

==================== evaluation at iteration: 10 ====================
train total loss: 86.209%
train max loss: 20.196%, reg loss: 75.219%
time spent training so far: 0:05:57.927199
train attack total time: 13.495s
train attack init time: 2.492s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.281s


train attack loss increase over inner max: -1.276
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.775
total grad norm: 5.300

==================== evaluation at iteration: 11 ====================
train total loss: 86.266%
train max loss: 19.387%, reg loss: 75.379%
time spent training so far: 0:06:08.555536
train attack total time: 10.283s
train attack init time: 2.331s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.181s


train attack loss increase over inner max: -0.570
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.702
total grad norm: 6.653

==================== evaluation at iteration: 12 ====================
train total loss: 87.386%
train max loss: 19.507%, reg loss: 75.450%
time spent training so far: 0:06:24.002265
train attack total time: 15.054s
train attack init time: 2.464s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.335s


train attack loss increase over inner max: -0.299
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.952
total grad norm: 6.854

==================== evaluation at iteration: 13 ====================
train total loss: 88.304%
train max loss: 22.238%, reg loss: 75.380%
time spent training so far: 0:06:39.356976
train attack total time: 15.014s
train attack init time: 2.230s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.341s


train attack loss increase over inner max: 2.916
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.008
total grad norm: 10.503

==================== evaluation at iteration: 14 ====================
train total loss: 87.932%
train max loss: 21.576%, reg loss: 75.408%
time spent training so far: 0:06:54.740072
train attack total time: 15.043s
train attack init time: 2.719s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: -0.272
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.737
total grad norm: 9.377

==================== evaluation at iteration: 15 ====================
train total loss: 87.474%
train max loss: 21.218%, reg loss: 75.255%
time spent training so far: 0:07:06.686778
train attack total time: 11.600s
train attack init time: 2.213s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.227s


train attack loss increase over inner max: -0.275
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.803
total grad norm: 12.688

==================== evaluation at iteration: 16 ====================
train total loss: 88.902%
train max loss: 20.127%, reg loss: 75.347%
time spent training so far: 0:07:21.258964
train attack total time: 14.225s
train attack init time: 2.524s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.304s


train attack loss increase over inner max: -0.678
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.663
total grad norm: 6.272

==================== evaluation at iteration: 17 ====================
train total loss: 86.912%
train max loss: 21.365%, reg loss: 75.277%
time spent training so far: 0:07:34.621116
train attack total time: 12.975s
train attack init time: 2.706s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.269s


train attack loss increase over inner max: 0.809
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.667
total grad norm: 7.172

==================== evaluation at iteration: 18 ====================
train total loss: 87.260%
train max loss: 23.249%, reg loss: 75.257%
time spent training so far: 0:07:56.980847
train attack total time: 21.990s
train attack init time: 2.839s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.572s


train attack loss increase over inner max: 1.341
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.592
total grad norm: 6.587

==================== evaluation at iteration: 19 ====================
train total loss: 87.494%
train max loss: 23.954%, reg loss: 75.342%
time spent training so far: 0:08:07.012430
train attack total time: 9.705s
train attack init time: 2.459s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.164s


train attack loss increase over inner max: 2.440
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.586
total grad norm: 10.183

==================== evaluation at iteration: 20 ====================
train total loss: 87.617%
train max loss: 25.611%, reg loss: 75.276%
time spent training so far: 0:08:30.496629
train attack total time: 23.139s
train attack init time: 2.808s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.615s


train attack loss increase over inner max: 2.340
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.530
total grad norm: 7.385

==================== evaluation at iteration: 21 ====================
train total loss: 88.407%
train max loss: 26.318%, reg loss: 75.291%
time spent training so far: 0:08:43.092739
train attack total time: 12.219s
train attack init time: 2.337s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.254s


train attack loss increase over inner max: 2.432
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.411
total grad norm: 7.216

==================== evaluation at iteration: 22 ====================
train total loss: 89.210%
train max loss: 24.659%, reg loss: 75.235%
time spent training so far: 0:08:57.133702
train attack total time: 13.676s
train attack init time: 2.543s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.296s


train attack loss increase over inner max: 0.144
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.447
total grad norm: 5.608

==================== evaluation at iteration: 23 ====================
train total loss: 87.742%
train max loss: 22.391%, reg loss: 75.390%
time spent training so far: 0:09:14.880297
train attack total time: 17.405s
train attack init time: 2.806s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.417s


train attack loss increase over inner max: -48.520
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.376
total grad norm: 6.747

==================== evaluation at iteration: 24 ====================
train total loss: 88.366%
train max loss: 24.428%, reg loss: 75.292%
time spent training so far: 0:09:29.465861
train attack total time: 14.249s
train attack init time: 2.400s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.321s


train attack loss increase over inner max: 2.524
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.387
total grad norm: 5.806

==================== evaluation at iteration: 25 ====================
train total loss: 87.873%
train max loss: 22.538%, reg loss: 75.252%
time spent training so far: 0:09:40.631951
train attack total time: 10.827s
train attack init time: 2.795s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.188s


train attack loss increase over inner max: -1.572
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 5.200% of volume
percentage infeasible at boundary: 28.88%
mean, std amount infeasible at boundary: 1.46 +/- 3.14
max amount infeasible at boundary: 42.19

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.287
total grad norm: 3.512

==================== evaluation at iteration: 26 ====================
train total loss: 86.334%
train max loss: 21.186%, reg loss: 75.042%
time spent training so far: 0:12:05.918000
train attack total time: 16.171s
train attack init time: 2.450s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.385s


train attack loss increase over inner max: -0.550
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.286
total grad norm: 4.399

==================== evaluation at iteration: 27 ====================
train total loss: 86.679%
train max loss: 19.577%, reg loss: 75.365%
time spent training so far: 0:12:20.762565
train attack total time: 14.485s
train attack init time: 2.272s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.349s


train attack loss increase over inner max: -1.871
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.274
total grad norm: 4.909

==================== evaluation at iteration: 28 ====================
train total loss: 86.563%
train max loss: 18.769%, reg loss: 75.087%
time spent training so far: 0:12:35.548324
train attack total time: 14.421s
train attack init time: 2.199s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.349s


train attack loss increase over inner max: -0.953
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.219
total grad norm: 3.897

==================== evaluation at iteration: 29 ====================
train total loss: 87.481%
train max loss: 19.648%, reg loss: 75.120%
time spent training so far: 0:12:48.439550
train attack total time: 12.555s
train attack init time: 2.636s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: 0.901
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.176
total grad norm: 4.011

==================== evaluation at iteration: 30 ====================
train total loss: 87.660%
train max loss: 22.448%, reg loss: 74.962%
time spent training so far: 0:13:04.432798
train attack total time: 15.633s
train attack init time: 2.415s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.385s


train attack loss increase over inner max: 2.955
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.185
total grad norm: 2.947

==================== evaluation at iteration: 31 ====================
train total loss: 86.854%
train max loss: 22.097%, reg loss: 75.259%
time spent training so far: 0:13:18.256128
train attack total time: 13.464s
train attack init time: 2.550s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: -0.278
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.153
total grad norm: 3.766

==================== evaluation at iteration: 32 ====================
train total loss: 87.044%
train max loss: 21.445%, reg loss: 75.091%
time spent training so far: 0:13:31.184782
train attack total time: 12.587s
train attack init time: 2.327s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.278
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.157
total grad norm: 3.826

==================== evaluation at iteration: 33 ====================
train total loss: 88.526%
train max loss: 21.016%, reg loss: 75.068%
time spent training so far: 0:13:44.408565
train attack total time: 12.858s
train attack init time: 2.857s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.481
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.158
total grad norm: 3.847

==================== evaluation at iteration: 34 ====================
train total loss: 87.385%
train max loss: 20.795%, reg loss: 75.076%
time spent training so far: 0:13:56.406718
train attack total time: 11.610s
train attack init time: 2.698s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.231s


train attack loss increase over inner max: 0.016
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.193
total grad norm: 2.926

==================== evaluation at iteration: 35 ====================
train total loss: 86.942%
train max loss: 20.335%, reg loss: 75.063%
time spent training so far: 0:14:08.854254
train attack total time: 12.097s
train attack init time: 2.450s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: -0.184
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.175
total grad norm: 3.259

==================== evaluation at iteration: 36 ====================
train total loss: 86.063%
train max loss: 19.533%, reg loss: 74.988%
time spent training so far: 0:14:20.842594
train attack total time: 11.590s
train attack init time: 2.544s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.237s


train attack loss increase over inner max: -0.430
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.241
total grad norm: 4.673

==================== evaluation at iteration: 37 ====================
train total loss: 86.251%
train max loss: 19.211%, reg loss: 75.338%
time spent training so far: 0:14:32.262721
train attack total time: 11.077s
train attack init time: 2.610s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -0.425
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.251
total grad norm: 3.293

==================== evaluation at iteration: 38 ====================
train total loss: 86.594%
train max loss: 18.767%, reg loss: 75.182%
time spent training so far: 0:14:47.239946
train attack total time: 14.578s
train attack init time: 2.554s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.343s


train attack loss increase over inner max: -0.056
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.259
total grad norm: 4.939

==================== evaluation at iteration: 39 ====================
train total loss: 87.254%
train max loss: 21.311%, reg loss: 75.213%
time spent training so far: 0:14:59.802943
train attack total time: 12.178s
train attack init time: 2.424s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.275s


train attack loss increase over inner max: 2.483
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.311
total grad norm: 2.654

==================== evaluation at iteration: 40 ====================
train total loss: 85.270%
train max loss: 20.444%, reg loss: 75.257%
time spent training so far: 0:15:12.512505
train attack total time: 12.350s
train attack init time: 2.473s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.400
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.327
total grad norm: 3.889

==================== evaluation at iteration: 41 ====================
train total loss: 87.574%
train max loss: 19.580%, reg loss: 75.283%
time spent training so far: 0:15:28.180932
train attack total time: 15.301s
train attack init time: 2.481s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.390s


train attack loss increase over inner max: -0.550
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.354
total grad norm: 7.720

==================== evaluation at iteration: 42 ====================
train total loss: 85.095%
train max loss: 18.528%, reg loss: 75.147%
time spent training so far: 0:15:39.405754
train attack total time: 10.861s
train attack init time: 2.355s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.231s


train attack loss increase over inner max: -0.735
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.354
total grad norm: 4.394

==================== evaluation at iteration: 43 ====================
train total loss: 86.666%
train max loss: 22.874%, reg loss: 75.153%
time spent training so far: 0:15:50.607264
train attack total time: 10.849s
train attack init time: 2.326s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.230s


train attack loss increase over inner max: 4.907
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
At initialization: k0 is 0.002793
Max_n_steps: 30
Parameter containing:
tensor([[0.0038]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0066]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 29
Parameter containing:
tensor([[0.0048]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0056]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0058]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0046]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0068]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0036]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Attack: reprojection exited on timeout, max dist from =0 boundary:  3.6879425048828125
Parameter containing:
tensor([[0.0078]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0026]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0088]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0016]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 29
Parameter containing:
tensor([[0.0098]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0006]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0108]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0118]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0128]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0138]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 28
Parameter containing:
tensor([[0.0148]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0157]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0168]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0178]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0188]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 28
Parameter containing:
tensor([[0.0198]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0209]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0219]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0229]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0239]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 27
Parameter containing:
tensor([[0.0248]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0257]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Attack: reprojection exited on timeout, max dist from =0 boundary:  3.576791763305664
Parameter containing:
tensor([[0.0267]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0275]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0284]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 27
Parameter containing:
tensor([[0.0292]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0300]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0307]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0314]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0321]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 26
Parameter containing:
tensor([[0.0328]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0334]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0340]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0345]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0350]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 26
Parameter containing:
tensor([[0.0355]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0360]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0365]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0369]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0373]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 25
Parameter containing:
tensor([[0.0377]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0382]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0386]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.251
total grad norm: 5.458

==================== evaluation at iteration: 44 ====================
train total loss: 86.114%
train max loss: 22.346%, reg loss: 75.073%
time spent training so far: 0:16:04.519015
train attack total time: 13.582s
train attack init time: 2.331s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: -0.992
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.355
total grad norm: 4.315

==================== evaluation at iteration: 45 ====================
train total loss: 85.752%
train max loss: 20.906%, reg loss: 75.088%
time spent training so far: 0:16:17.776688
train attack total time: 12.878s
train attack init time: 2.494s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.295s


train attack loss increase over inner max: -1.323
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.253
total grad norm: 5.949

==================== evaluation at iteration: 46 ====================
train total loss: 87.168%
train max loss: 19.681%, reg loss: 75.056%
time spent training so far: 0:16:32.615146
train attack total time: 14.454s
train attack init time: 2.432s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.359s


train attack loss increase over inner max: -1.455
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.274
total grad norm: 2.407

==================== evaluation at iteration: 47 ====================
train total loss: 85.333%
train max loss: 20.691%, reg loss: 75.102%
time spent training so far: 0:16:47.645925
train attack total time: 14.638s
train attack init time: 2.476s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.363s


train attack loss increase over inner max: 1.164
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.275
total grad norm: 2.362

==================== evaluation at iteration: 48 ====================
train total loss: 86.569%
train max loss: 21.298%, reg loss: 75.194%
time spent training so far: 0:16:59.620104
train attack total time: 11.610s
train attack init time: 2.441s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.254s


train attack loss increase over inner max: 0.295
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.257
total grad norm: 3.086

==================== evaluation at iteration: 49 ====================
train total loss: 85.829%
train max loss: 20.846%, reg loss: 75.113%
time spent training so far: 0:17:09.402219
train attack total time: 9.453s
train attack init time: 2.122s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.185s


train attack loss increase over inner max: -0.290
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.290
total grad norm: 4.132

==================== evaluation at iteration: 50 ====================
train total loss: 86.414%
train max loss: 19.537%, reg loss: 75.175%
time spent training so far: 0:17:25.239987
train attack total time: 15.502s
train attack init time: 2.533s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.395s


train attack loss increase over inner max: -1.590
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 6.360% of volume
percentage infeasible at boundary: 26.88%
mean, std amount infeasible at boundary: 1.29 +/- 2.91
max amount infeasible at boundary: 26.75

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.281
total grad norm: 6.248

==================== evaluation at iteration: 51 ====================
train total loss: 86.625%
train max loss: 20.878%, reg loss: 75.073%
time spent training so far: 0:19:47.592838
train attack total time: 16.750s
train attack init time: 2.642s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.432s


train attack loss increase over inner max: 1.160
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.321
total grad norm: 4.101

==================== evaluation at iteration: 52 ====================
train total loss: 87.526%
train max loss: 20.732%, reg loss: 75.022%
time spent training so far: 0:20:01.639010
train attack total time: 13.633s
train attack init time: 2.594s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.337s


train attack loss increase over inner max: -0.436
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.295
total grad norm: 5.807

==================== evaluation at iteration: 53 ====================
train total loss: 87.487%
train max loss: 20.396%, reg loss: 74.971%
time spent training so far: 0:20:14.592466
train attack total time: 12.587s
train attack init time: 2.901s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.285s


train attack loss increase over inner max: -0.562
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.294
total grad norm: 6.760

==================== evaluation at iteration: 54 ====================
train total loss: 89.026%
train max loss: 25.423%, reg loss: 75.186%
time spent training so far: 0:20:30.071483
train attack total time: 15.080s
train attack init time: 2.495s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.397s


train attack loss increase over inner max: 4.845
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.316
total grad norm: 3.496

==================== evaluation at iteration: 55 ====================
train total loss: 88.131%
train max loss: 25.320%, reg loss: 75.120%
time spent training so far: 0:20:41.603473
train attack total time: 11.192s
train attack init time: 2.213s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: -0.009
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.325
total grad norm: 3.514

==================== evaluation at iteration: 56 ====================
train total loss: 88.635%
train max loss: 24.781%, reg loss: 75.135%
time spent training so far: 0:20:53.933287
train attack total time: 11.990s
train attack init time: 2.556s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.275s


train attack loss increase over inner max: -0.151
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.302
total grad norm: 2.571

==================== evaluation at iteration: 57 ====================
train total loss: 87.489%
train max loss: 24.433%, reg loss: 75.361%
time spent training so far: 0:21:06.231464
train attack total time: 11.943s
train attack init time: 2.316s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.285s


train attack loss increase over inner max: -0.464
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.353
total grad norm: 3.989

==================== evaluation at iteration: 58 ====================
train total loss: 88.105%
train max loss: 23.953%, reg loss: 75.121%
time spent training so far: 0:21:20.046247
train attack total time: 13.443s
train attack init time: 2.318s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.340s


train attack loss increase over inner max: -0.741
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.342
total grad norm: 2.438

==================== evaluation at iteration: 59 ====================
train total loss: 87.710%
train max loss: 22.741%, reg loss: 75.415%
time spent training so far: 0:21:31.315377
train attack total time: 10.943s
train attack init time: 2.014s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.257s


train attack loss increase over inner max: -1.263
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.323
total grad norm: 5.674

==================== evaluation at iteration: 60 ====================
train total loss: 86.548%
train max loss: 21.067%, reg loss: 75.196%
time spent training so far: 0:21:46.106268
train attack total time: 14.440s
train attack init time: 2.598s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.367s


train attack loss increase over inner max: -2.047
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.338
total grad norm: 5.265

==================== evaluation at iteration: 61 ====================
train total loss: 86.446%
train max loss: 19.691%, reg loss: 75.337%
time spent training so far: 0:22:02.727838
train attack total time: 16.241s
train attack init time: 2.843s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.424s


train attack loss increase over inner max: -1.326
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.219
total grad norm: 2.554

==================== evaluation at iteration: 62 ====================
train total loss: 85.583%
train max loss: 19.651%, reg loss: 75.144%
time spent training so far: 0:22:16.754691
train attack total time: 13.675s
train attack init time: 2.955s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.325s


train attack loss increase over inner max: -0.183
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.230
total grad norm: 3.295

==================== evaluation at iteration: 63 ====================
train total loss: 85.473%
train max loss: 18.575%, reg loss: 75.268%
time spent training so far: 0:22:32.828720
train attack total time: 15.694s
train attack init time: 2.699s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.413s


train attack loss increase over inner max: -0.984
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.150
total grad norm: 3.247

==================== evaluation at iteration: 64 ====================
train total loss: 85.978%
train max loss: 18.116%, reg loss: 74.991%
time spent training so far: 0:22:46.511940
train attack total time: 13.344s
train attack init time: 2.410s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.333s


train attack loss increase over inner max: 0.032
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.158
total grad norm: 2.888

==================== evaluation at iteration: 65 ====================
train total loss: 85.866%
train max loss: 17.906%, reg loss: 75.092%
time spent training so far: 0:23:01.494124
train attack total time: 14.631s
train attack init time: 2.431s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.383s


train attack loss increase over inner max: -0.158
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.148
total grad norm: 2.957

==================== evaluation at iteration: 66 ====================
train total loss: 86.603%
train max loss: 19.717%, reg loss: 75.007%
time spent training so far: 0:23:16.005795
train attack total time: 14.162s
train attack init time: 2.217s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: 2.150
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.135
total grad norm: 4.710

==================== evaluation at iteration: 67 ====================
train total loss: 84.885%
train max loss: 19.029%, reg loss: 75.027%
time spent training so far: 0:23:28.249061
train attack total time: 11.902s
train attack init time: 2.137s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.288s


train attack loss increase over inner max: -0.391
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.135
total grad norm: 3.208

==================== evaluation at iteration: 68 ====================
train total loss: 85.171%
train max loss: 18.153%, reg loss: 75.116%
time spent training so far: 0:23:39.511643
train attack total time: 10.919s
train attack init time: 2.484s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.239s


train attack loss increase over inner max: -0.609
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.188
total grad norm: 4.637

==================== evaluation at iteration: 69 ====================
train total loss: 86.948%
train max loss: 20.894%, reg loss: 75.130%
time spent training so far: 0:23:58.499016
train attack total time: 18.654s
train attack init time: 2.165s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.572s


train attack loss increase over inner max: 2.989
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.188
total grad norm: 5.223

==================== evaluation at iteration: 70 ====================
train total loss: 87.166%
train max loss: 23.147%, reg loss: 75.054%
time spent training so far: 0:24:09.523063
train attack total time: 10.698s
train attack init time: 2.140s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.255s


train attack loss increase over inner max: 1.916
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.237
total grad norm: 5.893

==================== evaluation at iteration: 71 ====================
train total loss: 88.667%
train max loss: 23.237%, reg loss: 75.114%
time spent training so far: 0:24:23.936006
train attack total time: 14.021s
train attack init time: 2.233s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.385s


train attack loss increase over inner max: -0.025
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.241
total grad norm: 3.922

==================== evaluation at iteration: 72 ====================
train total loss: 87.652%
train max loss: 22.074%, reg loss: 75.158%
time spent training so far: 0:24:38.605457
train attack total time: 14.314s
train attack init time: 2.294s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: -0.213
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.270
total grad norm: 5.616

==================== evaluation at iteration: 73 ====================
train total loss: 86.984%
train max loss: 21.109%, reg loss: 74.987%
time spent training so far: 0:24:49.932236
train attack total time: 10.955s
train attack init time: 2.184s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: -0.992
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.214
total grad norm: 3.253

==================== evaluation at iteration: 74 ====================
train total loss: 87.043%
train max loss: 20.740%, reg loss: 75.002%
time spent training so far: 0:25:01.109202
train attack total time: 10.836s
train attack init time: 2.174s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: -0.551
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.224
total grad norm: 2.925

==================== evaluation at iteration: 75 ====================
train total loss: 86.525%
train max loss: 20.202%, reg loss: 75.100%
time spent training so far: 0:25:12.172200
train attack total time: 10.671s
train attack init time: 2.266s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.249s


train attack loss increase over inner max: -0.540
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.920% of volume
percentage infeasible at boundary: 25.52%
mean, std amount infeasible at boundary: 1.26 +/- 2.94
max amount infeasible at boundary: 23.41

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.287
total grad norm: 3.250

==================== evaluation at iteration: 76 ====================
train total loss: 87.456%
train max loss: 19.832%, reg loss: 74.995%
time spent training so far: 0:27:32.962834
train attack total time: 12.530s
train attack init time: 2.479s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.314s


train attack loss increase over inner max: -0.789
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.239
total grad norm: 3.942

==================== evaluation at iteration: 77 ====================
train total loss: 84.899%
train max loss: 18.976%, reg loss: 75.105%
time spent training so far: 0:27:44.973952
train attack total time: 11.658s
train attack init time: 2.689s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.827
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.247
total grad norm: 4.524

==================== evaluation at iteration: 78 ====================
train total loss: 85.018%
train max loss: 18.680%, reg loss: 75.061%
time spent training so far: 0:27:58.218599
train attack total time: 12.864s
train attack init time: 3.068s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: -0.583
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.237
total grad norm: 4.808

==================== evaluation at iteration: 79 ====================
train total loss: 87.356%
train max loss: 20.540%, reg loss: 75.071%
time spent training so far: 0:28:10.876819
train attack total time: 12.211s
train attack init time: 2.382s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.300s


train attack loss increase over inner max: 2.400
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.194
total grad norm: 2.660

==================== evaluation at iteration: 80 ====================
train total loss: 86.959%
train max loss: 22.725%, reg loss: 74.974%
time spent training so far: 0:28:24.238566
train attack total time: 12.926s
train attack init time: 2.439s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: 2.656
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.251
total grad norm: 4.517

==================== evaluation at iteration: 81 ====================
train total loss: 85.529%
train max loss: 22.042%, reg loss: 75.132%
time spent training so far: 0:28:35.820341
train attack total time: 11.194s
train attack init time: 2.510s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.260s


train attack loss increase over inner max: -0.297
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.192
total grad norm: 4.245

==================== evaluation at iteration: 82 ====================
train total loss: 87.300%
train max loss: 21.468%, reg loss: 75.255%
time spent training so far: 0:28:48.612358
train attack total time: 12.413s
train attack init time: 2.689s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.298s


train attack loss increase over inner max: -0.461
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.256
total grad norm: 4.755

==================== evaluation at iteration: 83 ====================
train total loss: 88.429%
train max loss: 20.937%, reg loss: 75.198%
time spent training so far: 0:29:01.421951
train attack total time: 12.419s
train attack init time: 2.723s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.299s


train attack loss increase over inner max: -0.113
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.246
total grad norm: 3.607

==================== evaluation at iteration: 84 ====================
train total loss: 86.144%
train max loss: 20.547%, reg loss: 75.078%
time spent training so far: 0:29:14.660892
train attack total time: 12.847s
train attack init time: 2.683s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.316s


train attack loss increase over inner max: -0.602
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.266
total grad norm: 4.078

==================== evaluation at iteration: 85 ====================
train total loss: 86.551%
train max loss: 20.048%, reg loss: 75.213%
time spent training so far: 0:29:26.451866
train attack total time: 11.409s
train attack init time: 2.384s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.271s


train attack loss increase over inner max: -0.403
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.288
total grad norm: 4.470

==================== evaluation at iteration: 86 ====================
train total loss: 87.211%
train max loss: 20.534%, reg loss: 75.220%
time spent training so far: 0:29:38.708904
train attack total time: 11.882s
train attack init time: 2.622s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: 0.771
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.327
total grad norm: 5.097

==================== evaluation at iteration: 87 ====================
train total loss: 88.216%
train max loss: 20.555%, reg loss: 75.270%
time spent training so far: 0:29:51.333679
train attack total time: 12.250s
train attack init time: 2.393s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.305s


train attack loss increase over inner max: -0.563
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000

Max_n_steps: 25
Parameter containing:
tensor([[0.0390]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0395]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 25
Parameter containing:
tensor([[0.0399]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0404]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0408]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0412]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.11706492304801941
Parameter containing:
tensor([[0.0416]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 25
Parameter containing:
tensor([[0.0420]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0423]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0427]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0431]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0435]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 24
Parameter containing:
tensor([[0.0438]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0441]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0444]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0447]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0449]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 24
Parameter containing:
tensor([[0.0452]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.1189926415681839
Parameter containing:
tensor([[0.0456]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0459]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0462]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0464]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 24
Parameter containing:
tensor([[0.0467]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0470]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0473]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0476]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0478]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Parameter containing:
tensor([[0.0481]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0484]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0486]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0489]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0492]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.14134429395198822
Parameter containing:
tensor([[0.0494]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0496]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.2101573795080185
Parameter containing:
tensor([[0.0499]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0501]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0503]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Parameter containing:
tensor([[0.0505]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0506]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0507]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0508]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0509]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Parameter containing:
tensor([[0.0510]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0511]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.353
total grad norm: 3.105

==================== evaluation at iteration: 88 ====================
train total loss: 87.095%
train max loss: 20.105%, reg loss: 75.092%
time spent training so far: 0:30:06.012284
train attack total time: 14.297s
train attack init time: 2.699s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: 0.140
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.345
total grad norm: 4.387

==================== evaluation at iteration: 89 ====================
train total loss: 86.245%
train max loss: 18.743%, reg loss: 75.154%
time spent training so far: 0:30:17.999934
train attack total time: 11.578s
train attack init time: 2.726s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: -0.691
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.290
total grad norm: 3.715

==================== evaluation at iteration: 90 ====================
train total loss: 85.137%
train max loss: 17.876%, reg loss: 75.074%
time spent training so far: 0:30:30.832258
train attack total time: 12.432s
train attack init time: 3.012s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.287s


train attack loss increase over inner max: -1.179
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.316
total grad norm: 5.308

==================== evaluation at iteration: 91 ====================
train total loss: 86.383%
train max loss: 19.279%, reg loss: 75.071%
time spent training so far: 0:30:41.608646
train attack total time: 10.431s
train attack init time: 2.342s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.251s


train attack loss increase over inner max: 1.071
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.270
total grad norm: 3.308

==================== evaluation at iteration: 92 ====================
train total loss: 86.993%
train max loss: 19.725%, reg loss: 75.062%
time spent training so far: 0:30:51.864165
train attack total time: 9.930s
train attack init time: 2.413s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.225s


train attack loss increase over inner max: 0.844
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.209
total grad norm: 3.588

==================== evaluation at iteration: 93 ====================
train total loss: 87.242%
train max loss: 19.850%, reg loss: 75.010%
time spent training so far: 0:31:04.836728
train attack total time: 12.643s
train attack init time: 2.236s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.347s


train attack loss increase over inner max: -0.245
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.205
total grad norm: 3.681

==================== evaluation at iteration: 94 ====================
train total loss: 85.882%
train max loss: 20.074%, reg loss: 75.082%
time spent training so far: 0:31:18.147766
train attack total time: 12.987s
train attack init time: 2.145s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.360s


train attack loss increase over inner max: 0.384
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.201
total grad norm: 4.145

==================== evaluation at iteration: 95 ====================
train total loss: 85.754%
train max loss: 19.321%, reg loss: 75.104%
time spent training so far: 0:31:29.914110
train attack total time: 11.410s
train attack init time: 2.307s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.292s


train attack loss increase over inner max: -0.501
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.124
total grad norm: 2.009

==================== evaluation at iteration: 96 ====================
train total loss: 87.182%
train max loss: 19.701%, reg loss: 75.018%
time spent training so far: 0:31:40.335681
train attack total time: 10.045s
train attack init time: 2.492s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.228s


train attack loss increase over inner max: 0.581
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.140
total grad norm: 2.370

==================== evaluation at iteration: 97 ====================
train total loss: 87.472%
train max loss: 19.676%, reg loss: 75.168%
time spent training so far: 0:31:50.871713
train attack total time: 10.182s
train attack init time: 2.454s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.234s


train attack loss increase over inner max: 0.378
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.113
total grad norm: 2.406

==================== evaluation at iteration: 98 ====================
train total loss: 85.183%
train max loss: 18.718%, reg loss: 74.850%
time spent training so far: 0:32:03.547526
train attack total time: 12.334s
train attack init time: 2.308s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.332s


train attack loss increase over inner max: -1.011
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.125
total grad norm: 2.524

==================== evaluation at iteration: 99 ====================
train total loss: 88.744%
train max loss: 26.456%, reg loss: 75.116%
time spent training so far: 0:32:14.109262
train attack total time: 10.206s
train attack init time: 2.349s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 5.554
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.139
total grad norm: 2.729

==================== evaluation at iteration: 100 ====================
train total loss: 87.013%
train max loss: 25.899%, reg loss: 74.995%
time spent training so far: 0:32:27.928162
train attack total time: 13.470s
train attack init time: 2.558s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.368s


train attack loss increase over inner max: -0.607
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 6.320% of volume
percentage infeasible at boundary: 26.72%
mean, std amount infeasible at boundary: 1.29 +/- 2.92
max amount infeasible at boundary: 26.53

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.168
total grad norm: 2.537

==================== evaluation at iteration: 101 ====================
train total loss: 88.247%
train max loss: 25.319%, reg loss: 75.290%
time spent training so far: 0:34:37.943185
train attack total time: 10.964s
train attack init time: 2.265s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.276s


train attack loss increase over inner max: -0.827
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.167
total grad norm: 2.953

==================== evaluation at iteration: 102 ====================
train total loss: 88.293%
train max loss: 24.178%, reg loss: 75.144%
time spent training so far: 0:34:52.333982
train attack total time: 14.042s
train attack init time: 2.529s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.392s


train attack loss increase over inner max: -1.308
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.229
total grad norm: 3.084

==================== evaluation at iteration: 103 ====================
train total loss: 87.833%
train max loss: 23.385%, reg loss: 75.084%
time spent training so far: 0:35:05.533479
train attack total time: 12.833s
train attack init time: 2.253s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.352s


train attack loss increase over inner max: -1.281
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.238
total grad norm: 3.371

==================== evaluation at iteration: 104 ====================
train total loss: 89.214%
train max loss: 22.802%, reg loss: 74.980%
time spent training so far: 0:35:18.083604
train attack total time: 12.191s
train attack init time: 2.269s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.327s


train attack loss increase over inner max: -0.731
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.255
total grad norm: 4.922

==================== evaluation at iteration: 105 ====================
train total loss: 87.977%
train max loss: 23.227%, reg loss: 75.161%
time spent training so far: 0:35:30.900861
train attack total time: 12.475s
train attack init time: 2.286s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.338s


train attack loss increase over inner max: 0.706
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.323
total grad norm: 3.257

==================== evaluation at iteration: 106 ====================
train total loss: 87.265%
train max loss: 22.836%, reg loss: 75.136%
time spent training so far: 0:35:40.915168
train attack total time: 9.646s
train attack init time: 2.163s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.225s


train attack loss increase over inner max: -0.120
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.356
total grad norm: 4.261

==================== evaluation at iteration: 107 ====================
train total loss: 87.919%
train max loss: 21.735%, reg loss: 75.074%
time spent training so far: 0:35:53.767382
train attack total time: 12.505s
train attack init time: 2.187s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.342s


train attack loss increase over inner max: -1.320
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.382
total grad norm: 4.060

==================== evaluation at iteration: 108 ====================
train total loss: 87.751%
train max loss: 20.174%, reg loss: 75.135%
time spent training so far: 0:36:04.781415
train attack total time: 10.667s
train attack init time: 2.440s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.256s


train attack loss increase over inner max: -0.791
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.445
total grad norm: 4.903

==================== evaluation at iteration: 109 ====================
train total loss: 87.227%
train max loss: 20.796%, reg loss: 75.117%
time spent training so far: 0:36:19.662697
train attack total time: 14.455s
train attack init time: 2.699s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.402s


train attack loss increase over inner max: 1.463
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.515
total grad norm: 5.390

==================== evaluation at iteration: 110 ====================
train total loss: 86.126%
train max loss: 20.185%, reg loss: 75.213%
time spent training so far: 0:36:29.383920
train attack total time: 9.349s
train attack init time: 2.167s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.212s


train attack loss increase over inner max: -0.089
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.625
total grad norm: 5.515

==================== evaluation at iteration: 111 ====================
train total loss: 88.157%
train max loss: 24.118%, reg loss: 75.185%
time spent training so far: 0:36:39.943827
train attack total time: 10.186s
train attack init time: 2.307s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.242s


train attack loss increase over inner max: 4.548
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.605
total grad norm: 4.700

==================== evaluation at iteration: 112 ====================
train total loss: 88.258%
train max loss: 23.560%, reg loss: 75.259%
time spent training so far: 0:36:51.498825
train attack total time: 11.186s
train attack init time: 2.162s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.289s


train attack loss increase over inner max: 0.018
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.731
total grad norm: 4.425

==================== evaluation at iteration: 113 ====================
train total loss: 89.726%
train max loss: 28.032%, reg loss: 75.271%
time spent training so far: 0:37:07.318938
train attack total time: 15.477s
train attack init time: 2.247s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.464s


train attack loss increase over inner max: 4.049
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.792
total grad norm: 19.004

==================== evaluation at iteration: 114 ====================
train total loss: 90.237%
train max loss: 26.793%, reg loss: 75.215%
time spent training so far: 0:37:19.127037
train attack total time: 11.466s
train attack init time: 2.433s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.290s


train attack loss increase over inner max: -1.340
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.834
total grad norm: 13.687

==================== evaluation at iteration: 115 ====================
train total loss: 87.502%
train max loss: 26.293%, reg loss: 75.069%
time spent training so far: 0:37:32.362352
train attack total time: 12.875s
train attack init time: 2.215s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.356s


train attack loss increase over inner max: -0.951
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.893
total grad norm: 10.050

==================== evaluation at iteration: 116 ====================
train total loss: 88.768%
train max loss: 25.562%, reg loss: 75.277%
time spent training so far: 0:37:42.829158
train attack total time: 10.104s
train attack init time: 2.727s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.219s


train attack loss increase over inner max: -0.260
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.736
total grad norm: 9.610

==================== evaluation at iteration: 117 ====================
train total loss: 87.308%
train max loss: 23.790%, reg loss: 75.185%
time spent training so far: 0:38:01.101025
train attack total time: 17.927s
train attack init time: 2.367s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.562s


train attack loss increase over inner max: -1.044
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.628
total grad norm: 5.195

==================== evaluation at iteration: 118 ====================
train total loss: 86.074%
train max loss: 23.117%, reg loss: 75.228%
time spent training so far: 0:38:15.100525
train attack total time: 13.653s
train attack init time: 2.269s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.386s


train attack loss increase over inner max: -1.233
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.567
total grad norm: 6.259

==================== evaluation at iteration: 119 ====================
train total loss: 88.326%
train max loss: 25.393%, reg loss: 75.240%
time spent training so far: 0:38:25.825231
train attack total time: 10.324s
train attack init time: 2.073s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: 2.269
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.470
total grad norm: 4.759

==================== evaluation at iteration: 120 ====================
train total loss: 87.799%
train max loss: 24.415%, reg loss: 75.172%
time spent training so far: 0:38:39.702117
train attack total time: 13.517s
train attack init time: 2.217s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.382s


train attack loss increase over inner max: -1.175
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.370
total grad norm: 5.343

==================== evaluation at iteration: 121 ====================
train total loss: 87.255%
train max loss: 23.801%, reg loss: 75.259%
time spent training so far: 0:38:51.965837
train attack total time: 11.925s
train attack init time: 2.239s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.333s


train attack loss increase over inner max: -0.765
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.377
total grad norm: 4.210

==================== evaluation at iteration: 122 ====================
train total loss: 87.379%
train max loss: 22.663%, reg loss: 75.156%
time spent training so far: 0:39:02.618571
train attack total time: 10.279s
train attack init time: 2.380s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.257s


train attack loss increase over inner max: -0.823
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.404
total grad norm: 3.313

==================== evaluation at iteration: 123 ====================
train total loss: 87.242%
train max loss: 21.272%, reg loss: 75.231%
time spent training so far: 0:39:17.060976
train attack total time: 14.110s
train attack init time: 2.498s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.418s


train attack loss increase over inner max: -1.725
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.282
total grad norm: 3.837

==================== evaluation at iteration: 124 ====================
train total loss: 85.743%
train max loss: 20.457%, reg loss: 75.009%
time spent training so far: 0:39:31.070016
train attack total time: 13.660s
train attack init time: 2.251s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.410s


train attack loss increase over inner max: -0.957
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.315
total grad norm: 4.029

==================== evaluation at iteration: 125 ====================
train total loss: 85.610%
train max loss: 20.239%, reg loss: 75.181%
time spent training so far: 0:39:42.046384
train attack total time: 10.623s
train attack init time: 2.235s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.532
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.800% of volume
percentage infeasible at boundary: 24.52%
mean, std amount infeasible at boundary: 1.17 +/- 2.75
max amount infeasible at boundary: 17.56

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.297
total grad norm: 4.687

==================== evaluation at iteration: 126 ====================
train total loss: 86.500%
train max loss: 19.569%, reg loss: 75.103%
time spent training so far: 0:41:54.646375
train attack total time: 14.174s
train attack init time: 2.774s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.408s


train attack loss increase over inner max: -0.630
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.298
total grad norm: 3.911

==================== evaluation at iteration: 127 ====================
train total loss: 86.283%
train max loss: 18.498%, reg loss: 75.151%
time spent training so far: 0:42:08.210130
train attack total time: 13.222s
train attack init time: 2.428s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.382s


train attack loss increase over inner max: -0.859
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.377
total grad norm: 5.058

==================== evaluation at iteration: 128 ====================
train total loss: 86.807%
train max loss: 22.023%, reg loss: 75.262%
time spent training so far: 0:42:18.867652
train attack total time: 10.329s
train attack init time: 2.163s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: 3.885
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.334
total grad norm: 4.372

==================== evaluation at iteration: 129 ====================
train total loss: 86.458%
train max loss: 20.339%, reg loss: 75.098%
time spent training so far: 0:42:33.883710
train attack total time: 14.693s
train attack init time: 2.342s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.448s


train attack loss increase over inner max: -1.516
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.357
total grad norm: 4.595

==================== evaluation at iteration: 130 ====================
train total loss: 85.521%
train max loss: 18.140%, reg loss: 75.176%
time spent training so far: 0:42:46.918367
train attack total time: 12.646s
train attack init time: 2.438s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.355s


train attack loss increase over inner max: -2.237
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.424
total grad norm: 5.535

==================== evaluation at iteration: 131 ====================
train total loss: 87.364%
train max loss: 21.702%, reg loss: 75.030%
time spent training so far: 0:42:57.429392
train attack total time: 10.122s
train attack init time: 2.106s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.260s


train attack loss increase over inner max: 3.959
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0513]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0514]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0515]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0517]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0519]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0520]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0521]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0527]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.21827587485313416
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0527]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0531]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0535]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0540]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.27988192439079285
Parameter containing:
tensor([[0.0545]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0551]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0556]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0562]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0566]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0571]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0574]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0577]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0580]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0583]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0585]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0586]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0587]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0589]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0590]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.411
total grad norm: 3.791

==================== evaluation at iteration: 132 ====================
train total loss: 85.971%
train max loss: 19.187%, reg loss: 75.224%
time spent training so far: 0:43:09.447778
train attack total time: 11.659s
train attack init time: 2.404s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.312s


train attack loss increase over inner max: -1.273
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.373
total grad norm: 3.316

==================== evaluation at iteration: 133 ====================
train total loss: 86.138%
train max loss: 19.011%, reg loss: 75.177%
time spent training so far: 0:43:20.736046
train attack total time: 10.902s
train attack init time: 2.697s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.313
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.367
total grad norm: 4.671

==================== evaluation at iteration: 134 ====================
train total loss: 89.377%
train max loss: 26.805%, reg loss: 75.230%
time spent training so far: 0:43:35.096988
train attack total time: 13.992s
train attack init time: 2.436s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.415s


train attack loss increase over inner max: 7.387
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.602
total grad norm: 2.812

==================== evaluation at iteration: 135 ====================
train total loss: 89.282%
train max loss: 28.284%, reg loss: 75.195%
time spent training so far: 0:43:45.182044
train attack total time: 9.727s
train attack init time: 2.045s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.247s


train attack loss increase over inner max: 1.948
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.554
total grad norm: 4.990

==================== evaluation at iteration: 136 ====================
train total loss: 89.375%
train max loss: 25.882%, reg loss: 75.238%
time spent training so far: 0:43:59.028786
train attack total time: 13.474s
train attack init time: 2.294s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.399s


train attack loss increase over inner max: -2.362
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.461
total grad norm: 3.931

==================== evaluation at iteration: 137 ====================
train total loss: 87.457%
train max loss: 25.311%, reg loss: 75.141%
time spent training so far: 0:44:10.047943
train attack total time: 10.622s
train attack init time: 2.373s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.270s


train attack loss increase over inner max: -0.342
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.519
total grad norm: 5.485

==================== evaluation at iteration: 138 ====================
train total loss: 88.190%
train max loss: 23.272%, reg loss: 75.269%
time spent training so far: 0:44:22.750080
train attack total time: 12.344s
train attack init time: 2.354s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.346s


train attack loss increase over inner max: -1.775
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.542
total grad norm: 5.977

==================== evaluation at iteration: 139 ====================
train total loss: 88.012%
train max loss: 23.550%, reg loss: 75.417%
time spent training so far: 0:44:33.698280
train attack total time: 10.573s
train attack init time: 2.519s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: 0.090
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.379
total grad norm: 5.640

==================== evaluation at iteration: 140 ====================
train total loss: 86.459%
train max loss: 21.912%, reg loss: 75.183%
time spent training so far: 0:44:45.593135
train attack total time: 11.539s
train attack init time: 2.121s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: -1.519
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.441
total grad norm: 4.434

==================== evaluation at iteration: 141 ====================
train total loss: 85.619%
train max loss: 20.396%, reg loss: 75.148%
time spent training so far: 0:44:59.313598
train attack total time: 13.355s
train attack init time: 2.037s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.405s


train attack loss increase over inner max: -0.903
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.452
total grad norm: 5.546

==================== evaluation at iteration: 142 ====================
train total loss: 85.027%
train max loss: 20.074%, reg loss: 75.271%
time spent training so far: 0:45:09.507622
train attack total time: 9.816s
train attack init time: 2.237s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: -0.149
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.466
total grad norm: 8.369

==================== evaluation at iteration: 143 ====================
train total loss: 87.335%
train max loss: 23.416%, reg loss: 75.218%
time spent training so far: 0:45:19.499294
train attack total time: 9.642s
train attack init time: 2.542s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.220s


train attack loss increase over inner max: 4.152
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.475
total grad norm: 6.266

==================== evaluation at iteration: 144 ====================
train total loss: 85.724%
train max loss: 21.871%, reg loss: 75.253%
time spent training so far: 0:45:31.048142
train attack total time: 11.182s
train attack init time: 2.134s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.307s


train attack loss increase over inner max: -2.075
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.462
total grad norm: 6.719

==================== evaluation at iteration: 145 ====================
train total loss: 84.680%
train max loss: 22.178%, reg loss: 75.174%
time spent training so far: 0:45:41.536461
train attack total time: 10.119s
train attack init time: 1.976s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.267s


train attack loss increase over inner max: 0.333
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.530
total grad norm: 6.817

==================== evaluation at iteration: 146 ====================
train total loss: 87.788%
train max loss: 21.765%, reg loss: 75.237%
time spent training so far: 0:45:52.970305
train attack total time: 11.079s
train attack init time: 2.533s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.283s


train attack loss increase over inner max: 0.251
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.615
total grad norm: 9.744

==================== evaluation at iteration: 147 ====================
train total loss: 85.118%
train max loss: 20.289%, reg loss: 75.210%
time spent training so far: 0:46:07.791662
train attack total time: 14.471s
train attack init time: 2.020s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.454s


train attack loss increase over inner max: -1.419
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.577
total grad norm: 6.972

==================== evaluation at iteration: 148 ====================
train total loss: 84.376%
train max loss: 19.698%, reg loss: 75.114%
time spent training so far: 0:46:21.695190
train attack total time: 13.515s
train attack init time: 2.307s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.400s


train attack loss increase over inner max: -0.531
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.480
total grad norm: 4.393

==================== evaluation at iteration: 149 ====================
train total loss: 84.982%
train max loss: 18.492%, reg loss: 75.216%
time spent training so far: 0:46:31.840799
train attack total time: 9.783s
train attack init time: 2.019s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.250s


train attack loss increase over inner max: -0.688
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.673
total grad norm: 5.973

==================== evaluation at iteration: 150 ====================
train total loss: 86.822%
train max loss: 21.439%, reg loss: 75.210%
time spent training so far: 0:46:43.184155
train attack total time: 10.981s
train attack init time: 1.959s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: 0.034
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.440% of volume
percentage infeasible at boundary: 23.72%
mean, std amount infeasible at boundary: 1.27 +/- 12.15
max amount infeasible at boundary: 592.76

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.606
total grad norm: 6.682

==================== evaluation at iteration: 151 ====================
train total loss: 86.903%
train max loss: 21.104%, reg loss: 75.261%
time spent training so far: 0:48:44.852654
train attack total time: 11.562s
train attack init time: 2.109s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.324s


train attack loss increase over inner max: -0.760
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.654
total grad norm: 5.725

==================== evaluation at iteration: 152 ====================
train total loss: 83.927%
train max loss: 19.538%, reg loss: 75.225%
time spent training so far: 0:48:56.076706
train attack total time: 10.886s
train attack init time: 1.866s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.304s


train attack loss increase over inner max: -8.459
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.765
total grad norm: 6.012

==================== evaluation at iteration: 153 ====================
train total loss: 86.078%
train max loss: 19.434%, reg loss: 75.282%
time spent training so far: 0:49:06.717404
train attack total time: 10.293s
train attack init time: 1.994s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.273s


train attack loss increase over inner max: 0.158
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.719
total grad norm: 6.978

==================== evaluation at iteration: 154 ====================
train total loss: 85.325%
train max loss: 19.297%, reg loss: 75.383%
time spent training so far: 0:49:16.884874
train attack total time: 9.828s
train attack init time: 1.920s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.258s


train attack loss increase over inner max: 0.660
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.677
total grad norm: 8.596

==================== evaluation at iteration: 155 ====================
train total loss: 84.841%
train max loss: 18.722%, reg loss: 75.371%
time spent training so far: 0:49:26.067744
train attack total time: 8.858s
train attack init time: 2.229s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.218s


train attack loss increase over inner max: -0.204
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.719
total grad norm: 9.770

==================== evaluation at iteration: 156 ====================
train total loss: 86.328%
train max loss: 17.995%, reg loss: 75.102%
time spent training so far: 0:49:34.635798
train attack total time: 8.206s
train attack init time: 2.135s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: 0.061
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.673
total grad norm: 8.745

==================== evaluation at iteration: 157 ====================
train total loss: 86.249%
train max loss: 16.725%, reg loss: 75.408%
time spent training so far: 0:49:45.290401
train attack total time: 10.311s
train attack init time: 2.343s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.790
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.731
total grad norm: 7.668

==================== evaluation at iteration: 158 ====================
train total loss: 86.840%
train max loss: 19.476%, reg loss: 75.264%
time spent training so far: 0:49:55.056052
train attack total time: 9.424s
train attack init time: 2.196s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 2.621
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.746
total grad norm: 8.626

==================== evaluation at iteration: 159 ====================
train total loss: 85.436%
train max loss: 18.945%, reg loss: 75.350%
time spent training so far: 0:50:06.034182
train attack total time: 10.602s
train attack init time: 2.152s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.297s


train attack loss increase over inner max: -140.448
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.915
total grad norm: 5.611

==================== evaluation at iteration: 160 ====================
train total loss: 83.995%
train max loss: 18.368%, reg loss: 75.254%
time spent training so far: 0:50:13.615756
train attack total time: 7.307s
train attack init time: 2.149s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.154s


train attack loss increase over inner max: -0.785
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.874
total grad norm: 8.477

==================== evaluation at iteration: 161 ====================
train total loss: 84.193%
train max loss: 18.035%, reg loss: 75.356%
time spent training so far: 0:50:22.705648
train attack total time: 8.753s
train attack init time: 2.066s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.217s


train attack loss increase over inner max: -0.197
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.736
total grad norm: 9.216

==================== evaluation at iteration: 162 ====================
train total loss: 85.628%
train max loss: 20.041%, reg loss: 75.355%
time spent training so far: 0:50:31.423614
train attack total time: 8.401s
train attack init time: 2.196s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: 1.811
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.875
total grad norm: 19.615

==================== evaluation at iteration: 163 ====================
train total loss: 85.532%
train max loss: 21.141%, reg loss: 75.387%
time spent training so far: 0:50:39.010491
train attack total time: 7.260s
train attack init time: 2.307s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.145s


train attack loss increase over inner max: 1.084
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.946
total grad norm: 6.901

==================== evaluation at iteration: 164 ====================
train total loss: 84.401%
train max loss: 18.508%, reg loss: 75.383%
time spent training so far: 0:50:48.204449
train attack total time: 8.861s
train attack init time: 2.013s
train attack avg grad step time: 0.075s
train attack avg reproj time: 0.220s


train attack loss increase over inner max: -0.492
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.811
total grad norm: 8.119

==================== evaluation at iteration: 165 ====================
train total loss: 84.650%
train max loss: 17.924%, reg loss: 75.420%
time spent training so far: 0:50:56.047485
train attack total time: 7.514s
train attack init time: 2.035s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.165s


train attack loss increase over inner max: -0.113
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.837
total grad norm: 12.087

==================== evaluation at iteration: 166 ====================
train total loss: 85.006%
train max loss: 20.202%, reg loss: 75.388%
time spent training so far: 0:51:03.361078
train attack total time: 6.975s
train attack init time: 2.039s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.140s


train attack loss increase over inner max: 2.876
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.861
total grad norm: 14.728

==================== evaluation at iteration: 167 ====================
train total loss: 82.898%
train max loss: 19.291%, reg loss: 75.434%
time spent training so far: 0:51:09.931848
train attack total time: 6.281s
train attack init time: 2.064s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.114s


train attack loss increase over inner max: -0.174
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.884
total grad norm: 10.832

==================== evaluation at iteration: 168 ====================
train total loss: 85.086%
train max loss: 17.528%, reg loss: 75.362%
time spent training so far: 0:51:17.165923
train attack total time: 6.895s
train attack init time: 2.072s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.141s


train attack loss increase over inner max: -0.108
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.893
total grad norm: 7.059

==================== evaluation at iteration: 169 ====================
train total loss: 84.842%
train max loss: 16.979%, reg loss: 75.404%
time spent training so far: 0:51:25.280363
train attack total time: 7.753s
train attack init time: 2.206s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: -0.437
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.961
total grad norm: 20.351

==================== evaluation at iteration: 170 ====================
train total loss: 85.137%
train max loss: 20.263%, reg loss: 75.321%
time spent training so far: 0:51:32.783560
train attack total time: 7.135s
train attack init time: 2.063s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.149s


train attack loss increase over inner max: 2.883
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.922
total grad norm: 17.257

==================== evaluation at iteration: 171 ====================
train total loss: 83.152%
train max loss: 16.091%, reg loss: 75.310%
time spent training so far: 0:51:40.077348
train attack total time: 6.866s
train attack init time: 2.011s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.140s


train attack loss increase over inner max: -0.748
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.857
total grad norm: 9.169

==================== evaluation at iteration: 172 ====================
train total loss: 83.839%
train max loss: 16.041%, reg loss: 75.481%
time spent training so far: 0:51:47.174969
train attack total time: 6.779s
train attack init time: 1.977s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.133s


train attack loss increase over inner max: 0.459
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.077
total grad norm: 51.567

==================== evaluation at iteration: 173 ====================
train total loss: 86.286%
train max loss: 24.661%, reg loss: 75.345%
time spent training so far: 0:51:54.884225
train attack total time: 7.415s
train attack init time: 1.949s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: 9.449
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.866
total grad norm: 19.128

==================== evaluation at iteration: 174 ====================
train total loss: 82.677%
train max loss: 18.520%, reg loss: 75.283%
time spent training so far: 0:52:03.344416
train attack total time: 8.128s
train attack init time: 2.054s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.204s


train attack loss increase over inner max: 3.047
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.742
total grad norm: 9.703

==================== evaluation at iteration: 175 ====================
train total loss: 87.067%
train max loss: 22.834%, reg loss: 75.345%
time spent training so far: 0:52:10.159731
train attack total time: 6.512s
train attack init time: 2.057s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.131s


train attack loss increase over inner max: 4.336
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 5.040% of volume
percentage infeasible at boundary: 23.20%
mean, std amount infeasible at boundary: 0.82 +/- 2.19
max amount infeasible at boundary: 35.82

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.743
total grad norm: 12.118

==================== evaluation at iteration: 176 ====================
train total loss: 86.454%
train max loss: 22.060%, reg loss: 75.258%
time spent training so far: 0:54:04.271788
train attack total time: 7.212s
train attack init time: 2.047s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.162s


train attack loss increase over inner max: -7.580
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000

Max_n_steps: 21
Parameter containing:
tensor([[0.0591]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0592]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0593]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0594]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0596]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0598]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0602]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0604]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0607]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0611]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0615]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0620]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0626]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0632]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.35346055030822754
Parameter containing:
tensor([[0.0639]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0647]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0656]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0664]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0673]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0684]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0695]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0708]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0722]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0739]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0756]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0773]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.17152349650859833
Parameter containing:
tensor([[0.0791]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0809]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0828]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0847]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0868]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0888]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0908]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0929]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0951]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0973]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0994]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.1016]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.1037]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.1059]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1082]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1105]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1128]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1149]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.896
total grad norm: 8.004

==================== evaluation at iteration: 177 ====================
train total loss: 85.842%
train max loss: 21.846%, reg loss: 75.351%
time spent training so far: 0:54:12.137679
train attack total time: 7.535s
train attack init time: 2.114s
train attack avg grad step time: 0.077s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.637
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.773
total grad norm: 10.416

==================== evaluation at iteration: 178 ====================
train total loss: 87.118%
train max loss: 21.074%, reg loss: 75.206%
time spent training so far: 0:54:19.714120
train attack total time: 7.256s
train attack init time: 2.100s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.165s


train attack loss increase over inner max: -0.892
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.796
total grad norm: 7.447

==================== evaluation at iteration: 179 ====================
train total loss: 88.003%
train max loss: 26.446%, reg loss: 75.280%
time spent training so far: 0:54:26.934004
train attack total time: 6.873s
train attack init time: 2.111s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.148s


train attack loss increase over inner max: 5.412
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.811
total grad norm: 12.171

==================== evaluation at iteration: 180 ====================
train total loss: 87.331%
train max loss: 25.831%, reg loss: 75.235%
time spent training so far: 0:54:34.802778
train attack total time: 7.558s
train attack init time: 2.189s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.174s


train attack loss increase over inner max: -0.080
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.830
total grad norm: 7.989

==================== evaluation at iteration: 181 ====================
train total loss: 87.035%
train max loss: 24.918%, reg loss: 75.094%
time spent training so far: 0:54:42.570887
train attack total time: 7.462s
train attack init time: 2.009s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.177s


train attack loss increase over inner max: -2.294
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.803
total grad norm: 11.274

==================== evaluation at iteration: 182 ====================
train total loss: 86.868%
train max loss: 23.606%, reg loss: 75.141%
time spent training so far: 0:54:49.452758
train attack total time: 6.472s
train attack init time: 2.172s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.128s


train attack loss increase over inner max: 0.392
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.784
total grad norm: 7.785

==================== evaluation at iteration: 183 ====================
train total loss: 85.783%
train max loss: 22.039%, reg loss: 75.333%
time spent training so far: 0:54:57.393843
train attack total time: 7.613s
train attack init time: 1.993s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: 0.483
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.773
total grad norm: 7.570

==================== evaluation at iteration: 184 ====================
train total loss: 86.886%
train max loss: 22.713%, reg loss: 75.252%
time spent training so far: 0:55:04.760300
train attack total time: 7.038s
train attack init time: 2.089s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.149s


train attack loss increase over inner max: 1.726
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.873
total grad norm: 6.495

==================== evaluation at iteration: 185 ====================
train total loss: 84.810%
train max loss: 21.147%, reg loss: 75.278%
time spent training so far: 0:55:12.258326
train attack total time: 7.174s
train attack init time: 2.165s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: 0.011
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.776
total grad norm: 52.393

==================== evaluation at iteration: 186 ====================
train total loss: 88.844%
train max loss: 28.302%, reg loss: 75.232%
time spent training so far: 0:55:23.335492
train attack total time: 10.684s
train attack init time: 1.951s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: -482.682
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.919
total grad norm: 21.935

==================== evaluation at iteration: 187 ====================
train total loss: 86.503%
train max loss: 19.857%, reg loss: 75.329%
time spent training so far: 0:55:31.975148
train attack total time: 8.273s
train attack init time: 2.054s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.211s


train attack loss increase over inner max: -0.676
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.776
total grad norm: 9.975

==================== evaluation at iteration: 188 ====================
train total loss: 84.392%
train max loss: 21.188%, reg loss: 75.246%
time spent training so far: 0:55:38.520229
train attack total time: 6.228s
train attack init time: 2.023s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.122s


train attack loss increase over inner max: 1.393
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.859
total grad norm: 9.953

==================== evaluation at iteration: 189 ====================
train total loss: 85.222%
train max loss: 20.170%, reg loss: 75.169%
time spent training so far: 0:55:46.161483
train attack total time: 7.307s
train attack init time: 2.011s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -1.169
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.880
total grad norm: 12.421

==================== evaluation at iteration: 190 ====================
train total loss: 85.360%
train max loss: 17.732%, reg loss: 75.309%
time spent training so far: 0:55:55.230125
train attack total time: 8.799s
train attack init time: 2.104s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: -1.813
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.072
total grad norm: 8.871

==================== evaluation at iteration: 191 ====================
train total loss: 86.565%
train max loss: 18.067%, reg loss: 75.071%
time spent training so far: 0:56:03.336958
train attack total time: 7.665s
train attack init time: 2.103s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.185s


train attack loss increase over inner max: 0.642
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.867
total grad norm: 8.420

==================== evaluation at iteration: 192 ====================
train total loss: 86.139%
train max loss: 17.326%, reg loss: 75.404%
time spent training so far: 0:56:11.687023
train attack total time: 8.031s
train attack init time: 2.157s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: -0.701
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.088
total grad norm: 8.833

==================== evaluation at iteration: 193 ====================
train total loss: 84.086%
train max loss: 17.746%, reg loss: 75.228%
time spent training so far: 0:56:19.277356
train attack total time: 7.269s
train attack init time: 2.065s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.169s


train attack loss increase over inner max: 0.067
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.207
total grad norm: 8.248

==================== evaluation at iteration: 194 ====================
train total loss: 85.904%
train max loss: 20.268%, reg loss: 75.209%
time spent training so far: 0:56:26.063381
train attack total time: 6.483s
train attack init time: 1.900s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.142s


train attack loss increase over inner max: 2.784
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.425
total grad norm: 13.814

==================== evaluation at iteration: 195 ====================
train total loss: 86.457%
train max loss: 20.316%, reg loss: 75.287%
time spent training so far: 0:56:34.596146
train attack total time: 8.234s
train attack init time: 2.176s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: -0.248
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.188
total grad norm: 24.155

==================== evaluation at iteration: 196 ====================
train total loss: 86.833%
train max loss: 22.157%, reg loss: 75.405%
time spent training so far: 0:56:44.715661
train attack total time: 9.712s
train attack init time: 2.101s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.274s


train attack loss increase over inner max: 2.093
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.111
total grad norm: 17.685

==================== evaluation at iteration: 197 ====================
train total loss: 84.453%
train max loss: 18.146%, reg loss: 75.143%
time spent training so far: 0:56:53.238025
train attack total time: 8.161s
train attack init time: 2.054s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.208s


train attack loss increase over inner max: -0.351
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.157
total grad norm: 12.654

==================== evaluation at iteration: 198 ====================
train total loss: 84.932%
train max loss: 17.510%, reg loss: 75.436%
time spent training so far: 0:57:00.491075
train attack total time: 6.902s
train attack init time: 2.213s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.143s


train attack loss increase over inner max: -0.561
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.259
total grad norm: 5.856

==================== evaluation at iteration: 199 ====================
train total loss: 85.254%
train max loss: 21.902%, reg loss: 75.301%
time spent training so far: 0:57:08.437955
train attack total time: 7.638s
train attack init time: 2.004s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.183s


train attack loss increase over inner max: 4.648
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.465
total grad norm: 8.258

==================== evaluation at iteration: 200 ====================
train total loss: 86.010%
train max loss: 20.646%, reg loss: 75.271%
time spent training so far: 0:57:17.100792
train attack total time: 8.394s
train attack init time: 2.286s
train attack avg grad step time: 0.064s
train attack avg reproj time: 0.211s


train attack loss increase over inner max: -1.050
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 3.480% of volume
percentage infeasible at boundary: 20.36%
mean, std amount infeasible at boundary: 0.85 +/- 2.41
max amount infeasible at boundary: 34.00

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.926
total grad norm: 10.427

==================== evaluation at iteration: 201 ====================
train total loss: 88.067%
train max loss: 25.155%, reg loss: 75.320%
time spent training so far: 0:59:16.599167
train attack total time: 8.680s
train attack init time: 2.215s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: 4.575
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.958
total grad norm: 10.079

==================== evaluation at iteration: 202 ====================
train total loss: 88.260%
train max loss: 23.816%, reg loss: 75.453%
time spent training so far: 0:59:25.049280
train attack total time: 8.146s
train attack init time: 2.104s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.202s


train attack loss increase over inner max: -1.032
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.054
total grad norm: 5.708

==================== evaluation at iteration: 203 ====================
train total loss: 87.203%
train max loss: 22.811%, reg loss: 75.165%
time spent training so far: 0:59:35.713561
train attack total time: 10.319s
train attack init time: 2.126s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.300s


train attack loss increase over inner max: -23.351
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.938
total grad norm: 17.189

==================== evaluation at iteration: 204 ====================
train total loss: 88.737%
train max loss: 25.179%, reg loss: 75.183%
time spent training so far: 0:59:42.963660
train attack total time: 6.956s
train attack init time: 2.044s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.149s


train attack loss increase over inner max: 2.177
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.355
total grad norm: 34.551

==================== evaluation at iteration: 205 ====================
train total loss: 88.808%
train max loss: 24.796%, reg loss: 75.252%
time spent training so far: 0:59:49.969168
train attack total time: 6.686s
train attack init time: 1.986s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.144s


train attack loss increase over inner max: 2.409
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.834
total grad norm: 8.613

==================== evaluation at iteration: 206 ====================
train total loss: 86.290%
train max loss: 21.698%, reg loss: 75.234%
time spent training so far: 0:59:58.791673
train attack total time: 8.511s
train attack init time: 2.219s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -1.173
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.138
total grad norm: 19.607

==================== evaluation at iteration: 207 ====================
train total loss: 85.386%
train max loss: 20.155%, reg loss: 75.306%
time spent training so far: 1:00:08.394204
train attack total time: 9.253s
train attack init time: 2.046s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.258s


train attack loss increase over inner max: -350.212
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.950
total grad norm: 12.876

==================== evaluation at iteration: 208 ====================
train total loss: 84.636%
train max loss: 19.786%, reg loss: 75.307%
time spent training so far: 1:00:16.124511
train attack total time: 7.392s
train attack init time: 1.985s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: -1.042
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.041
total grad norm: 9.144

==================== evaluation at iteration: 209 ====================
train total loss: 84.968%
train max loss: 18.916%, reg loss: 75.218%
time spent training so far: 1:00:24.106520
train attack total time: 7.666s
train attack init time: 2.105s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.181s


train attack loss increase over inner max: 0.303
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.973
total grad norm: 7.939

==================== evaluation at iteration: 210 ====================
train total loss: 85.539%
train max loss: 18.219%, reg loss: 75.240%
time spent training so far: 1:00:30.314057
train attack total time: 5.880s
train attack init time: 2.061s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.103s


train attack loss increase over inner max: -0.731
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.885
total grad norm: 7.417

==================== evaluation at iteration: 211 ====================
train total loss: 85.264%
train max loss: 19.055%, reg loss: 75.361%
time spent training so far: 1:00:37.772625
train attack total time: 7.107s
train attack init time: 2.345s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.146s


train attack loss increase over inner max: 1.900
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.811
total grad norm: 7.978

==================== evaluation at iteration: 212 ====================
train total loss: 86.049%
train max loss: 18.452%, reg loss: 75.308%
time spent training so far: 1:00:46.152678
train attack total time: 8.059s
train attack init time: 2.316s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: -1.022
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.058
total grad norm: 9.524

==================== evaluation at iteration: 213 ====================
train total loss: 84.137%
train max loss: 17.865%, reg loss: 75.200%
time spent training so far: 1:00:52.911786
train attack total time: 6.431s
train attack init time: 2.076s
train attack avg grad step time: 0.063s
train attack avg reproj time: 0.133s


train attack loss increase over inner max: -1.101
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.952
total grad norm: 8.498

==================== evaluation at iteration: 214 ====================
train total loss: 86.610%
train max loss: 20.512%, reg loss: 75.410%
time spent training so far: 1:00:59.364463
train attack total time: 6.139s
train attack init time: 2.040s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.114s


train attack loss increase over inner max: 1.951
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.139
total grad norm: 9.348

==================== evaluation at iteration: 215 ====================
train total loss: 88.764%
train max loss: 27.472%, reg loss: 75.306%
time spent training so far: 1:01:07.152222
train attack total time: 7.464s
train attack init time: 1.943s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.179s


train attack loss increase over inner max: 6.298
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.059
total grad norm: 6.315

==================== evaluation at iteration: 216 ====================
train total loss: 89.450%
train max loss: 28.015%, reg loss: 75.321%
time spent training so far: 1:01:14.745913
train attack total time: 7.275s
train attack init time: 2.111s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.165s


train attack loss increase over inner max: 0.385
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.976
total grad norm: 6.077

==================== evaluation at iteration: 217 ====================
train total loss: 88.287%
train max loss: 26.146%, reg loss: 75.289%
time spent training so far: 1:01:21.834144
train attack total time: 6.752s
train attack init time: 2.228s
train attack avg grad step time: 0.064s
train attack avg reproj time: 0.139s


train attack loss increase over inner max: -0.663
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.005
total grad norm: 8.780

==================== evaluation at iteration: 218 ====================
train total loss: 87.952%
train max loss: 26.393%, reg loss: 75.221%
time spent training so far: 1:01:28.839066
train attack total time: 6.694s
train attack init time: 2.066s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: -0.406
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.151
total grad norm: 7.028

==================== evaluation at iteration: 219 ====================
train total loss: 86.821%
train max loss: 24.840%, reg loss: 75.401%
time spent training so far: 1:01:36.656845
train attack total time: 7.515s
train attack init time: 2.075s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.176s


train attack loss increase over inner max: -1.659
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.156
total grad norm: 8.381

==================== evaluation at iteration: 220 ====================
train total loss: 86.002%
train max loss: 24.609%, reg loss: 75.413%
time spent training so far: 1:01:43.992276
train attack total time: 7.001s
train attack init time: 1.946s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.159s


train attack loss increase over inner max: -0.724
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1170]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1189]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1206]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1222]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1236]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1249]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1260]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1271]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1280]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  397.8602294921875
Parameter containing:
tensor([[0.1293]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1308]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1322]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1335]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1347]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1359]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1369]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1379]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1388]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1398]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1408]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1418]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1428]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1436]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1445]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1452]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1459]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.7717676162719727
Parameter containing:
tensor([[0.1464]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1471]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1480]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1490]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  132.5690155029297
Parameter containing:
tensor([[0.1501]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1512]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1532]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1541]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1549]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1556]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1562]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1568]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1574]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1578]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1581]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1585]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1588]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 1.029
total grad norm: 8.402

==================== evaluation at iteration: 221 ====================
train total loss: 89.465%
train max loss: 23.718%, reg loss: 75.396%
time spent training so far: 1:01:51.834936
train attack total time: 7.473s
train attack init time: 2.215s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: 0.236
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.242
total grad norm: 7.793

==================== evaluation at iteration: 222 ====================
train total loss: 88.345%
train max loss: 23.274%, reg loss: 75.364%
time spent training so far: 1:01:58.033169
train attack total time: 5.869s
train attack init time: 1.955s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.107s


train attack loss increase over inner max: -0.552
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.023
total grad norm: 9.406

==================== evaluation at iteration: 223 ====================
train total loss: 87.441%
train max loss: 22.327%, reg loss: 75.440%
time spent training so far: 1:02:05.993489
train attack total time: 7.645s
train attack init time: 1.992s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.189s


train attack loss increase over inner max: -0.221
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.368
total grad norm: 9.492

==================== evaluation at iteration: 224 ====================
train total loss: 85.375%
train max loss: 21.663%, reg loss: 75.671%
time spent training so far: 1:02:12.809892
train attack total time: 6.467s
train attack init time: 2.061s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.127s


train attack loss increase over inner max: 0.193
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.285
total grad norm: 9.678

==================== evaluation at iteration: 225 ====================
train total loss: 86.237%
train max loss: 20.588%, reg loss: 75.390%
time spent training so far: 1:02:20.304112
train attack total time: 7.192s
train attack init time: 2.058s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.851
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 2.960% of volume
percentage infeasible at boundary: 17.48%
mean, std amount infeasible at boundary: 0.67 +/- 2.57
max amount infeasible at boundary: 80.07

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 1.230
total grad norm: 112.177

==================== evaluation at iteration: 226 ====================
train total loss: 105.469%
train max loss: 43.475%, reg loss: 75.381%
time spent training so far: 1:04:13.902666
train attack total time: 7.556s
train attack init time: 2.156s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.175s


train attack loss increase over inner max: 22.665
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.165
total grad norm: 72.347

==================== evaluation at iteration: 227 ====================
train total loss: 94.164%
train max loss: 34.603%, reg loss: 75.345%
time spent training so far: 1:04:21.190232
train attack total time: 6.912s
train attack init time: 2.027s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.150s


train attack loss increase over inner max: 9.647
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.233
total grad norm: 20.287

==================== evaluation at iteration: 228 ====================
train total loss: 85.999%
train max loss: 19.146%, reg loss: 75.460%
time spent training so far: 1:04:28.979618
train attack total time: 7.440s
train attack init time: 2.148s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.173s


train attack loss increase over inner max: -0.061
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.445
total grad norm: 22.556

==================== evaluation at iteration: 229 ====================
train total loss: 84.366%
train max loss: 18.287%, reg loss: 75.525%
time spent training so far: 1:04:37.266615
train attack total time: 7.950s
train attack init time: 2.153s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.189s


train attack loss increase over inner max: -0.012
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.212
total grad norm: 12.582

==================== evaluation at iteration: 230 ====================
train total loss: 83.903%
train max loss: 17.781%, reg loss: 75.428%
time spent training so far: 1:04:44.911774
train attack total time: 7.323s
train attack init time: 1.869s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: -0.739
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.543
total grad norm: 9.278

==================== evaluation at iteration: 231 ====================
train total loss: 85.600%
train max loss: 17.513%, reg loss: 75.495%
time spent training so far: 1:04:53.054167
train attack total time: 7.769s
train attack init time: 2.133s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: -0.476
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.558
total grad norm: 7.159

==================== evaluation at iteration: 232 ====================
train total loss: 84.753%
train max loss: 17.680%, reg loss: 75.481%
time spent training so far: 1:05:00.838367
train attack total time: 7.488s
train attack init time: 2.249s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.162s


train attack loss increase over inner max: -0.067
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.836
total grad norm: 6.839

==================== evaluation at iteration: 233 ====================
train total loss: 87.186%
train max loss: 19.218%, reg loss: 75.634%
time spent training so far: 1:05:08.330517
train attack total time: 7.157s
train attack init time: 2.250s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.154s


train attack loss increase over inner max: 1.699
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.067
total grad norm: 6.592

==================== evaluation at iteration: 234 ====================
train total loss: 86.585%
train max loss: 19.902%, reg loss: 75.718%
time spent training so far: 1:05:15.748668
train attack total time: 7.117s
train attack init time: 2.095s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: -0.018
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.861
total grad norm: 7.924

==================== evaluation at iteration: 235 ====================
train total loss: 85.124%
train max loss: 18.694%, reg loss: 75.619%
time spent training so far: 1:05:22.769619
train attack total time: 6.736s
train attack init time: 2.113s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.139s


train attack loss increase over inner max: -0.670
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.022
total grad norm: 9.773

==================== evaluation at iteration: 236 ====================
train total loss: 89.963%
train max loss: 28.274%, reg loss: 75.610%
time spent training so far: 1:05:30.191979
train attack total time: 7.089s
train attack init time: 2.124s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.154s


train attack loss increase over inner max: 8.804
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.866
total grad norm: 6.193

==================== evaluation at iteration: 237 ====================
train total loss: 89.685%
train max loss: 27.887%, reg loss: 75.533%
time spent training so far: 1:05:37.644340
train attack total time: 7.115s
train attack init time: 1.956s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.604
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.719
total grad norm: 7.130

==================== evaluation at iteration: 238 ====================
train total loss: 88.361%
train max loss: 27.331%, reg loss: 75.515%
time spent training so far: 1:05:45.491167
train attack total time: 7.542s
train attack init time: 2.304s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.169s


train attack loss increase over inner max: -0.267
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.740
total grad norm: 5.598

==================== evaluation at iteration: 239 ====================
train total loss: 89.318%
train max loss: 26.792%, reg loss: 75.425%
time spent training so far: 1:05:53.254998
train attack total time: 7.477s
train attack init time: 2.181s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: 0.803
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.442
total grad norm: 7.241

==================== evaluation at iteration: 240 ====================
train total loss: 87.146%
train max loss: 25.388%, reg loss: 75.474%
time spent training so far: 1:06:00.804749
train attack total time: 7.226s
train attack init time: 2.069s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.160s


train attack loss increase over inner max: -0.656
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.727
total grad norm: 6.869

==================== evaluation at iteration: 241 ====================
train total loss: 87.621%
train max loss: 24.599%, reg loss: 75.447%
time spent training so far: 1:06:08.299045
train attack total time: 7.096s
train attack init time: 2.301s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.147s


train attack loss increase over inner max: 0.489
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.929
total grad norm: 9.096

==================== evaluation at iteration: 242 ====================
train total loss: 87.335%
train max loss: 23.605%, reg loss: 75.246%
time spent training so far: 1:06:15.986647
train attack total time: 7.298s
train attack init time: 2.164s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.863
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.778
total grad norm: 9.992

==================== evaluation at iteration: 243 ====================
train total loss: 86.421%
train max loss: 21.539%, reg loss: 75.590%
time spent training so far: 1:06:28.140105
train attack total time: 11.832s
train attack init time: 2.009s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.375s


train attack loss increase over inner max: 0.137
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.573
total grad norm: 26.034

==================== evaluation at iteration: 244 ====================
train total loss: 86.868%
train max loss: 21.135%, reg loss: 75.443%
time spent training so far: 1:06:37.310295
train attack total time: 8.895s
train attack init time: 2.323s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.230s


train attack loss increase over inner max: 0.242
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.552
total grad norm: 15.550

==================== evaluation at iteration: 245 ====================
train total loss: 85.916%
train max loss: 20.741%, reg loss: 75.460%
time spent training so far: 1:06:45.515506
train attack total time: 7.905s
train attack init time: 2.097s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.191s


train attack loss increase over inner max: 1.428
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.597
total grad norm: 9.719

==================== evaluation at iteration: 246 ====================
train total loss: 86.414%
train max loss: 20.083%, reg loss: 75.529%
time spent training so far: 1:06:52.676158
train attack total time: 6.797s
train attack init time: 2.204s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.141s


train attack loss increase over inner max: 0.110
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.578
total grad norm: 10.816

==================== evaluation at iteration: 247 ====================
train total loss: 87.382%
train max loss: 19.581%, reg loss: 75.417%
time spent training so far: 1:07:00.558025
train attack total time: 7.512s
train attack init time: 2.033s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: -0.419
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.526
total grad norm: 8.359

==================== evaluation at iteration: 248 ====================
train total loss: 84.668%
train max loss: 18.527%, reg loss: 75.522%
time spent training so far: 1:07:08.227457
train attack total time: 7.340s
train attack init time: 2.098s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.822
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.270
total grad norm: 8.975

==================== evaluation at iteration: 249 ====================
train total loss: 84.133%
train max loss: 17.088%, reg loss: 75.437%
time spent training so far: 1:07:14.922483
train attack total time: 6.383s
train attack init time: 1.974s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.130s


train attack loss increase over inner max: -1.021
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.431
total grad norm: 7.923

==================== evaluation at iteration: 250 ====================
train total loss: 83.515%
train max loss: 17.015%, reg loss: 75.492%
time spent training so far: 1:07:22.511794
train attack total time: 7.258s
train attack init time: 1.985s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.171s


train attack loss increase over inner max: 0.631
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 3.520% of volume
percentage infeasible at boundary: 18.20%
mean, std amount infeasible at boundary: 0.64 +/- 1.87
max amount infeasible at boundary: 16.34

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 1.861
total grad norm: 7.514

==================== evaluation at iteration: 251 ====================
train total loss: 86.123%
train max loss: 21.129%, reg loss: 75.481%
time spent training so far: 1:09:12.779387
train attack total time: 7.244s
train attack init time: 1.895s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 4.586
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.552
total grad norm: 9.698

==================== evaluation at iteration: 252 ====================
train total loss: 86.163%
train max loss: 21.431%, reg loss: 75.473%
time spent training so far: 1:09:19.759512
train attack total time: 6.703s
train attack init time: 1.980s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.145s


train attack loss increase over inner max: 1.919
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.401
total grad norm: 6.852

==================== evaluation at iteration: 253 ====================
train total loss: 84.802%
train max loss: 20.180%, reg loss: 75.460%
time spent training so far: 1:09:26.575535
train attack total time: 6.476s
train attack init time: 2.024s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.134s


train attack loss increase over inner max: -1.197
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.298
total grad norm: 36.507

==================== evaluation at iteration: 254 ====================
train total loss: 89.014%
train max loss: 27.219%, reg loss: 75.552%
time spent training so far: 1:09:33.963760
train attack total time: 7.037s
train attack init time: 2.046s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.155s


train attack loss increase over inner max: 7.654
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.249
total grad norm: 11.338

==================== evaluation at iteration: 255 ====================
train total loss: 82.860%
train max loss: 18.934%, reg loss: 75.536%
time spent training so far: 1:09:41.957862
train attack total time: 7.658s
train attack init time: 1.939s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.191s


train attack loss increase over inner max: -1.802
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.375
total grad norm: 9.272

==================== evaluation at iteration: 256 ====================
train total loss: 86.009%
train max loss: 19.909%, reg loss: 75.500%
time spent training so far: 1:09:49.413432
train attack total time: 7.068s
train attack init time: 1.869s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: 3.665
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.408
total grad norm: 12.083

==================== evaluation at iteration: 257 ====================
train total loss: 86.031%
train max loss: 19.514%, reg loss: 75.542%
time spent training so far: 1:09:56.658434
train attack total time: 6.943s
train attack init time: 2.055s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.150s


train attack loss increase over inner max: 0.468
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.185
total grad norm: 16.957

==================== evaluation at iteration: 258 ====================
train total loss: 84.978%
train max loss: 20.377%, reg loss: 75.480%
time spent training so far: 1:10:04.243790
train attack total time: 7.234s
train attack init time: 2.146s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.158s


train attack loss increase over inner max: 2.907
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.294
total grad norm: 17.336

==================== evaluation at iteration: 259 ====================
train total loss: 86.102%
train max loss: 23.959%, reg loss: 75.401%
time spent training so far: 1:10:12.343420
train attack total time: 7.793s
train attack init time: 2.019s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: 5.153
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.461
total grad norm: 16.343

==================== evaluation at iteration: 260 ====================
train total loss: 87.412%
train max loss: 22.371%, reg loss: 75.580%
time spent training so far: 1:10:20.062388
train attack total time: 7.357s
train attack init time: 2.047s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: 1.561
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.417
total grad norm: 13.468

==================== evaluation at iteration: 261 ====================
train total loss: 84.501%
train max loss: 20.269%, reg loss: 75.470%
time spent training so far: 1:10:27.092556
train attack total time: 6.624s
train attack init time: 2.184s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.125s


train attack loss increase over inner max: -1.011
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.423
total grad norm: 16.965

==================== evaluation at iteration: 262 ====================
train total loss: 85.637%
train max loss: 19.553%, reg loss: 75.538%
time spent training so far: 1:10:35.335403
train attack total time: 7.904s
train attack init time: 2.151s
train attack avg grad step time: 0.075s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: 0.117
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.350
total grad norm: 12.285

==================== evaluation at iteration: 263 ====================
train total loss: 85.264%
train max loss: 18.418%, reg loss: 75.541%
time spent training so far: 1:10:43.441052
train attack total time: 7.823s
train attack init time: 1.998s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.191s


train attack loss increase over inner max: -0.413
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.478
total grad norm: 9.977

==================== evaluation at iteration: 264 ====================
train total loss: 83.482%
train max loss: 18.107%, reg loss: 75.470%
time spent training so far: 1:10:51.218583
train attack total time: 7.468s
train attack init time: 2.052s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.171s


train attack loss increase over inner max: 1.617
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.226
total grad norm: 10.473

==================== evaluation at iteration: 265 ====================
train total loss: 85.105%
train max loss: 17.424%, reg loss: 75.556%
time spent training so far: 1:10:59.450899
train attack total time: 7.926s
train attack init time: 1.999s
train attack avg grad step time: 0.077s
train attack avg reproj time: 0.190s


train attack loss increase over inner max: 0.888
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000

Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1590]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1593]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1595]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1597]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1612]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1628]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1644]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1660]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1676]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1690]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1702]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1713]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1721]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1729]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1736]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1741]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1745]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1748]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1750]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1752]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1753]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  1.7979050874710083
Parameter containing:
tensor([[0.1754]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1756]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1757]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1759]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1761]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1763]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1764]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1766]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1767]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1768]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1769]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1771]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1773]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1774]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1776]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1779]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1783]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1789]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1795]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1802]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1810]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1817]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1824]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 1.379
total grad norm: 13.527

==================== evaluation at iteration: 266 ====================
train total loss: 86.366%
train max loss: 18.693%, reg loss: 75.503%
time spent training so far: 1:11:07.369003
train attack total time: 7.617s
train attack init time: 1.967s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: -4.574
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.425
total grad norm: 8.434

==================== evaluation at iteration: 267 ====================
train total loss: 84.865%
train max loss: 20.703%, reg loss: 75.489%
time spent training so far: 1:11:15.350316
train attack total time: 7.641s
train attack init time: 2.098s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: 1.683
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.588
total grad norm: 11.649

==================== evaluation at iteration: 268 ====================
train total loss: 86.479%
train max loss: 19.952%, reg loss: 75.544%
time spent training so far: 1:11:23.340390
train attack total time: 7.673s
train attack init time: 2.042s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: -1.097
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.495
total grad norm: 8.546

==================== evaluation at iteration: 269 ====================
train total loss: 85.891%
train max loss: 19.923%, reg loss: 75.552%
time spent training so far: 1:11:30.486216
train attack total time: 6.812s
train attack init time: 2.129s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.141s


train attack loss increase over inner max: 0.005
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.347
total grad norm: 7.748

==================== evaluation at iteration: 270 ====================
train total loss: 85.563%
train max loss: 19.166%, reg loss: 75.515%
time spent training so far: 1:11:37.916261
train attack total time: 7.097s
train attack init time: 1.868s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.056
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.756
total grad norm: 9.633

==================== evaluation at iteration: 271 ====================
train total loss: 84.260%
train max loss: 19.259%, reg loss: 75.585%
time spent training so far: 1:11:45.971278
train attack total time: 7.655s
train attack init time: 1.993s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: -24.178
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.387
total grad norm: 10.478

==================== evaluation at iteration: 272 ====================
train total loss: 86.633%
train max loss: 18.249%, reg loss: 75.669%
time spent training so far: 1:11:54.141818
train attack total time: 7.822s
train attack init time: 2.029s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.193s


train attack loss increase over inner max: -0.885
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.405
total grad norm: 13.793

==================== evaluation at iteration: 273 ====================
train total loss: 84.287%
train max loss: 18.337%, reg loss: 75.689%
time spent training so far: 1:12:02.533632
train attack total time: 8.083s
train attack init time: 2.014s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: -0.335
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.554
total grad norm: 11.541

==================== evaluation at iteration: 274 ====================
train total loss: 84.048%
train max loss: 18.177%, reg loss: 75.629%
time spent training so far: 1:12:10.313371
train attack total time: 7.467s
train attack init time: 2.074s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.174s


train attack loss increase over inner max: -0.318
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.895
total grad norm: 9.282

==================== evaluation at iteration: 275 ====================
train total loss: 85.509%
train max loss: 17.566%, reg loss: 75.646%
time spent training so far: 1:12:18.468821
train attack total time: 7.858s
train attack init time: 2.039s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: -0.401
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 2.680% of volume
percentage infeasible at boundary: 19.68%
mean, std amount infeasible at boundary: 0.69 +/- 2.63
max amount infeasible at boundary: 94.84

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 1.535
total grad norm: 8.378

==================== evaluation at iteration: 276 ====================
train total loss: 84.240%
train max loss: 17.045%, reg loss: 75.586%
time spent training so far: 1:14:09.106391
train attack total time: 7.881s
train attack init time: 2.012s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.192s


train attack loss increase over inner max: -0.046
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.626
total grad norm: 10.510

==================== evaluation at iteration: 277 ====================
train total loss: 87.672%
train max loss: 23.861%, reg loss: 75.833%
time spent training so far: 1:14:16.509931
train attack total time: 7.101s
train attack init time: 2.002s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.160s


train attack loss increase over inner max: 6.971
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.678
total grad norm: 8.447

==================== evaluation at iteration: 278 ====================
train total loss: 86.992%
train max loss: 25.172%, reg loss: 75.593%
time spent training so far: 1:14:24.252410
train attack total time: 7.351s
train attack init time: 1.975s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: 1.391
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.770
total grad norm: 13.953

==================== evaluation at iteration: 279 ====================
train total loss: 86.483%
train max loss: 24.419%, reg loss: 75.774%
time spent training so far: 1:14:31.752538
train attack total time: 7.157s
train attack init time: 2.152s
train attack avg grad step time: 0.063s
train attack avg reproj time: 0.162s


train attack loss increase over inner max: -0.900
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.602
total grad norm: 8.853

==================== evaluation at iteration: 280 ====================
train total loss: 86.253%
train max loss: 23.917%, reg loss: 75.798%
time spent training so far: 1:14:38.719380
train attack total time: 6.639s
train attack init time: 1.924s
train attack avg grad step time: 0.063s
train attack avg reproj time: 0.148s


train attack loss increase over inner max: -1.067
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.617
total grad norm: 8.280

==================== evaluation at iteration: 281 ====================
train total loss: 88.380%
train max loss: 23.359%, reg loss: 75.828%
time spent training so far: 1:14:45.537047
train attack total time: 6.472s
train attack init time: 1.986s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.133s


train attack loss increase over inner max: -0.157
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.981
total grad norm: 7.674

==================== evaluation at iteration: 282 ====================
train total loss: 86.132%
train max loss: 23.060%, reg loss: 75.761%
time spent training so far: 1:14:53.383478
train attack total time: 7.498s
train attack init time: 1.999s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.179s


train attack loss increase over inner max: 0.514
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.542
total grad norm: 11.204

==================== evaluation at iteration: 283 ====================
train total loss: 85.944%
train max loss: 22.124%, reg loss: 75.801%
time spent training so far: 1:15:01.046837
train attack total time: 7.315s
train attack init time: 2.002s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: -0.142
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.558
total grad norm: 10.800

==================== evaluation at iteration: 284 ====================
train total loss: 86.107%
train max loss: 21.041%, reg loss: 75.828%
time spent training so far: 1:15:08.869037
train attack total time: 7.509s
train attack init time: 2.143s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.173s


train attack loss increase over inner max: -0.477
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.843
total grad norm: 7.783

==================== evaluation at iteration: 285 ====================
train total loss: 85.209%
train max loss: 20.346%, reg loss: 75.754%
time spent training so far: 1:15:17.174832
train attack total time: 7.964s
train attack init time: 2.200s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: -0.326
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.754
total grad norm: 8.082

==================== evaluation at iteration: 286 ====================
train total loss: 84.181%
train max loss: 19.676%, reg loss: 75.829%
time spent training so far: 1:15:24.258844
train attack total time: 6.715s
train attack init time: 2.091s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: -0.510
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.988
total grad norm: 8.949

==================== evaluation at iteration: 287 ====================
train total loss: 85.038%
train max loss: 19.254%, reg loss: 75.862%
time spent training so far: 1:15:31.177292
train attack total time: 6.579s
train attack init time: 2.032s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: -0.181
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.010
total grad norm: 10.726

==================== evaluation at iteration: 288 ====================
train total loss: 86.452%
train max loss: 19.886%, reg loss: 75.950%
time spent training so far: 1:15:39.201095
train attack total time: 7.766s
train attack init time: 1.975s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: 1.330
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.305
total grad norm: 16.937

==================== evaluation at iteration: 289 ====================
train total loss: 86.600%
train max loss: 19.881%, reg loss: 75.943%
time spent training so far: 1:15:47.526283
train attack total time: 8.018s
train attack init time: 2.143s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: 0.152
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.039
total grad norm: 9.750

==================== evaluation at iteration: 290 ====================
train total loss: 84.835%
train max loss: 18.902%, reg loss: 75.899%
time spent training so far: 1:15:55.081600
train attack total time: 7.251s
train attack init time: 1.973s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: -0.548
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.805
total grad norm: 12.061

==================== evaluation at iteration: 291 ====================
train total loss: 83.883%
train max loss: 18.801%, reg loss: 75.846%
time spent training so far: 1:16:02.085440
train attack total time: 6.634s
train attack init time: 1.980s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.143s


train attack loss increase over inner max: 0.015
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.077
total grad norm: 11.829

==================== evaluation at iteration: 292 ====================
train total loss: 84.310%
train max loss: 18.653%, reg loss: 75.939%
time spent training so far: 1:16:09.545101
train attack total time: 7.178s
train attack init time: 2.016s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: -0.451
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.084
total grad norm: 12.326

==================== evaluation at iteration: 293 ====================
train total loss: 85.415%
train max loss: 18.412%, reg loss: 75.910%
time spent training so far: 1:16:16.923654
train attack total time: 7.014s
train attack init time: 2.109s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.151s


train attack loss increase over inner max: 0.965
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.053
total grad norm: 8.611

==================== evaluation at iteration: 294 ====================
train total loss: 84.585%
train max loss: 20.090%, reg loss: 76.006%
time spent training so far: 1:16:24.900000
train attack total time: 7.668s
train attack init time: 2.102s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.176s


train attack loss increase over inner max: 2.645
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.826
total grad norm: 12.015

==================== evaluation at iteration: 295 ====================
train total loss: 84.661%
train max loss: 19.871%, reg loss: 76.124%
time spent training so far: 1:16:33.071359
train attack total time: 7.844s
train attack init time: 2.099s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.188s


train attack loss increase over inner max: -0.363
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.472
total grad norm: 17.287

==================== evaluation at iteration: 296 ====================
train total loss: 83.949%
train max loss: 17.158%, reg loss: 76.091%
time spent training so far: 1:16:42.364048
train attack total time: 8.930s
train attack init time: 1.970s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.245s


train attack loss increase over inner max: -1.369
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.015
total grad norm: 10.680

==================== evaluation at iteration: 297 ====================
train total loss: 84.394%
train max loss: 17.028%, reg loss: 76.042%
time spent training so far: 1:16:50.183982
train attack total time: 7.484s
train attack init time: 2.130s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: -1.039
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.298
total grad norm: 12.074

==================== evaluation at iteration: 298 ====================
train total loss: 82.732%
train max loss: 15.136%, reg loss: 76.051%
time spent training so far: 1:16:57.844289
train attack total time: 7.364s
train attack init time: 2.057s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 0.149
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.122
total grad norm: 10.067

==================== evaluation at iteration: 299 ====================
train total loss: 84.268%
train max loss: 17.596%, reg loss: 76.112%
time spent training so far: 1:17:04.164207
train attack total time: 6.047s
train attack init time: 1.899s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.121s


train attack loss increase over inner max: 2.394
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.431
total grad norm: 9.990

==================== evaluation at iteration: 300 ====================
train total loss: 84.255%
train max loss: 18.407%, reg loss: 76.189%
time spent training so far: 1:17:11.872199
train attack total time: 7.438s
train attack init time: 2.032s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.175s


train attack loss increase over inner max: 0.525
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 1.760% of volume
percentage infeasible at boundary: 13.60%
mean, std amount infeasible at boundary: 0.40 +/- 1.41
max amount infeasible at boundary: 15.94

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 2.334
total grad norm: 11.018

==================== evaluation at iteration: 301 ====================
train total loss: 83.468%
train max loss: 17.343%, reg loss: 76.160%
time spent training so far: 1:19:09.464840
train attack total time: 9.111s
train attack init time: 2.144s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: -0.387
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.392
total grad norm: 11.578

==================== evaluation at iteration: 302 ====================
train total loss: 83.758%
train max loss: 17.335%, reg loss: 76.204%
time spent training so far: 1:19:20.543317
train attack total time: 10.617s
train attack init time: 3.327s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.240s


train attack loss increase over inner max: -0.056
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.458
total grad norm: 13.119

==================== evaluation at iteration: 303 ====================
train total loss: 84.079%
train max loss: 16.222%, reg loss: 76.206%
time spent training so far: 1:19:30.362757
train attack total time: 9.351s
train attack init time: 3.360s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.183s


train attack loss increase over inner max: 0.488
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.710
total grad norm: 9.097

==================== evaluation at iteration: 304 ====================
train total loss: 83.139%
train max loss: 15.467%, reg loss: 76.400%
time spent training so far: 1:19:40.815459
train attack total time: 9.995s
train attack init time: 3.414s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.203s


train attack loss increase over inner max: -0.429
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

