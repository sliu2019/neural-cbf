nohup: ignoring input
problem          : flying_inv_pend
reg_weight       : 150.0
reg_n_samples    : 250
critic_n_samples : 50
critic_max_n_steps : 20
test_N_volume_samples : 2500
test_N_boundary_samples : 2500
learner_stopping_condition : n_steps
learner_early_stopping_patience : 100
learner_n_steps  : 3000
learner_lr       : 0.001
random_seed      : 1
affix            : repro_after_round1_edits
log_root         : log
model_root       : checkpoint
n_checkpoint_step : 5
n_test_loss_step : 25
gpu              : 0
log_folder       : log/flying_inv_pend_repro_after_round1_edits
model_folder     : checkpoint/flying_inv_pend_repro_after_round1_edits
Reg grad norm: 0.423
total grad norm: 6.321

==================== evaluation at iteration: 0 ====================
train total loss: 95.123%
train max loss: 27.705%, reg loss: 75.574%
time spent training so far: 0:00:26.928259
train attack total time: 26.588s
train attack init time: 2.897s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.656s


train attack loss increase over inner max: 14.579
OOM debug. Mem allocated and reserved: 727552.000000, 2097152.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.680% of volume
percentage infeasible at boundary: 31.40%
mean, std amount infeasible at boundary: 2.63 +/- 5.34
max amount infeasible at boundary: 36.79

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.531
total grad norm: 7.073

==================== evaluation at iteration: 1 ====================
train total loss: 97.868%
train max loss: 35.178%, reg loss: 75.420%
time spent training so far: 0:03:18.968423
train attack total time: 16.707s
train attack init time: 3.016s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.354s


train attack loss increase over inner max: 4.080
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.546
total grad norm: 7.395

==================== evaluation at iteration: 2 ====================
train total loss: 96.530%
train max loss: 30.227%, reg loss: 75.553%
time spent training so far: 0:03:40.838101
train attack total time: 21.471s
train attack init time: 3.132s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.506s


train attack loss increase over inner max: -1.363
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.586
total grad norm: 5.284

==================== evaluation at iteration: 3 ====================
train total loss: 91.956%
train max loss: 26.282%, reg loss: 75.499%
time spent training so far: 0:04:03.256081
train attack total time: 22.060s
train attack init time: 2.991s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.530s


train attack loss increase over inner max: -0.551
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.590
total grad norm: 10.047

==================== evaluation at iteration: 4 ====================
train total loss: 89.201%
train max loss: 24.792%, reg loss: 75.138%
time spent training so far: 0:04:19.773115
train attack total time: 16.200s
train attack init time: 2.849s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.343s


train attack loss increase over inner max: -144.454
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.583
total grad norm: 6.461

==================== evaluation at iteration: 5 ====================
train total loss: 91.001%
train max loss: 24.095%, reg loss: 75.658%
time spent training so far: 0:04:38.180424
train attack total time: 18.067s
train attack init time: 2.770s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.407s


train attack loss increase over inner max: 3.498
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.602
total grad norm: 5.412

==================== evaluation at iteration: 6 ====================
train total loss: 88.917%
train max loss: 21.087%, reg loss: 75.400%
time spent training so far: 0:05:00.371978
train attack total time: 21.847s
train attack init time: 3.023s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.521s


train attack loss increase over inner max: -1.473
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.694
total grad norm: 5.529

==================== evaluation at iteration: 7 ====================
train total loss: 88.021%
train max loss: 22.650%, reg loss: 75.273%
time spent training so far: 0:05:14.647275
train attack total time: 13.884s
train attack init time: 2.611s
train attack avg grad step time: 0.081s
train attack avg reproj time: 0.280s


train attack loss increase over inner max: 3.877
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.645
total grad norm: 5.673

==================== evaluation at iteration: 8 ====================
train total loss: 86.448%
train max loss: 20.647%, reg loss: 75.251%
time spent training so far: 0:05:30.209513
train attack total time: 15.200s
train attack init time: 2.968s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.321s


train attack loss increase over inner max: -0.393
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.706
total grad norm: 4.038

==================== evaluation at iteration: 9 ====================
train total loss: 88.860%
train max loss: 21.069%, reg loss: 75.413%
time spent training so far: 0:05:44.022066
train attack total time: 13.429s
train attack init time: 2.835s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.265s


train attack loss increase over inner max: 0.978
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.723
total grad norm: 5.688

==================== evaluation at iteration: 10 ====================
train total loss: 86.209%
train max loss: 20.196%, reg loss: 75.219%
time spent training so far: 0:05:57.927199
train attack total time: 13.495s
train attack init time: 2.492s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.281s


train attack loss increase over inner max: -1.276
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.775
total grad norm: 5.300

==================== evaluation at iteration: 11 ====================
train total loss: 86.266%
train max loss: 19.387%, reg loss: 75.379%
time spent training so far: 0:06:08.555536
train attack total time: 10.283s
train attack init time: 2.331s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.181s


train attack loss increase over inner max: -0.570
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.702
total grad norm: 6.653

==================== evaluation at iteration: 12 ====================
train total loss: 87.386%
train max loss: 19.507%, reg loss: 75.450%
time spent training so far: 0:06:24.002265
train attack total time: 15.054s
train attack init time: 2.464s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.335s


train attack loss increase over inner max: -0.299
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.952
total grad norm: 6.854

==================== evaluation at iteration: 13 ====================
train total loss: 88.304%
train max loss: 22.238%, reg loss: 75.380%
time spent training so far: 0:06:39.356976
train attack total time: 15.014s
train attack init time: 2.230s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.341s


train attack loss increase over inner max: 2.916
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.008
total grad norm: 10.503

==================== evaluation at iteration: 14 ====================
train total loss: 87.932%
train max loss: 21.576%, reg loss: 75.408%
time spent training so far: 0:06:54.740072
train attack total time: 15.043s
train attack init time: 2.719s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: -0.272
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.737
total grad norm: 9.377

==================== evaluation at iteration: 15 ====================
train total loss: 87.474%
train max loss: 21.218%, reg loss: 75.255%
time spent training so far: 0:07:06.686778
train attack total time: 11.600s
train attack init time: 2.213s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.227s


train attack loss increase over inner max: -0.275
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.803
total grad norm: 12.688

==================== evaluation at iteration: 16 ====================
train total loss: 88.902%
train max loss: 20.127%, reg loss: 75.347%
time spent training so far: 0:07:21.258964
train attack total time: 14.225s
train attack init time: 2.524s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.304s


train attack loss increase over inner max: -0.678
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.663
total grad norm: 6.272

==================== evaluation at iteration: 17 ====================
train total loss: 86.912%
train max loss: 21.365%, reg loss: 75.277%
time spent training so far: 0:07:34.621116
train attack total time: 12.975s
train attack init time: 2.706s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.269s


train attack loss increase over inner max: 0.809
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.667
total grad norm: 7.172

==================== evaluation at iteration: 18 ====================
train total loss: 87.260%
train max loss: 23.249%, reg loss: 75.257%
time spent training so far: 0:07:56.980847
train attack total time: 21.990s
train attack init time: 2.839s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.572s


train attack loss increase over inner max: 1.341
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.592
total grad norm: 6.587

==================== evaluation at iteration: 19 ====================
train total loss: 87.494%
train max loss: 23.954%, reg loss: 75.342%
time spent training so far: 0:08:07.012430
train attack total time: 9.705s
train attack init time: 2.459s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.164s


train attack loss increase over inner max: 2.440
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.586
total grad norm: 10.183

==================== evaluation at iteration: 20 ====================
train total loss: 87.617%
train max loss: 25.611%, reg loss: 75.276%
time spent training so far: 0:08:30.496629
train attack total time: 23.139s
train attack init time: 2.808s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.615s


train attack loss increase over inner max: 2.340
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.530
total grad norm: 7.385

==================== evaluation at iteration: 21 ====================
train total loss: 88.407%
train max loss: 26.318%, reg loss: 75.291%
time spent training so far: 0:08:43.092739
train attack total time: 12.219s
train attack init time: 2.337s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.254s


train attack loss increase over inner max: 2.432
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.411
total grad norm: 7.216

==================== evaluation at iteration: 22 ====================
train total loss: 89.210%
train max loss: 24.659%, reg loss: 75.235%
time spent training so far: 0:08:57.133702
train attack total time: 13.676s
train attack init time: 2.543s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.296s


train attack loss increase over inner max: 0.144
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.447
total grad norm: 5.608

==================== evaluation at iteration: 23 ====================
train total loss: 87.742%
train max loss: 22.391%, reg loss: 75.390%
time spent training so far: 0:09:14.880297
train attack total time: 17.405s
train attack init time: 2.806s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.417s


train attack loss increase over inner max: -48.520
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.376
total grad norm: 6.747

==================== evaluation at iteration: 24 ====================
train total loss: 88.366%
train max loss: 24.428%, reg loss: 75.292%
time spent training so far: 0:09:29.465861
train attack total time: 14.249s
train attack init time: 2.400s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.321s


train attack loss increase over inner max: 2.524
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.387
total grad norm: 5.806

==================== evaluation at iteration: 25 ====================
train total loss: 87.873%
train max loss: 22.538%, reg loss: 75.252%
time spent training so far: 0:09:40.631951
train attack total time: 10.827s
train attack init time: 2.795s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.188s


train attack loss increase over inner max: -1.572
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 5.200% of volume
percentage infeasible at boundary: 28.88%
mean, std amount infeasible at boundary: 1.46 +/- 3.14
max amount infeasible at boundary: 42.19

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.287
total grad norm: 3.512

==================== evaluation at iteration: 26 ====================
train total loss: 86.334%
train max loss: 21.186%, reg loss: 75.042%
time spent training so far: 0:12:05.918000
train attack total time: 16.171s
train attack init time: 2.450s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.385s


train attack loss increase over inner max: -0.550
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.286
total grad norm: 4.399

==================== evaluation at iteration: 27 ====================
train total loss: 86.679%
train max loss: 19.577%, reg loss: 75.365%
time spent training so far: 0:12:20.762565
train attack total time: 14.485s
train attack init time: 2.272s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.349s


train attack loss increase over inner max: -1.871
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.274
total grad norm: 4.909

==================== evaluation at iteration: 28 ====================
train total loss: 86.563%
train max loss: 18.769%, reg loss: 75.087%
time spent training so far: 0:12:35.548324
train attack total time: 14.421s
train attack init time: 2.199s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.349s


train attack loss increase over inner max: -0.953
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.219
total grad norm: 3.897

==================== evaluation at iteration: 29 ====================
train total loss: 87.481%
train max loss: 19.648%, reg loss: 75.120%
time spent training so far: 0:12:48.439550
train attack total time: 12.555s
train attack init time: 2.636s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: 0.901
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.176
total grad norm: 4.011

==================== evaluation at iteration: 30 ====================
train total loss: 87.660%
train max loss: 22.448%, reg loss: 74.962%
time spent training so far: 0:13:04.432798
train attack total time: 15.633s
train attack init time: 2.415s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.385s


train attack loss increase over inner max: 2.955
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.185
total grad norm: 2.947

==================== evaluation at iteration: 31 ====================
train total loss: 86.854%
train max loss: 22.097%, reg loss: 75.259%
time spent training so far: 0:13:18.256128
train attack total time: 13.464s
train attack init time: 2.550s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: -0.278
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.153
total grad norm: 3.766

==================== evaluation at iteration: 32 ====================
train total loss: 87.044%
train max loss: 21.445%, reg loss: 75.091%
time spent training so far: 0:13:31.184782
train attack total time: 12.587s
train attack init time: 2.327s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.278
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.157
total grad norm: 3.826

==================== evaluation at iteration: 33 ====================
train total loss: 88.526%
train max loss: 21.016%, reg loss: 75.068%
time spent training so far: 0:13:44.408565
train attack total time: 12.858s
train attack init time: 2.857s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.481
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.158
total grad norm: 3.847

==================== evaluation at iteration: 34 ====================
train total loss: 87.385%
train max loss: 20.795%, reg loss: 75.076%
time spent training so far: 0:13:56.406718
train attack total time: 11.610s
train attack init time: 2.698s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.231s


train attack loss increase over inner max: 0.016
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.193
total grad norm: 2.926

==================== evaluation at iteration: 35 ====================
train total loss: 86.942%
train max loss: 20.335%, reg loss: 75.063%
time spent training so far: 0:14:08.854254
train attack total time: 12.097s
train attack init time: 2.450s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: -0.184
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.175
total grad norm: 3.259

==================== evaluation at iteration: 36 ====================
train total loss: 86.063%
train max loss: 19.533%, reg loss: 74.988%
time spent training so far: 0:14:20.842594
train attack total time: 11.590s
train attack init time: 2.544s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.237s


train attack loss increase over inner max: -0.430
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.241
total grad norm: 4.673

==================== evaluation at iteration: 37 ====================
train total loss: 86.251%
train max loss: 19.211%, reg loss: 75.338%
time spent training so far: 0:14:32.262721
train attack total time: 11.077s
train attack init time: 2.610s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -0.425
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.251
total grad norm: 3.293

==================== evaluation at iteration: 38 ====================
train total loss: 86.594%
train max loss: 18.767%, reg loss: 75.182%
time spent training so far: 0:14:47.239946
train attack total time: 14.578s
train attack init time: 2.554s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.343s


train attack loss increase over inner max: -0.056
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.259
total grad norm: 4.939

==================== evaluation at iteration: 39 ====================
train total loss: 87.254%
train max loss: 21.311%, reg loss: 75.213%
time spent training so far: 0:14:59.802943
train attack total time: 12.178s
train attack init time: 2.424s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.275s


train attack loss increase over inner max: 2.483
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.311
total grad norm: 2.654

==================== evaluation at iteration: 40 ====================
train total loss: 85.270%
train max loss: 20.444%, reg loss: 75.257%
time spent training so far: 0:15:12.512505
train attack total time: 12.350s
train attack init time: 2.473s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.400
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.327
total grad norm: 3.889

==================== evaluation at iteration: 41 ====================
train total loss: 87.574%
train max loss: 19.580%, reg loss: 75.283%
time spent training so far: 0:15:28.180932
train attack total time: 15.301s
train attack init time: 2.481s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.390s


train attack loss increase over inner max: -0.550
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.354
total grad norm: 7.720

==================== evaluation at iteration: 42 ====================
train total loss: 85.095%
train max loss: 18.528%, reg loss: 75.147%
time spent training so far: 0:15:39.405754
train attack total time: 10.861s
train attack init time: 2.355s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.231s


train attack loss increase over inner max: -0.735
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.354
total grad norm: 4.394

==================== evaluation at iteration: 43 ====================
train total loss: 86.666%
train max loss: 22.874%, reg loss: 75.153%
time spent training so far: 0:15:50.607264
train attack total time: 10.849s
train attack init time: 2.326s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.230s


train attack loss increase over inner max: 4.907
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
At initialization: k0 is 0.002793
Max_n_steps: 30
Parameter containing:
tensor([[0.0038]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0066]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 29
Parameter containing:
tensor([[0.0048]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0056]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0058]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0046]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0068]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0036]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Attack: reprojection exited on timeout, max dist from =0 boundary:  3.6879425048828125
Parameter containing:
tensor([[0.0078]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0026]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0088]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0016]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 29
Parameter containing:
tensor([[0.0098]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0006]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0108]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0118]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0128]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0138]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 28
Parameter containing:
tensor([[0.0148]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0157]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0168]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0178]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0188]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 28
Parameter containing:
tensor([[0.0198]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0209]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0219]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0229]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0239]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 27
Parameter containing:
tensor([[0.0248]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0257]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Attack: reprojection exited on timeout, max dist from =0 boundary:  3.576791763305664
Parameter containing:
tensor([[0.0267]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0275]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0284]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 27
Parameter containing:
tensor([[0.0292]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0300]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0307]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0314]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0321]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 26
Parameter containing:
tensor([[0.0328]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0334]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0340]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0345]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0350]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 26
Parameter containing:
tensor([[0.0355]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0360]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0365]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0369]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0373]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 25
Parameter containing:
tensor([[0.0377]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0382]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0386]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.251
total grad norm: 5.458

==================== evaluation at iteration: 44 ====================
train total loss: 86.114%
train max loss: 22.346%, reg loss: 75.073%
time spent training so far: 0:16:04.519015
train attack total time: 13.582s
train attack init time: 2.331s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: -0.992
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.355
total grad norm: 4.315

==================== evaluation at iteration: 45 ====================
train total loss: 85.752%
train max loss: 20.906%, reg loss: 75.088%
time spent training so far: 0:16:17.776688
train attack total time: 12.878s
train attack init time: 2.494s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.295s


train attack loss increase over inner max: -1.323
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.253
total grad norm: 5.949

==================== evaluation at iteration: 46 ====================
train total loss: 87.168%
train max loss: 19.681%, reg loss: 75.056%
time spent training so far: 0:16:32.615146
train attack total time: 14.454s
train attack init time: 2.432s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.359s


train attack loss increase over inner max: -1.455
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.274
total grad norm: 2.407

==================== evaluation at iteration: 47 ====================
train total loss: 85.333%
train max loss: 20.691%, reg loss: 75.102%
time spent training so far: 0:16:47.645925
train attack total time: 14.638s
train attack init time: 2.476s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.363s


train attack loss increase over inner max: 1.164
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.275
total grad norm: 2.362

==================== evaluation at iteration: 48 ====================
train total loss: 86.569%
train max loss: 21.298%, reg loss: 75.194%
time spent training so far: 0:16:59.620104
train attack total time: 11.610s
train attack init time: 2.441s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.254s


train attack loss increase over inner max: 0.295
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.257
total grad norm: 3.086

==================== evaluation at iteration: 49 ====================
train total loss: 85.829%
train max loss: 20.846%, reg loss: 75.113%
time spent training so far: 0:17:09.402219
train attack total time: 9.453s
train attack init time: 2.122s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.185s


train attack loss increase over inner max: -0.290
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.290
total grad norm: 4.132

==================== evaluation at iteration: 50 ====================
train total loss: 86.414%
train max loss: 19.537%, reg loss: 75.175%
time spent training so far: 0:17:25.239987
train attack total time: 15.502s
train attack init time: 2.533s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.395s


train attack loss increase over inner max: -1.590
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 6.360% of volume
percentage infeasible at boundary: 26.88%
mean, std amount infeasible at boundary: 1.29 +/- 2.91
max amount infeasible at boundary: 26.75

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.281
total grad norm: 6.248

==================== evaluation at iteration: 51 ====================
train total loss: 86.625%
train max loss: 20.878%, reg loss: 75.073%
time spent training so far: 0:19:47.592838
train attack total time: 16.750s
train attack init time: 2.642s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.432s


train attack loss increase over inner max: 1.160
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.321
total grad norm: 4.101

==================== evaluation at iteration: 52 ====================
train total loss: 87.526%
train max loss: 20.732%, reg loss: 75.022%
time spent training so far: 0:20:01.639010
train attack total time: 13.633s
train attack init time: 2.594s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.337s


train attack loss increase over inner max: -0.436
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.295
total grad norm: 5.807

==================== evaluation at iteration: 53 ====================
train total loss: 87.487%
train max loss: 20.396%, reg loss: 74.971%
time spent training so far: 0:20:14.592466
train attack total time: 12.587s
train attack init time: 2.901s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.285s


train attack loss increase over inner max: -0.562
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.294
total grad norm: 6.760

==================== evaluation at iteration: 54 ====================
train total loss: 89.026%
train max loss: 25.423%, reg loss: 75.186%
time spent training so far: 0:20:30.071483
train attack total time: 15.080s
train attack init time: 2.495s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.397s


train attack loss increase over inner max: 4.845
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.316
total grad norm: 3.496

==================== evaluation at iteration: 55 ====================
train total loss: 88.131%
train max loss: 25.320%, reg loss: 75.120%
time spent training so far: 0:20:41.603473
train attack total time: 11.192s
train attack init time: 2.213s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: -0.009
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.325
total grad norm: 3.514

==================== evaluation at iteration: 56 ====================
train total loss: 88.635%
train max loss: 24.781%, reg loss: 75.135%
time spent training so far: 0:20:53.933287
train attack total time: 11.990s
train attack init time: 2.556s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.275s


train attack loss increase over inner max: -0.151
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.302
total grad norm: 2.571

==================== evaluation at iteration: 57 ====================
train total loss: 87.489%
train max loss: 24.433%, reg loss: 75.361%
time spent training so far: 0:21:06.231464
train attack total time: 11.943s
train attack init time: 2.316s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.285s


train attack loss increase over inner max: -0.464
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.353
total grad norm: 3.989

==================== evaluation at iteration: 58 ====================
train total loss: 88.105%
train max loss: 23.953%, reg loss: 75.121%
time spent training so far: 0:21:20.046247
train attack total time: 13.443s
train attack init time: 2.318s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.340s


train attack loss increase over inner max: -0.741
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.342
total grad norm: 2.438

==================== evaluation at iteration: 59 ====================
train total loss: 87.710%
train max loss: 22.741%, reg loss: 75.415%
time spent training so far: 0:21:31.315377
train attack total time: 10.943s
train attack init time: 2.014s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.257s


train attack loss increase over inner max: -1.263
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.323
total grad norm: 5.674

==================== evaluation at iteration: 60 ====================
train total loss: 86.548%
train max loss: 21.067%, reg loss: 75.196%
time spent training so far: 0:21:46.106268
train attack total time: 14.440s
train attack init time: 2.598s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.367s


train attack loss increase over inner max: -2.047
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.338
total grad norm: 5.265

==================== evaluation at iteration: 61 ====================
train total loss: 86.446%
train max loss: 19.691%, reg loss: 75.337%
time spent training so far: 0:22:02.727838
train attack total time: 16.241s
train attack init time: 2.843s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.424s


train attack loss increase over inner max: -1.326
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.219
total grad norm: 2.554

==================== evaluation at iteration: 62 ====================
train total loss: 85.583%
train max loss: 19.651%, reg loss: 75.144%
time spent training so far: 0:22:16.754691
train attack total time: 13.675s
train attack init time: 2.955s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.325s


train attack loss increase over inner max: -0.183
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.230
total grad norm: 3.295

==================== evaluation at iteration: 63 ====================
train total loss: 85.473%
train max loss: 18.575%, reg loss: 75.268%
time spent training so far: 0:22:32.828720
train attack total time: 15.694s
train attack init time: 2.699s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.413s


train attack loss increase over inner max: -0.984
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.150
total grad norm: 3.247

==================== evaluation at iteration: 64 ====================
train total loss: 85.978%
train max loss: 18.116%, reg loss: 74.991%
time spent training so far: 0:22:46.511940
train attack total time: 13.344s
train attack init time: 2.410s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.333s


train attack loss increase over inner max: 0.032
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.158
total grad norm: 2.888

==================== evaluation at iteration: 65 ====================
train total loss: 85.866%
train max loss: 17.906%, reg loss: 75.092%
time spent training so far: 0:23:01.494124
train attack total time: 14.631s
train attack init time: 2.431s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.383s


train attack loss increase over inner max: -0.158
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.148
total grad norm: 2.957

==================== evaluation at iteration: 66 ====================
train total loss: 86.603%
train max loss: 19.717%, reg loss: 75.007%
time spent training so far: 0:23:16.005795
train attack total time: 14.162s
train attack init time: 2.217s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: 2.150
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.135
total grad norm: 4.710

==================== evaluation at iteration: 67 ====================
train total loss: 84.885%
train max loss: 19.029%, reg loss: 75.027%
time spent training so far: 0:23:28.249061
train attack total time: 11.902s
train attack init time: 2.137s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.288s


train attack loss increase over inner max: -0.391
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.135
total grad norm: 3.208

==================== evaluation at iteration: 68 ====================
train total loss: 85.171%
train max loss: 18.153%, reg loss: 75.116%
time spent training so far: 0:23:39.511643
train attack total time: 10.919s
train attack init time: 2.484s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.239s


train attack loss increase over inner max: -0.609
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.188
total grad norm: 4.637

==================== evaluation at iteration: 69 ====================
train total loss: 86.948%
train max loss: 20.894%, reg loss: 75.130%
time spent training so far: 0:23:58.499016
train attack total time: 18.654s
train attack init time: 2.165s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.572s


train attack loss increase over inner max: 2.989
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.188
total grad norm: 5.223

==================== evaluation at iteration: 70 ====================
train total loss: 87.166%
train max loss: 23.147%, reg loss: 75.054%
time spent training so far: 0:24:09.523063
train attack total time: 10.698s
train attack init time: 2.140s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.255s


train attack loss increase over inner max: 1.916
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.237
total grad norm: 5.893

==================== evaluation at iteration: 71 ====================
train total loss: 88.667%
train max loss: 23.237%, reg loss: 75.114%
time spent training so far: 0:24:23.936006
train attack total time: 14.021s
train attack init time: 2.233s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.385s


train attack loss increase over inner max: -0.025
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.241
total grad norm: 3.922

==================== evaluation at iteration: 72 ====================
train total loss: 87.652%
train max loss: 22.074%, reg loss: 75.158%
time spent training so far: 0:24:38.605457
train attack total time: 14.314s
train attack init time: 2.294s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: -0.213
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.270
total grad norm: 5.616

==================== evaluation at iteration: 73 ====================
train total loss: 86.984%
train max loss: 21.109%, reg loss: 74.987%
time spent training so far: 0:24:49.932236
train attack total time: 10.955s
train attack init time: 2.184s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: -0.992
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.214
total grad norm: 3.253

==================== evaluation at iteration: 74 ====================
train total loss: 87.043%
train max loss: 20.740%, reg loss: 75.002%
time spent training so far: 0:25:01.109202
train attack total time: 10.836s
train attack init time: 2.174s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: -0.551
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.224
total grad norm: 2.925

==================== evaluation at iteration: 75 ====================
train total loss: 86.525%
train max loss: 20.202%, reg loss: 75.100%
time spent training so far: 0:25:12.172200
train attack total time: 10.671s
train attack init time: 2.266s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.249s


train attack loss increase over inner max: -0.540
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.920% of volume
percentage infeasible at boundary: 25.52%
mean, std amount infeasible at boundary: 1.26 +/- 2.94
max amount infeasible at boundary: 23.41

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.287
total grad norm: 3.250

==================== evaluation at iteration: 76 ====================
train total loss: 87.456%
train max loss: 19.832%, reg loss: 74.995%
time spent training so far: 0:27:32.962834
train attack total time: 12.530s
train attack init time: 2.479s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.314s


train attack loss increase over inner max: -0.789
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.239
total grad norm: 3.942

==================== evaluation at iteration: 77 ====================
train total loss: 84.899%
train max loss: 18.976%, reg loss: 75.105%
time spent training so far: 0:27:44.973952
train attack total time: 11.658s
train attack init time: 2.689s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.827
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.247
total grad norm: 4.524

==================== evaluation at iteration: 78 ====================
train total loss: 85.018%
train max loss: 18.680%, reg loss: 75.061%
time spent training so far: 0:27:58.218599
train attack total time: 12.864s
train attack init time: 3.068s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: -0.583
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.237
total grad norm: 4.808

==================== evaluation at iteration: 79 ====================
train total loss: 87.356%
train max loss: 20.540%, reg loss: 75.071%
time spent training so far: 0:28:10.876819
train attack total time: 12.211s
train attack init time: 2.382s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.300s


train attack loss increase over inner max: 2.400
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.194
total grad norm: 2.660

==================== evaluation at iteration: 80 ====================
train total loss: 86.959%
train max loss: 22.725%, reg loss: 74.974%
time spent training so far: 0:28:24.238566
train attack total time: 12.926s
train attack init time: 2.439s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: 2.656
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.251
total grad norm: 4.517

==================== evaluation at iteration: 81 ====================
train total loss: 85.529%
train max loss: 22.042%, reg loss: 75.132%
time spent training so far: 0:28:35.820341
train attack total time: 11.194s
train attack init time: 2.510s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.260s


train attack loss increase over inner max: -0.297
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.192
total grad norm: 4.245

==================== evaluation at iteration: 82 ====================
train total loss: 87.300%
train max loss: 21.468%, reg loss: 75.255%
time spent training so far: 0:28:48.612358
train attack total time: 12.413s
train attack init time: 2.689s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.298s


train attack loss increase over inner max: -0.461
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.256
total grad norm: 4.755

==================== evaluation at iteration: 83 ====================
train total loss: 88.429%
train max loss: 20.937%, reg loss: 75.198%
time spent training so far: 0:29:01.421951
train attack total time: 12.419s
train attack init time: 2.723s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.299s


train attack loss increase over inner max: -0.113
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.246
total grad norm: 3.607

==================== evaluation at iteration: 84 ====================
train total loss: 86.144%
train max loss: 20.547%, reg loss: 75.078%
time spent training so far: 0:29:14.660892
train attack total time: 12.847s
train attack init time: 2.683s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.316s


train attack loss increase over inner max: -0.602
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.266
total grad norm: 4.078

==================== evaluation at iteration: 85 ====================
train total loss: 86.551%
train max loss: 20.048%, reg loss: 75.213%
time spent training so far: 0:29:26.451866
train attack total time: 11.409s
train attack init time: 2.384s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.271s


train attack loss increase over inner max: -0.403
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.288
total grad norm: 4.470

==================== evaluation at iteration: 86 ====================
train total loss: 87.211%
train max loss: 20.534%, reg loss: 75.220%
time spent training so far: 0:29:38.708904
train attack total time: 11.882s
train attack init time: 2.622s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: 0.771
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.327
total grad norm: 5.097

==================== evaluation at iteration: 87 ====================
train total loss: 88.216%
train max loss: 20.555%, reg loss: 75.270%
time spent training so far: 0:29:51.333679
train attack total time: 12.250s
train attack init time: 2.393s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.305s


train attack loss increase over inner max: -0.563
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000

Max_n_steps: 25
Parameter containing:
tensor([[0.0390]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0395]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 25
Parameter containing:
tensor([[0.0399]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0404]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0408]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0412]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.11706492304801941
Parameter containing:
tensor([[0.0416]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 25
Parameter containing:
tensor([[0.0420]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0423]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0427]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0431]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0435]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 24
Parameter containing:
tensor([[0.0438]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0441]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0444]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0447]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0449]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 24
Parameter containing:
tensor([[0.0452]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.1189926415681839
Parameter containing:
tensor([[0.0456]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0459]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0462]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0464]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 24
Parameter containing:
tensor([[0.0467]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0470]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0473]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0476]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0478]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Parameter containing:
tensor([[0.0481]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0484]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0486]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0489]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0492]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.14134429395198822
Parameter containing:
tensor([[0.0494]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0496]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.2101573795080185
Parameter containing:
tensor([[0.0499]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0501]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0503]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Parameter containing:
tensor([[0.0505]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0506]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0507]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0508]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0509]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Parameter containing:
tensor([[0.0510]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0511]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.353
total grad norm: 3.105

==================== evaluation at iteration: 88 ====================
train total loss: 87.095%
train max loss: 20.105%, reg loss: 75.092%
time spent training so far: 0:30:06.012284
train attack total time: 14.297s
train attack init time: 2.699s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: 0.140
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.345
total grad norm: 4.387

==================== evaluation at iteration: 89 ====================
train total loss: 86.245%
train max loss: 18.743%, reg loss: 75.154%
time spent training so far: 0:30:17.999934
train attack total time: 11.578s
train attack init time: 2.726s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: -0.691
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.290
total grad norm: 3.715

==================== evaluation at iteration: 90 ====================
train total loss: 85.137%
train max loss: 17.876%, reg loss: 75.074%
time spent training so far: 0:30:30.832258
train attack total time: 12.432s
train attack init time: 3.012s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.287s


train attack loss increase over inner max: -1.179
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.316
total grad norm: 5.308

==================== evaluation at iteration: 91 ====================
train total loss: 86.383%
train max loss: 19.279%, reg loss: 75.071%
time spent training so far: 0:30:41.608646
train attack total time: 10.431s
train attack init time: 2.342s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.251s


train attack loss increase over inner max: 1.071
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.270
total grad norm: 3.308

==================== evaluation at iteration: 92 ====================
train total loss: 86.993%
train max loss: 19.725%, reg loss: 75.062%
time spent training so far: 0:30:51.864165
train attack total time: 9.930s
train attack init time: 2.413s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.225s


train attack loss increase over inner max: 0.844
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.209
total grad norm: 3.588

==================== evaluation at iteration: 93 ====================
train total loss: 87.242%
train max loss: 19.850%, reg loss: 75.010%
time spent training so far: 0:31:04.836728
train attack total time: 12.643s
train attack init time: 2.236s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.347s


train attack loss increase over inner max: -0.245
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.205
total grad norm: 3.681

==================== evaluation at iteration: 94 ====================
train total loss: 85.882%
train max loss: 20.074%, reg loss: 75.082%
time spent training so far: 0:31:18.147766
train attack total time: 12.987s
train attack init time: 2.145s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.360s


train attack loss increase over inner max: 0.384
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.201
total grad norm: 4.145

==================== evaluation at iteration: 95 ====================
train total loss: 85.754%
train max loss: 19.321%, reg loss: 75.104%
time spent training so far: 0:31:29.914110
train attack total time: 11.410s
train attack init time: 2.307s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.292s


train attack loss increase over inner max: -0.501
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.124
total grad norm: 2.009

==================== evaluation at iteration: 96 ====================
train total loss: 87.182%
train max loss: 19.701%, reg loss: 75.018%
time spent training so far: 0:31:40.335681
train attack total time: 10.045s
train attack init time: 2.492s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.228s


train attack loss increase over inner max: 0.581
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.140
total grad norm: 2.370

==================== evaluation at iteration: 97 ====================
train total loss: 87.472%
train max loss: 19.676%, reg loss: 75.168%
time spent training so far: 0:31:50.871713
train attack total time: 10.182s
train attack init time: 2.454s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.234s


train attack loss increase over inner max: 0.378
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.113
total grad norm: 2.406

==================== evaluation at iteration: 98 ====================
train total loss: 85.183%
train max loss: 18.718%, reg loss: 74.850%
time spent training so far: 0:32:03.547526
train attack total time: 12.334s
train attack init time: 2.308s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.332s


train attack loss increase over inner max: -1.011
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.125
total grad norm: 2.524

==================== evaluation at iteration: 99 ====================
train total loss: 88.744%
train max loss: 26.456%, reg loss: 75.116%
time spent training so far: 0:32:14.109262
train attack total time: 10.206s
train attack init time: 2.349s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 5.554
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.139
total grad norm: 2.729

==================== evaluation at iteration: 100 ====================
train total loss: 87.013%
train max loss: 25.899%, reg loss: 74.995%
time spent training so far: 0:32:27.928162
train attack total time: 13.470s
train attack init time: 2.558s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.368s


train attack loss increase over inner max: -0.607
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 6.320% of volume
percentage infeasible at boundary: 26.72%
mean, std amount infeasible at boundary: 1.29 +/- 2.92
max amount infeasible at boundary: 26.53

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.168
total grad norm: 2.537

==================== evaluation at iteration: 101 ====================
train total loss: 88.247%
train max loss: 25.319%, reg loss: 75.290%
time spent training so far: 0:34:37.943185
train attack total time: 10.964s
train attack init time: 2.265s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.276s


train attack loss increase over inner max: -0.827
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.167
total grad norm: 2.953

==================== evaluation at iteration: 102 ====================
train total loss: 88.293%
train max loss: 24.178%, reg loss: 75.144%
time spent training so far: 0:34:52.333982
train attack total time: 14.042s
train attack init time: 2.529s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.392s


train attack loss increase over inner max: -1.308
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.229
total grad norm: 3.084

==================== evaluation at iteration: 103 ====================
train total loss: 87.833%
train max loss: 23.385%, reg loss: 75.084%
time spent training so far: 0:35:05.533479
train attack total time: 12.833s
train attack init time: 2.253s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.352s


train attack loss increase over inner max: -1.281
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.238
total grad norm: 3.371

==================== evaluation at iteration: 104 ====================
train total loss: 89.214%
train max loss: 22.802%, reg loss: 74.980%
time spent training so far: 0:35:18.083604
train attack total time: 12.191s
train attack init time: 2.269s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.327s


train attack loss increase over inner max: -0.731
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.255
total grad norm: 4.922

==================== evaluation at iteration: 105 ====================
train total loss: 87.977%
train max loss: 23.227%, reg loss: 75.161%
time spent training so far: 0:35:30.900861
train attack total time: 12.475s
train attack init time: 2.286s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.338s


train attack loss increase over inner max: 0.706
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.323
total grad norm: 3.257

==================== evaluation at iteration: 106 ====================
train total loss: 87.265%
train max loss: 22.836%, reg loss: 75.136%
time spent training so far: 0:35:40.915168
train attack total time: 9.646s
train attack init time: 2.163s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.225s


train attack loss increase over inner max: -0.120
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.356
total grad norm: 4.261

==================== evaluation at iteration: 107 ====================
train total loss: 87.919%
train max loss: 21.735%, reg loss: 75.074%
time spent training so far: 0:35:53.767382
train attack total time: 12.505s
train attack init time: 2.187s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.342s


train attack loss increase over inner max: -1.320
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.382
total grad norm: 4.060

==================== evaluation at iteration: 108 ====================
train total loss: 87.751%
train max loss: 20.174%, reg loss: 75.135%
time spent training so far: 0:36:04.781415
train attack total time: 10.667s
train attack init time: 2.440s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.256s


train attack loss increase over inner max: -0.791
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.445
total grad norm: 4.903

==================== evaluation at iteration: 109 ====================
train total loss: 87.227%
train max loss: 20.796%, reg loss: 75.117%
time spent training so far: 0:36:19.662697
train attack total time: 14.455s
train attack init time: 2.699s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.402s


train attack loss increase over inner max: 1.463
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.515
total grad norm: 5.390

==================== evaluation at iteration: 110 ====================
train total loss: 86.126%
train max loss: 20.185%, reg loss: 75.213%
time spent training so far: 0:36:29.383920
train attack total time: 9.349s
train attack init time: 2.167s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.212s


train attack loss increase over inner max: -0.089
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.625
total grad norm: 5.515

==================== evaluation at iteration: 111 ====================
train total loss: 88.157%
train max loss: 24.118%, reg loss: 75.185%
time spent training so far: 0:36:39.943827
train attack total time: 10.186s
train attack init time: 2.307s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.242s


train attack loss increase over inner max: 4.548
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.605
total grad norm: 4.700

==================== evaluation at iteration: 112 ====================
train total loss: 88.258%
train max loss: 23.560%, reg loss: 75.259%
time spent training so far: 0:36:51.498825
train attack total time: 11.186s
train attack init time: 2.162s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.289s


train attack loss increase over inner max: 0.018
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.731
total grad norm: 4.425

==================== evaluation at iteration: 113 ====================
train total loss: 89.726%
train max loss: 28.032%, reg loss: 75.271%
time spent training so far: 0:37:07.318938
train attack total time: 15.477s
train attack init time: 2.247s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.464s


train attack loss increase over inner max: 4.049
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.792
total grad norm: 19.004

==================== evaluation at iteration: 114 ====================
train total loss: 90.237%
train max loss: 26.793%, reg loss: 75.215%
time spent training so far: 0:37:19.127037
train attack total time: 11.466s
train attack init time: 2.433s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.290s


train attack loss increase over inner max: -1.340
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.834
total grad norm: 13.687

==================== evaluation at iteration: 115 ====================
train total loss: 87.502%
train max loss: 26.293%, reg loss: 75.069%
time spent training so far: 0:37:32.362352
train attack total time: 12.875s
train attack init time: 2.215s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.356s


train attack loss increase over inner max: -0.951
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.893
total grad norm: 10.050

==================== evaluation at iteration: 116 ====================
train total loss: 88.768%
train max loss: 25.562%, reg loss: 75.277%
time spent training so far: 0:37:42.829158
train attack total time: 10.104s
train attack init time: 2.727s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.219s


train attack loss increase over inner max: -0.260
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.736
total grad norm: 9.610

==================== evaluation at iteration: 117 ====================
train total loss: 87.308%
train max loss: 23.790%, reg loss: 75.185%
time spent training so far: 0:38:01.101025
train attack total time: 17.927s
train attack init time: 2.367s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.562s


train attack loss increase over inner max: -1.044
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.628
total grad norm: 5.195

==================== evaluation at iteration: 118 ====================
train total loss: 86.074%
train max loss: 23.117%, reg loss: 75.228%
time spent training so far: 0:38:15.100525
train attack total time: 13.653s
train attack init time: 2.269s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.386s


train attack loss increase over inner max: -1.233
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.567
total grad norm: 6.259

==================== evaluation at iteration: 119 ====================
train total loss: 88.326%
train max loss: 25.393%, reg loss: 75.240%
time spent training so far: 0:38:25.825231
train attack total time: 10.324s
train attack init time: 2.073s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: 2.269
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.470
total grad norm: 4.759

==================== evaluation at iteration: 120 ====================
train total loss: 87.799%
train max loss: 24.415%, reg loss: 75.172%
time spent training so far: 0:38:39.702117
train attack total time: 13.517s
train attack init time: 2.217s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.382s


train attack loss increase over inner max: -1.175
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.370
total grad norm: 5.343

==================== evaluation at iteration: 121 ====================
train total loss: 87.255%
train max loss: 23.801%, reg loss: 75.259%
time spent training so far: 0:38:51.965837
train attack total time: 11.925s
train attack init time: 2.239s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.333s


train attack loss increase over inner max: -0.765
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.377
total grad norm: 4.210

==================== evaluation at iteration: 122 ====================
train total loss: 87.379%
train max loss: 22.663%, reg loss: 75.156%
time spent training so far: 0:39:02.618571
train attack total time: 10.279s
train attack init time: 2.380s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.257s


train attack loss increase over inner max: -0.823
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.404
total grad norm: 3.313

==================== evaluation at iteration: 123 ====================
train total loss: 87.242%
train max loss: 21.272%, reg loss: 75.231%
time spent training so far: 0:39:17.060976
train attack total time: 14.110s
train attack init time: 2.498s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.418s


train attack loss increase over inner max: -1.725
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.282
total grad norm: 3.837

==================== evaluation at iteration: 124 ====================
train total loss: 85.743%
train max loss: 20.457%, reg loss: 75.009%
time spent training so far: 0:39:31.070016
train attack total time: 13.660s
train attack init time: 2.251s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.410s


train attack loss increase over inner max: -0.957
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.315
total grad norm: 4.029

==================== evaluation at iteration: 125 ====================
train total loss: 85.610%
train max loss: 20.239%, reg loss: 75.181%
time spent training so far: 0:39:42.046384
train attack total time: 10.623s
train attack init time: 2.235s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.532
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.800% of volume
percentage infeasible at boundary: 24.52%
mean, std amount infeasible at boundary: 1.17 +/- 2.75
max amount infeasible at boundary: 17.56

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.297
total grad norm: 4.687

==================== evaluation at iteration: 126 ====================
train total loss: 86.500%
train max loss: 19.569%, reg loss: 75.103%
time spent training so far: 0:41:54.646375
train attack total time: 14.174s
train attack init time: 2.774s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.408s


train attack loss increase over inner max: -0.630
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.298
total grad norm: 3.911

==================== evaluation at iteration: 127 ====================
train total loss: 86.283%
train max loss: 18.498%, reg loss: 75.151%
time spent training so far: 0:42:08.210130
train attack total time: 13.222s
train attack init time: 2.428s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.382s


train attack loss increase over inner max: -0.859
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.377
total grad norm: 5.058

==================== evaluation at iteration: 128 ====================
train total loss: 86.807%
train max loss: 22.023%, reg loss: 75.262%
time spent training so far: 0:42:18.867652
train attack total time: 10.329s
train attack init time: 2.163s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: 3.885
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.334
total grad norm: 4.372

==================== evaluation at iteration: 129 ====================
train total loss: 86.458%
train max loss: 20.339%, reg loss: 75.098%
time spent training so far: 0:42:33.883710
train attack total time: 14.693s
train attack init time: 2.342s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.448s


train attack loss increase over inner max: -1.516
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.357
total grad norm: 4.595

==================== evaluation at iteration: 130 ====================
train total loss: 85.521%
train max loss: 18.140%, reg loss: 75.176%
time spent training so far: 0:42:46.918367
train attack total time: 12.646s
train attack init time: 2.438s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.355s


train attack loss increase over inner max: -2.237
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.424
total grad norm: 5.535

==================== evaluation at iteration: 131 ====================
train total loss: 87.364%
train max loss: 21.702%, reg loss: 75.030%
time spent training so far: 0:42:57.429392
train attack total time: 10.122s
train attack init time: 2.106s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.260s


train attack loss increase over inner max: 3.959
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0513]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0514]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0515]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0517]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0519]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0520]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0521]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0527]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.21827587485313416
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0527]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0531]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0535]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0540]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.27988192439079285
Parameter containing:
tensor([[0.0545]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0551]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0556]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0562]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0566]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0571]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0574]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0577]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0580]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0583]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0585]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0586]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0587]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0589]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0590]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.411
total grad norm: 3.791

==================== evaluation at iteration: 132 ====================
train total loss: 85.971%
train max loss: 19.187%, reg loss: 75.224%
time spent training so far: 0:43:09.447778
train attack total time: 11.659s
train attack init time: 2.404s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.312s


train attack loss increase over inner max: -1.273
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.373
total grad norm: 3.316

==================== evaluation at iteration: 133 ====================
train total loss: 86.138%
train max loss: 19.011%, reg loss: 75.177%
time spent training so far: 0:43:20.736046
train attack total time: 10.902s
train attack init time: 2.697s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.313
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.367
total grad norm: 4.671

==================== evaluation at iteration: 134 ====================
train total loss: 89.377%
train max loss: 26.805%, reg loss: 75.230%
time spent training so far: 0:43:35.096988
train attack total time: 13.992s
train attack init time: 2.436s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.415s


train attack loss increase over inner max: 7.387
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.602
total grad norm: 2.812

==================== evaluation at iteration: 135 ====================
train total loss: 89.282%
train max loss: 28.284%, reg loss: 75.195%
time spent training so far: 0:43:45.182044
train attack total time: 9.727s
train attack init time: 2.045s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.247s


train attack loss increase over inner max: 1.948
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.554
total grad norm: 4.990

==================== evaluation at iteration: 136 ====================
train total loss: 89.375%
train max loss: 25.882%, reg loss: 75.238%
time spent training so far: 0:43:59.028786
train attack total time: 13.474s
train attack init time: 2.294s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.399s


train attack loss increase over inner max: -2.362
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.461
total grad norm: 3.931

==================== evaluation at iteration: 137 ====================
train total loss: 87.457%
train max loss: 25.311%, reg loss: 75.141%
time spent training so far: 0:44:10.047943
train attack total time: 10.622s
train attack init time: 2.373s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.270s


train attack loss increase over inner max: -0.342
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.519
total grad norm: 5.485

==================== evaluation at iteration: 138 ====================
train total loss: 88.190%
train max loss: 23.272%, reg loss: 75.269%
time spent training so far: 0:44:22.750080
train attack total time: 12.344s
train attack init time: 2.354s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.346s


train attack loss increase over inner max: -1.775
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.542
total grad norm: 5.977

==================== evaluation at iteration: 139 ====================
train total loss: 88.012%
train max loss: 23.550%, reg loss: 75.417%
time spent training so far: 0:44:33.698280
train attack total time: 10.573s
train attack init time: 2.519s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: 0.090
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.379
total grad norm: 5.640

==================== evaluation at iteration: 140 ====================
train total loss: 86.459%
train max loss: 21.912%, reg loss: 75.183%
time spent training so far: 0:44:45.593135
train attack total time: 11.539s
train attack init time: 2.121s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: -1.519
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.441
total grad norm: 4.434

==================== evaluation at iteration: 141 ====================
train total loss: 85.619%
train max loss: 20.396%, reg loss: 75.148%
time spent training so far: 0:44:59.313598
train attack total time: 13.355s
train attack init time: 2.037s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.405s


train attack loss increase over inner max: -0.903
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.452
total grad norm: 5.546

==================== evaluation at iteration: 142 ====================
train total loss: 85.027%
train max loss: 20.074%, reg loss: 75.271%
time spent training so far: 0:45:09.507622
train attack total time: 9.816s
train attack init time: 2.237s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: -0.149
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.466
total grad norm: 8.369

==================== evaluation at iteration: 143 ====================
train total loss: 87.335%
train max loss: 23.416%, reg loss: 75.218%
time spent training so far: 0:45:19.499294
train attack total time: 9.642s
train attack init time: 2.542s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.220s


train attack loss increase over inner max: 4.152
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.475
total grad norm: 6.266

==================== evaluation at iteration: 144 ====================
train total loss: 85.724%
train max loss: 21.871%, reg loss: 75.253%
time spent training so far: 0:45:31.048142
train attack total time: 11.182s
train attack init time: 2.134s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.307s


train attack loss increase over inner max: -2.075
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.462
total grad norm: 6.719

==================== evaluation at iteration: 145 ====================
train total loss: 84.680%
train max loss: 22.178%, reg loss: 75.174%
time spent training so far: 0:45:41.536461
train attack total time: 10.119s
train attack init time: 1.976s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.267s


train attack loss increase over inner max: 0.333
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.530
total grad norm: 6.817

==================== evaluation at iteration: 146 ====================
train total loss: 87.788%
train max loss: 21.765%, reg loss: 75.237%
time spent training so far: 0:45:52.970305
train attack total time: 11.079s
train attack init time: 2.533s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.283s


train attack loss increase over inner max: 0.251
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.615
total grad norm: 9.744

==================== evaluation at iteration: 147 ====================
train total loss: 85.118%
train max loss: 20.289%, reg loss: 75.210%
time spent training so far: 0:46:07.791662
train attack total time: 14.471s
train attack init time: 2.020s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.454s


train attack loss increase over inner max: -1.419
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.577
total grad norm: 6.972

==================== evaluation at iteration: 148 ====================
train total loss: 84.376%
train max loss: 19.698%, reg loss: 75.114%
time spent training so far: 0:46:21.695190
train attack total time: 13.515s
train attack init time: 2.307s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.400s


train attack loss increase over inner max: -0.531
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.480
total grad norm: 4.393

==================== evaluation at iteration: 149 ====================
train total loss: 84.982%
train max loss: 18.492%, reg loss: 75.216%
time spent training so far: 0:46:31.840799
train attack total time: 9.783s
train attack init time: 2.019s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.250s


train attack loss increase over inner max: -0.688
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.673
total grad norm: 5.973

==================== evaluation at iteration: 150 ====================
train total loss: 86.822%
train max loss: 21.439%, reg loss: 75.210%
time spent training so far: 0:46:43.184155
train attack total time: 10.981s
train attack init time: 1.959s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: 0.034
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.440% of volume
percentage infeasible at boundary: 23.72%
mean, std amount infeasible at boundary: 1.27 +/- 12.15
max amount infeasible at boundary: 592.76

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.606
total grad norm: 6.682

==================== evaluation at iteration: 151 ====================
train total loss: 86.903%
train max loss: 21.104%, reg loss: 75.261%
time spent training so far: 0:48:44.852654
train attack total time: 11.562s
train attack init time: 2.109s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.324s


train attack loss increase over inner max: -0.760
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.654
total grad norm: 5.725

==================== evaluation at iteration: 152 ====================
train total loss: 83.927%
train max loss: 19.538%, reg loss: 75.225%
time spent training so far: 0:48:56.076706
train attack total time: 10.886s
train attack init time: 1.866s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.304s


train attack loss increase over inner max: -8.459
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.765
total grad norm: 6.012

==================== evaluation at iteration: 153 ====================
train total loss: 86.078%
train max loss: 19.434%, reg loss: 75.282%
time spent training so far: 0:49:06.717404
train attack total time: 10.293s
train attack init time: 1.994s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.273s


train attack loss increase over inner max: 0.158
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.719
total grad norm: 6.978

==================== evaluation at iteration: 154 ====================
train total loss: 85.325%
train max loss: 19.297%, reg loss: 75.383%
time spent training so far: 0:49:16.884874
train attack total time: 9.828s
train attack init time: 1.920s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.258s


train attack loss increase over inner max: 0.660
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.677
total grad norm: 8.596

==================== evaluation at iteration: 155 ====================
train total loss: 84.841%
train max loss: 18.722%, reg loss: 75.371%
time spent training so far: 0:49:26.067744
train attack total time: 8.858s
train attack init time: 2.229s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.218s


train attack loss increase over inner max: -0.204
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.719
total grad norm: 9.770

==================== evaluation at iteration: 156 ====================
train total loss: 86.328%
train max loss: 17.995%, reg loss: 75.102%
time spent training so far: 0:49:34.635798
train attack total time: 8.206s
train attack init time: 2.135s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: 0.061
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.673
total grad norm: 8.745

==================== evaluation at iteration: 157 ====================
train total loss: 86.249%
train max loss: 16.725%, reg loss: 75.408%
time spent training so far: 0:49:45.290401
train attack total time: 10.311s
train attack init time: 2.343s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.790
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.731
total grad norm: 7.668

==================== evaluation at iteration: 158 ====================
train total loss: 86.840%
train max loss: 19.476%, reg loss: 75.264%
time spent training so far: 0:49:55.056052
train attack total time: 9.424s
train attack init time: 2.196s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 2.621
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.746
total grad norm: 8.626

==================== evaluation at iteration: 159 ====================
train total loss: 85.436%
train max loss: 18.945%, reg loss: 75.350%
time spent training so far: 0:50:06.034182
train attack total time: 10.602s
train attack init time: 2.152s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.297s


train attack loss increase over inner max: -140.448
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.915
total grad norm: 5.611

==================== evaluation at iteration: 160 ====================
train total loss: 83.995%
train max loss: 18.368%, reg loss: 75.254%
time spent training so far: 0:50:13.615756
train attack total time: 7.307s
train attack init time: 2.149s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.154s


train attack loss increase over inner max: -0.785
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.874
total grad norm: 8.477

==================== evaluation at iteration: 161 ====================
train total loss: 84.193%
train max loss: 18.035%, reg loss: 75.356%
time spent training so far: 0:50:22.705648
train attack total time: 8.753s
train attack init time: 2.066s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.217s


train attack loss increase over inner max: -0.197
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.736
total grad norm: 9.216

==================== evaluation at iteration: 162 ====================
train total loss: 85.628%
train max loss: 20.041%, reg loss: 75.355%
time spent training so far: 0:50:31.423614
train attack total time: 8.401s
train attack init time: 2.196s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: 1.811
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.875
total grad norm: 19.615

==================== evaluation at iteration: 163 ====================
train total loss: 85.532%
train max loss: 21.141%, reg loss: 75.387%
time spent training so far: 0:50:39.010491
train attack total time: 7.260s
train attack init time: 2.307s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.145s


train attack loss increase over inner max: 1.084
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.946
total grad norm: 6.901

==================== evaluation at iteration: 164 ====================
train total loss: 84.401%
train max loss: 18.508%, reg loss: 75.383%
time spent training so far: 0:50:48.204449
train attack total time: 8.861s
train attack init time: 2.013s
train attack avg grad step time: 0.075s
train attack avg reproj time: 0.220s


train attack loss increase over inner max: -0.492
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.811
total grad norm: 8.119

==================== evaluation at iteration: 165 ====================
train total loss: 84.650%
train max loss: 17.924%, reg loss: 75.420%
time spent training so far: 0:50:56.047485
train attack total time: 7.514s
train attack init time: 2.035s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.165s


train attack loss increase over inner max: -0.113
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.837
total grad norm: 12.087

==================== evaluation at iteration: 166 ====================
train total loss: 85.006%
train max loss: 20.202%, reg loss: 75.388%
time spent training so far: 0:51:03.361078
train attack total time: 6.975s
train attack init time: 2.039s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.140s


train attack loss increase over inner max: 2.876
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.861
total grad norm: 14.728

==================== evaluation at iteration: 167 ====================
train total loss: 82.898%
train max loss: 19.291%, reg loss: 75.434%
time spent training so far: 0:51:09.931848
train attack total time: 6.281s
train attack init time: 2.064s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.114s


train attack loss increase over inner max: -0.174
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.884
total grad norm: 10.832

==================== evaluation at iteration: 168 ====================
train total loss: 85.086%
train max loss: 17.528%, reg loss: 75.362%
time spent training so far: 0:51:17.165923
train attack total time: 6.895s
train attack init time: 2.072s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.141s


train attack loss increase over inner max: -0.108
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.893
total grad norm: 7.059

==================== evaluation at iteration: 169 ====================
train total loss: 84.842%
train max loss: 16.979%, reg loss: 75.404%
time spent training so far: 0:51:25.280363
train attack total time: 7.753s
train attack init time: 2.206s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: -0.437
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.961
total grad norm: 20.351

==================== evaluation at iteration: 170 ====================
train total loss: 85.137%
train max loss: 20.263%, reg loss: 75.321%
time spent training so far: 0:51:32.783560
train attack total time: 7.135s
train attack init time: 2.063s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.149s


train attack loss increase over inner max: 2.883
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.922
total grad norm: 17.257

==================== evaluation at iteration: 171 ====================
train total loss: 83.152%
train max loss: 16.091%, reg loss: 75.310%
time spent training so far: 0:51:40.077348
train attack total time: 6.866s
train attack init time: 2.011s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.140s


train attack loss increase over inner max: -0.748
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.857
total grad norm: 9.169

==================== evaluation at iteration: 172 ====================
train total loss: 83.839%
train max loss: 16.041%, reg loss: 75.481%
time spent training so far: 0:51:47.174969
train attack total time: 6.779s
train attack init time: 1.977s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.133s


train attack loss increase over inner max: 0.459
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.077
total grad norm: 51.567

==================== evaluation at iteration: 173 ====================
train total loss: 86.286%
train max loss: 24.661%, reg loss: 75.345%
time spent training so far: 0:51:54.884225
train attack total time: 7.415s
train attack init time: 1.949s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: 9.449
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.866
total grad norm: 19.128

==================== evaluation at iteration: 174 ====================
train total loss: 82.677%
train max loss: 18.520%, reg loss: 75.283%
time spent training so far: 0:52:03.344416
train attack total time: 8.128s
train attack init time: 2.054s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.204s


train attack loss increase over inner max: 3.047
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.742
total grad norm: 9.703

==================== evaluation at iteration: 175 ====================
train total loss: 87.067%
train max loss: 22.834%, reg loss: 75.345%
time spent training so far: 0:52:10.159731
train attack total time: 6.512s
train attack init time: 2.057s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.131s


train attack loss increase over inner max: 4.336
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 5.040% of volume
percentage infeasible at boundary: 23.20%
mean, std amount infeasible at boundary: 0.82 +/- 2.19
max amount infeasible at boundary: 35.82

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.743
total grad norm: 12.118

==================== evaluation at iteration: 176 ====================
train total loss: 86.454%
train max loss: 22.060%, reg loss: 75.258%
time spent training so far: 0:54:04.271788
train attack total time: 7.212s
train attack init time: 2.047s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.162s


train attack loss increase over inner max: -7.580
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000

Max_n_steps: 21
Parameter containing:
tensor([[0.0591]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0592]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0593]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0594]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0596]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0598]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0602]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0604]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0607]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0611]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0615]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0620]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0626]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0632]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.35346055030822754
Parameter containing:
tensor([[0.0639]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0647]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0656]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0664]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0673]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0684]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0695]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0708]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0722]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0739]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0756]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0773]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.17152349650859833
Parameter containing:
tensor([[0.0791]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0809]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0828]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0847]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0868]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0888]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0908]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0929]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0951]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0973]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0994]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.1016]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.1037]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.1059]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1082]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1105]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1128]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1149]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.896
total grad norm: 8.004

==================== evaluation at iteration: 177 ====================
train total loss: 85.842%
train max loss: 21.846%, reg loss: 75.351%
time spent training so far: 0:54:12.137679
train attack total time: 7.535s
train attack init time: 2.114s
train attack avg grad step time: 0.077s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.637
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.773
total grad norm: 10.416

==================== evaluation at iteration: 178 ====================
train total loss: 87.118%
train max loss: 21.074%, reg loss: 75.206%
time spent training so far: 0:54:19.714120
train attack total time: 7.256s
train attack init time: 2.100s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.165s


train attack loss increase over inner max: -0.892
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.796
total grad norm: 7.447

==================== evaluation at iteration: 179 ====================
train total loss: 88.003%
train max loss: 26.446%, reg loss: 75.280%
time spent training so far: 0:54:26.934004
train attack total time: 6.873s
train attack init time: 2.111s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.148s


train attack loss increase over inner max: 5.412
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.811
total grad norm: 12.171

==================== evaluation at iteration: 180 ====================
train total loss: 87.331%
train max loss: 25.831%, reg loss: 75.235%
time spent training so far: 0:54:34.802778
train attack total time: 7.558s
train attack init time: 2.189s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.174s


train attack loss increase over inner max: -0.080
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.830
total grad norm: 7.989

==================== evaluation at iteration: 181 ====================
train total loss: 87.035%
train max loss: 24.918%, reg loss: 75.094%
time spent training so far: 0:54:42.570887
train attack total time: 7.462s
train attack init time: 2.009s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.177s


train attack loss increase over inner max: -2.294
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.803
total grad norm: 11.274

==================== evaluation at iteration: 182 ====================
train total loss: 86.868%
train max loss: 23.606%, reg loss: 75.141%
time spent training so far: 0:54:49.452758
train attack total time: 6.472s
train attack init time: 2.172s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.128s


train attack loss increase over inner max: 0.392
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.784
total grad norm: 7.785

==================== evaluation at iteration: 183 ====================
train total loss: 85.783%
train max loss: 22.039%, reg loss: 75.333%
time spent training so far: 0:54:57.393843
train attack total time: 7.613s
train attack init time: 1.993s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: 0.483
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.773
total grad norm: 7.570

==================== evaluation at iteration: 184 ====================
train total loss: 86.886%
train max loss: 22.713%, reg loss: 75.252%
time spent training so far: 0:55:04.760300
train attack total time: 7.038s
train attack init time: 2.089s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.149s


train attack loss increase over inner max: 1.726
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.873
total grad norm: 6.495

==================== evaluation at iteration: 185 ====================
train total loss: 84.810%
train max loss: 21.147%, reg loss: 75.278%
time spent training so far: 0:55:12.258326
train attack total time: 7.174s
train attack init time: 2.165s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: 0.011
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.776
total grad norm: 52.393

==================== evaluation at iteration: 186 ====================
train total loss: 88.844%
train max loss: 28.302%, reg loss: 75.232%
time spent training so far: 0:55:23.335492
train attack total time: 10.684s
train attack init time: 1.951s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: -482.682
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.919
total grad norm: 21.935

==================== evaluation at iteration: 187 ====================
train total loss: 86.503%
train max loss: 19.857%, reg loss: 75.329%
time spent training so far: 0:55:31.975148
train attack total time: 8.273s
train attack init time: 2.054s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.211s


train attack loss increase over inner max: -0.676
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.776
total grad norm: 9.975

==================== evaluation at iteration: 188 ====================
train total loss: 84.392%
train max loss: 21.188%, reg loss: 75.246%
time spent training so far: 0:55:38.520229
train attack total time: 6.228s
train attack init time: 2.023s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.122s


train attack loss increase over inner max: 1.393
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.859
total grad norm: 9.953

==================== evaluation at iteration: 189 ====================
train total loss: 85.222%
train max loss: 20.170%, reg loss: 75.169%
time spent training so far: 0:55:46.161483
train attack total time: 7.307s
train attack init time: 2.011s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -1.169
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.880
total grad norm: 12.421

==================== evaluation at iteration: 190 ====================
train total loss: 85.360%
train max loss: 17.732%, reg loss: 75.309%
time spent training so far: 0:55:55.230125
train attack total time: 8.799s
train attack init time: 2.104s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: -1.813
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.072
total grad norm: 8.871

==================== evaluation at iteration: 191 ====================
train total loss: 86.565%
train max loss: 18.067%, reg loss: 75.071%
time spent training so far: 0:56:03.336958
train attack total time: 7.665s
train attack init time: 2.103s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.185s


train attack loss increase over inner max: 0.642
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.867
total grad norm: 8.420

==================== evaluation at iteration: 192 ====================
train total loss: 86.139%
train max loss: 17.326%, reg loss: 75.404%
time spent training so far: 0:56:11.687023
train attack total time: 8.031s
train attack init time: 2.157s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: -0.701
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.088
total grad norm: 8.833

==================== evaluation at iteration: 193 ====================
train total loss: 84.086%
train max loss: 17.746%, reg loss: 75.228%
time spent training so far: 0:56:19.277356
train attack total time: 7.269s
train attack init time: 2.065s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.169s


train attack loss increase over inner max: 0.067
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.207
total grad norm: 8.248

==================== evaluation at iteration: 194 ====================
train total loss: 85.904%
train max loss: 20.268%, reg loss: 75.209%
time spent training so far: 0:56:26.063381
train attack total time: 6.483s
train attack init time: 1.900s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.142s


train attack loss increase over inner max: 2.784
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.425
total grad norm: 13.814

==================== evaluation at iteration: 195 ====================
train total loss: 86.457%
train max loss: 20.316%, reg loss: 75.287%
time spent training so far: 0:56:34.596146
train attack total time: 8.234s
train attack init time: 2.176s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: -0.248
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.188
total grad norm: 24.155

==================== evaluation at iteration: 196 ====================
train total loss: 86.833%
train max loss: 22.157%, reg loss: 75.405%
time spent training so far: 0:56:44.715661
train attack total time: 9.712s
train attack init time: 2.101s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.274s


train attack loss increase over inner max: 2.093
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.111
total grad norm: 17.685

==================== evaluation at iteration: 197 ====================
train total loss: 84.453%
train max loss: 18.146%, reg loss: 75.143%
time spent training so far: 0:56:53.238025
train attack total time: 8.161s
train attack init time: 2.054s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.208s


train attack loss increase over inner max: -0.351
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.157
total grad norm: 12.654

==================== evaluation at iteration: 198 ====================
train total loss: 84.932%
train max loss: 17.510%, reg loss: 75.436%
time spent training so far: 0:57:00.491075
train attack total time: 6.902s
train attack init time: 2.213s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.143s


train attack loss increase over inner max: -0.561
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.259
total grad norm: 5.856

==================== evaluation at iteration: 199 ====================
train total loss: 85.254%
train max loss: 21.902%, reg loss: 75.301%
time spent training so far: 0:57:08.437955
train attack total time: 7.638s
train attack init time: 2.004s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.183s


train attack loss increase over inner max: 4.648
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.465
total grad norm: 8.258

==================== evaluation at iteration: 200 ====================
train total loss: 86.010%
train max loss: 20.646%, reg loss: 75.271%
time spent training so far: 0:57:17.100792
train attack total time: 8.394s
train attack init time: 2.286s
train attack avg grad step time: 0.064s
train attack avg reproj time: 0.211s


train attack loss increase over inner max: -1.050
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 3.480% of volume
percentage infeasible at boundary: 20.36%
mean, std amount infeasible at boundary: 0.85 +/- 2.41
max amount infeasible at boundary: 34.00

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.926
total grad norm: 10.427

==================== evaluation at iteration: 201 ====================
train total loss: 88.067%
train max loss: 25.155%, reg loss: 75.320%
time spent training so far: 0:59:16.599167
train attack total time: 8.680s
train attack init time: 2.215s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: 4.575
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.958
total grad norm: 10.079

==================== evaluation at iteration: 202 ====================
train total loss: 88.260%
train max loss: 23.816%, reg loss: 75.453%
time spent training so far: 0:59:25.049280
train attack total time: 8.146s
train attack init time: 2.104s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.202s


train attack loss increase over inner max: -1.032
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.054
total grad norm: 5.708

==================== evaluation at iteration: 203 ====================
train total loss: 87.203%
train max loss: 22.811%, reg loss: 75.165%
time spent training so far: 0:59:35.713561
train attack total time: 10.319s
train attack init time: 2.126s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.300s


train attack loss increase over inner max: -23.351
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.938
total grad norm: 17.189

==================== evaluation at iteration: 204 ====================
train total loss: 88.737%
train max loss: 25.179%, reg loss: 75.183%
time spent training so far: 0:59:42.963660
train attack total time: 6.956s
train attack init time: 2.044s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.149s


train attack loss increase over inner max: 2.177
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.355
total grad norm: 34.551

==================== evaluation at iteration: 205 ====================
train total loss: 88.808%
train max loss: 24.796%, reg loss: 75.252%
time spent training so far: 0:59:49.969168
train attack total time: 6.686s
train attack init time: 1.986s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.144s


train attack loss increase over inner max: 2.409
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.834
total grad norm: 8.613

==================== evaluation at iteration: 206 ====================
train total loss: 86.290%
train max loss: 21.698%, reg loss: 75.234%
time spent training so far: 0:59:58.791673
train attack total time: 8.511s
train attack init time: 2.219s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -1.173
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.138
total grad norm: 19.607

==================== evaluation at iteration: 207 ====================
train total loss: 85.386%
train max loss: 20.155%, reg loss: 75.306%
time spent training so far: 1:00:08.394204
train attack total time: 9.253s
train attack init time: 2.046s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.258s


train attack loss increase over inner max: -350.212
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.950
total grad norm: 12.876

==================== evaluation at iteration: 208 ====================
train total loss: 84.636%
train max loss: 19.786%, reg loss: 75.307%
time spent training so far: 1:00:16.124511
train attack total time: 7.392s
train attack init time: 1.985s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: -1.042
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.041
total grad norm: 9.144

==================== evaluation at iteration: 209 ====================
train total loss: 84.968%
train max loss: 18.916%, reg loss: 75.218%
time spent training so far: 1:00:24.106520
train attack total time: 7.666s
train attack init time: 2.105s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.181s


train attack loss increase over inner max: 0.303
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.973
total grad norm: 7.939

==================== evaluation at iteration: 210 ====================
train total loss: 85.539%
train max loss: 18.219%, reg loss: 75.240%
time spent training so far: 1:00:30.314057
train attack total time: 5.880s
train attack init time: 2.061s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.103s


train attack loss increase over inner max: -0.731
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.885
total grad norm: 7.417

==================== evaluation at iteration: 211 ====================
train total loss: 85.264%
train max loss: 19.055%, reg loss: 75.361%
time spent training so far: 1:00:37.772625
train attack total time: 7.107s
train attack init time: 2.345s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.146s


train attack loss increase over inner max: 1.900
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.811
total grad norm: 7.978

==================== evaluation at iteration: 212 ====================
train total loss: 86.049%
train max loss: 18.452%, reg loss: 75.308%
time spent training so far: 1:00:46.152678
train attack total time: 8.059s
train attack init time: 2.316s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: -1.022
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.058
total grad norm: 9.524

==================== evaluation at iteration: 213 ====================
train total loss: 84.137%
train max loss: 17.865%, reg loss: 75.200%
time spent training so far: 1:00:52.911786
train attack total time: 6.431s
train attack init time: 2.076s
train attack avg grad step time: 0.063s
train attack avg reproj time: 0.133s


train attack loss increase over inner max: -1.101
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.952
total grad norm: 8.498

==================== evaluation at iteration: 214 ====================
train total loss: 86.610%
train max loss: 20.512%, reg loss: 75.410%
time spent training so far: 1:00:59.364463
train attack total time: 6.139s
train attack init time: 2.040s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.114s


train attack loss increase over inner max: 1.951
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.139
total grad norm: 9.348

==================== evaluation at iteration: 215 ====================
train total loss: 88.764%
train max loss: 27.472%, reg loss: 75.306%
time spent training so far: 1:01:07.152222
train attack total time: 7.464s
train attack init time: 1.943s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.179s


train attack loss increase over inner max: 6.298
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.059
total grad norm: 6.315

==================== evaluation at iteration: 216 ====================
train total loss: 89.450%
train max loss: 28.015%, reg loss: 75.321%
time spent training so far: 1:01:14.745913
train attack total time: 7.275s
train attack init time: 2.111s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.165s


train attack loss increase over inner max: 0.385
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.976
total grad norm: 6.077

==================== evaluation at iteration: 217 ====================
train total loss: 88.287%
train max loss: 26.146%, reg loss: 75.289%
time spent training so far: 1:01:21.834144
train attack total time: 6.752s
train attack init time: 2.228s
train attack avg grad step time: 0.064s
train attack avg reproj time: 0.139s


train attack loss increase over inner max: -0.663
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.005
total grad norm: 8.780

==================== evaluation at iteration: 218 ====================
train total loss: 87.952%
train max loss: 26.393%, reg loss: 75.221%
time spent training so far: 1:01:28.839066
train attack total time: 6.694s
train attack init time: 2.066s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: -0.406
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.151
total grad norm: 7.028

==================== evaluation at iteration: 219 ====================
train total loss: 86.821%
train max loss: 24.840%, reg loss: 75.401%
time spent training so far: 1:01:36.656845
train attack total time: 7.515s
train attack init time: 2.075s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.176s


train attack loss increase over inner max: -1.659
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.156
total grad norm: 8.381

==================== evaluation at iteration: 220 ====================
train total loss: 86.002%
train max loss: 24.609%, reg loss: 75.413%
time spent training so far: 1:01:43.992276
train attack total time: 7.001s
train attack init time: 1.946s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.159s


train attack loss increase over inner max: -0.724
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1170]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1189]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1206]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1222]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1236]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1249]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1260]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1271]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1280]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  397.8602294921875
Parameter containing:
tensor([[0.1293]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1308]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1322]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1335]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1347]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1359]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1369]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1379]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1388]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1398]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1408]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1418]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1428]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1436]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1445]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1452]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1459]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.7717676162719727
Parameter containing:
tensor([[0.1464]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1471]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1480]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1490]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  132.5690155029297
Parameter containing:
tensor([[0.1501]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1512]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1532]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1541]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1549]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1556]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1562]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1568]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1574]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1578]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1581]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1585]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1588]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 1.029
total grad norm: 8.402

==================== evaluation at iteration: 221 ====================
train total loss: 89.465%
train max loss: 23.718%, reg loss: 75.396%
time spent training so far: 1:01:51.834936
train attack total time: 7.473s
train attack init time: 2.215s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: 0.236
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.242
total grad norm: 7.793

==================== evaluation at iteration: 222 ====================
train total loss: 88.345%
train max loss: 23.274%, reg loss: 75.364%
time spent training so far: 1:01:58.033169
train attack total time: 5.869s
train attack init time: 1.955s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.107s


train attack loss increase over inner max: -0.552
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.023
total grad norm: 9.406

==================== evaluation at iteration: 223 ====================
train total loss: 87.441%
train max loss: 22.327%, reg loss: 75.440%
time spent training so far: 1:02:05.993489
train attack total time: 7.645s
train attack init time: 1.992s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.189s


train attack loss increase over inner max: -0.221
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.368
total grad norm: 9.492

==================== evaluation at iteration: 224 ====================
train total loss: 85.375%
train max loss: 21.663%, reg loss: 75.671%
time spent training so far: 1:02:12.809892
train attack total time: 6.467s
train attack init time: 2.061s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.127s


train attack loss increase over inner max: 0.193
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.285
total grad norm: 9.678

==================== evaluation at iteration: 225 ====================
train total loss: 86.237%
train max loss: 20.588%, reg loss: 75.390%
time spent training so far: 1:02:20.304112
train attack total time: 7.192s
train attack init time: 2.058s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.851
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 2.960% of volume
percentage infeasible at boundary: 17.48%
mean, std amount infeasible at boundary: 0.67 +/- 2.57
max amount infeasible at boundary: 80.07

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 1.230
total grad norm: 112.177

==================== evaluation at iteration: 226 ====================
train total loss: 105.469%
train max loss: 43.475%, reg loss: 75.381%
time spent training so far: 1:04:13.902666
train attack total time: 7.556s
train attack init time: 2.156s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.175s


train attack loss increase over inner max: 22.665
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.165
total grad norm: 72.347

==================== evaluation at iteration: 227 ====================
train total loss: 94.164%
train max loss: 34.603%, reg loss: 75.345%
time spent training so far: 1:04:21.190232
train attack total time: 6.912s
train attack init time: 2.027s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.150s


train attack loss increase over inner max: 9.647
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.233
total grad norm: 20.287

==================== evaluation at iteration: 228 ====================
train total loss: 85.999%
train max loss: 19.146%, reg loss: 75.460%
time spent training so far: 1:04:28.979618
train attack total time: 7.440s
train attack init time: 2.148s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.173s


train attack loss increase over inner max: -0.061
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.445
total grad norm: 22.556

==================== evaluation at iteration: 229 ====================
train total loss: 84.366%
train max loss: 18.287%, reg loss: 75.525%
time spent training so far: 1:04:37.266615
train attack total time: 7.950s
train attack init time: 2.153s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.189s


train attack loss increase over inner max: -0.012
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.212
total grad norm: 12.582

==================== evaluation at iteration: 230 ====================
train total loss: 83.903%
train max loss: 17.781%, reg loss: 75.428%
time spent training so far: 1:04:44.911774
train attack total time: 7.323s
train attack init time: 1.869s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: -0.739
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.543
total grad norm: 9.278

==================== evaluation at iteration: 231 ====================
train total loss: 85.600%
train max loss: 17.513%, reg loss: 75.495%
time spent training so far: 1:04:53.054167
train attack total time: 7.769s
train attack init time: 2.133s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: -0.476
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.558
total grad norm: 7.159

==================== evaluation at iteration: 232 ====================
train total loss: 84.753%
train max loss: 17.680%, reg loss: 75.481%
time spent training so far: 1:05:00.838367
train attack total time: 7.488s
train attack init time: 2.249s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.162s


train attack loss increase over inner max: -0.067
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.836
total grad norm: 6.839

==================== evaluation at iteration: 233 ====================
train total loss: 87.186%
train max loss: 19.218%, reg loss: 75.634%
time spent training so far: 1:05:08.330517
train attack total time: 7.157s
train attack init time: 2.250s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.154s


train attack loss increase over inner max: 1.699
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.067
total grad norm: 6.592

==================== evaluation at iteration: 234 ====================
train total loss: 86.585%
train max loss: 19.902%, reg loss: 75.718%
time spent training so far: 1:05:15.748668
train attack total time: 7.117s
train attack init time: 2.095s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: -0.018
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.861
total grad norm: 7.924

==================== evaluation at iteration: 235 ====================
train total loss: 85.124%
train max loss: 18.694%, reg loss: 75.619%
time spent training so far: 1:05:22.769619
train attack total time: 6.736s
train attack init time: 2.113s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.139s


train attack loss increase over inner max: -0.670
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.022
total grad norm: 9.773

==================== evaluation at iteration: 236 ====================
train total loss: 89.963%
train max loss: 28.274%, reg loss: 75.610%
time spent training so far: 1:05:30.191979
train attack total time: 7.089s
train attack init time: 2.124s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.154s


train attack loss increase over inner max: 8.804
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.866
total grad norm: 6.193

==================== evaluation at iteration: 237 ====================
train total loss: 89.685%
train max loss: 27.887%, reg loss: 75.533%
time spent training so far: 1:05:37.644340
train attack total time: 7.115s
train attack init time: 1.956s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.604
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.719
total grad norm: 7.130

==================== evaluation at iteration: 238 ====================
train total loss: 88.361%
train max loss: 27.331%, reg loss: 75.515%
time spent training so far: 1:05:45.491167
train attack total time: 7.542s
train attack init time: 2.304s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.169s


train attack loss increase over inner max: -0.267
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.740
total grad norm: 5.598

==================== evaluation at iteration: 239 ====================
train total loss: 89.318%
train max loss: 26.792%, reg loss: 75.425%
time spent training so far: 1:05:53.254998
train attack total time: 7.477s
train attack init time: 2.181s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: 0.803
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.442
total grad norm: 7.241

==================== evaluation at iteration: 240 ====================
train total loss: 87.146%
train max loss: 25.388%, reg loss: 75.474%
time spent training so far: 1:06:00.804749
train attack total time: 7.226s
train attack init time: 2.069s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.160s


train attack loss increase over inner max: -0.656
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.727
total grad norm: 6.869

==================== evaluation at iteration: 241 ====================
train total loss: 87.621%
train max loss: 24.599%, reg loss: 75.447%
time spent training so far: 1:06:08.299045
train attack total time: 7.096s
train attack init time: 2.301s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.147s


train attack loss increase over inner max: 0.489
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.929
total grad norm: 9.096

==================== evaluation at iteration: 242 ====================
train total loss: 87.335%
train max loss: 23.605%, reg loss: 75.246%
time spent training so far: 1:06:15.986647
train attack total time: 7.298s
train attack init time: 2.164s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.863
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.778
total grad norm: 9.992

==================== evaluation at iteration: 243 ====================
train total loss: 86.421%
train max loss: 21.539%, reg loss: 75.590%
time spent training so far: 1:06:28.140105
train attack total time: 11.832s
train attack init time: 2.009s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.375s


train attack loss increase over inner max: 0.137
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.573
total grad norm: 26.034

==================== evaluation at iteration: 244 ====================
train total loss: 86.868%
train max loss: 21.135%, reg loss: 75.443%
time spent training so far: 1:06:37.310295
train attack total time: 8.895s
train attack init time: 2.323s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.230s


train attack loss increase over inner max: 0.242
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.552
total grad norm: 15.550

==================== evaluation at iteration: 245 ====================
train total loss: 85.916%
train max loss: 20.741%, reg loss: 75.460%
time spent training so far: 1:06:45.515506
train attack total time: 7.905s
train attack init time: 2.097s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.191s


train attack loss increase over inner max: 1.428
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.597
total grad norm: 9.719

==================== evaluation at iteration: 246 ====================
train total loss: 86.414%
train max loss: 20.083%, reg loss: 75.529%
time spent training so far: 1:06:52.676158
train attack total time: 6.797s
train attack init time: 2.204s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.141s


train attack loss increase over inner max: 0.110
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.578
total grad norm: 10.816

==================== evaluation at iteration: 247 ====================
train total loss: 87.382%
train max loss: 19.581%, reg loss: 75.417%
time spent training so far: 1:07:00.558025
train attack total time: 7.512s
train attack init time: 2.033s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: -0.419
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.526
total grad norm: 8.359

==================== evaluation at iteration: 248 ====================
train total loss: 84.668%
train max loss: 18.527%, reg loss: 75.522%
time spent training so far: 1:07:08.227457
train attack total time: 7.340s
train attack init time: 2.098s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.822
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.270
total grad norm: 8.975

==================== evaluation at iteration: 249 ====================
train total loss: 84.133%
train max loss: 17.088%, reg loss: 75.437%
time spent training so far: 1:07:14.922483
train attack total time: 6.383s
train attack init time: 1.974s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.130s


train attack loss increase over inner max: -1.021
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.431
total grad norm: 7.923

==================== evaluation at iteration: 250 ====================
train total loss: 83.515%
train max loss: 17.015%, reg loss: 75.492%
time spent training so far: 1:07:22.511794
train attack total time: 7.258s
train attack init time: 1.985s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.171s


train attack loss increase over inner max: 0.631
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 3.520% of volume
percentage infeasible at boundary: 18.20%
mean, std amount infeasible at boundary: 0.64 +/- 1.87
max amount infeasible at boundary: 16.34

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 1.861
total grad norm: 7.514

==================== evaluation at iteration: 251 ====================
train total loss: 86.123%
train max loss: 21.129%, reg loss: 75.481%
time spent training so far: 1:09:12.779387
train attack total time: 7.244s
train attack init time: 1.895s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 4.586
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.552
total grad norm: 9.698

==================== evaluation at iteration: 252 ====================
train total loss: 86.163%
train max loss: 21.431%, reg loss: 75.473%
time spent training so far: 1:09:19.759512
train attack total time: 6.703s
train attack init time: 1.980s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.145s


train attack loss increase over inner max: 1.919
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.401
total grad norm: 6.852

==================== evaluation at iteration: 253 ====================
train total loss: 84.802%
train max loss: 20.180%, reg loss: 75.460%
time spent training so far: 1:09:26.575535
train attack total time: 6.476s
train attack init time: 2.024s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.134s


train attack loss increase over inner max: -1.197
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.298
total grad norm: 36.507

==================== evaluation at iteration: 254 ====================
train total loss: 89.014%
train max loss: 27.219%, reg loss: 75.552%
time spent training so far: 1:09:33.963760
train attack total time: 7.037s
train attack init time: 2.046s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.155s


train attack loss increase over inner max: 7.654
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.249
total grad norm: 11.338

==================== evaluation at iteration: 255 ====================
train total loss: 82.860%
train max loss: 18.934%, reg loss: 75.536%
time spent training so far: 1:09:41.957862
train attack total time: 7.658s
train attack init time: 1.939s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.191s


train attack loss increase over inner max: -1.802
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.375
total grad norm: 9.272

==================== evaluation at iteration: 256 ====================
train total loss: 86.009%
train max loss: 19.909%, reg loss: 75.500%
time spent training so far: 1:09:49.413432
train attack total time: 7.068s
train attack init time: 1.869s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: 3.665
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.408
total grad norm: 12.083

==================== evaluation at iteration: 257 ====================
train total loss: 86.031%
train max loss: 19.514%, reg loss: 75.542%
time spent training so far: 1:09:56.658434
train attack total time: 6.943s
train attack init time: 2.055s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.150s


train attack loss increase over inner max: 0.468
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.185
total grad norm: 16.957

==================== evaluation at iteration: 258 ====================
train total loss: 84.978%
train max loss: 20.377%, reg loss: 75.480%
time spent training so far: 1:10:04.243790
train attack total time: 7.234s
train attack init time: 2.146s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.158s


train attack loss increase over inner max: 2.907
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.294
total grad norm: 17.336

==================== evaluation at iteration: 259 ====================
train total loss: 86.102%
train max loss: 23.959%, reg loss: 75.401%
time spent training so far: 1:10:12.343420
train attack total time: 7.793s
train attack init time: 2.019s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: 5.153
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.461
total grad norm: 16.343

==================== evaluation at iteration: 260 ====================
train total loss: 87.412%
train max loss: 22.371%, reg loss: 75.580%
time spent training so far: 1:10:20.062388
train attack total time: 7.357s
train attack init time: 2.047s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: 1.561
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.417
total grad norm: 13.468

==================== evaluation at iteration: 261 ====================
train total loss: 84.501%
train max loss: 20.269%, reg loss: 75.470%
time spent training so far: 1:10:27.092556
train attack total time: 6.624s
train attack init time: 2.184s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.125s


train attack loss increase over inner max: -1.011
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.423
total grad norm: 16.965

==================== evaluation at iteration: 262 ====================
train total loss: 85.637%
train max loss: 19.553%, reg loss: 75.538%
time spent training so far: 1:10:35.335403
train attack total time: 7.904s
train attack init time: 2.151s
train attack avg grad step time: 0.075s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: 0.117
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.350
total grad norm: 12.285

==================== evaluation at iteration: 263 ====================
train total loss: 85.264%
train max loss: 18.418%, reg loss: 75.541%
time spent training so far: 1:10:43.441052
train attack total time: 7.823s
train attack init time: 1.998s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.191s


train attack loss increase over inner max: -0.413
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.478
total grad norm: 9.977

==================== evaluation at iteration: 264 ====================
train total loss: 83.482%
train max loss: 18.107%, reg loss: 75.470%
time spent training so far: 1:10:51.218583
train attack total time: 7.468s
train attack init time: 2.052s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.171s


train attack loss increase over inner max: 1.617
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.226
total grad norm: 10.473

==================== evaluation at iteration: 265 ====================
train total loss: 85.105%
train max loss: 17.424%, reg loss: 75.556%
time spent training so far: 1:10:59.450899
train attack total time: 7.926s
train attack init time: 1.999s
train attack avg grad step time: 0.077s
train attack avg reproj time: 0.190s


train attack loss increase over inner max: 0.888
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000

Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1590]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1593]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1595]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1597]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1612]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1628]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1644]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1660]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1676]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1690]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1702]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1713]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1721]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1729]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1736]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1741]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1745]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1748]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1750]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1752]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1753]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  1.7979050874710083
Parameter containing:
tensor([[0.1754]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1756]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1757]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1759]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1761]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1763]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1764]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1766]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1767]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1768]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1769]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1771]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1773]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1774]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1776]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1779]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1783]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1789]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1795]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1802]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1810]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1817]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1824]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 1.379
total grad norm: 13.527

==================== evaluation at iteration: 266 ====================
train total loss: 86.366%
train max loss: 18.693%, reg loss: 75.503%
time spent training so far: 1:11:07.369003
train attack total time: 7.617s
train attack init time: 1.967s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: -4.574
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.425
total grad norm: 8.434

==================== evaluation at iteration: 267 ====================
train total loss: 84.865%
train max loss: 20.703%, reg loss: 75.489%
time spent training so far: 1:11:15.350316
train attack total time: 7.641s
train attack init time: 2.098s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: 1.683
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.588
total grad norm: 11.649

==================== evaluation at iteration: 268 ====================
train total loss: 86.479%
train max loss: 19.952%, reg loss: 75.544%
time spent training so far: 1:11:23.340390
train attack total time: 7.673s
train attack init time: 2.042s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: -1.097
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.495
total grad norm: 8.546

==================== evaluation at iteration: 269 ====================
train total loss: 85.891%
train max loss: 19.923%, reg loss: 75.552%
time spent training so far: 1:11:30.486216
train attack total time: 6.812s
train attack init time: 2.129s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.141s


train attack loss increase over inner max: 0.005
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.347
total grad norm: 7.748

==================== evaluation at iteration: 270 ====================
train total loss: 85.563%
train max loss: 19.166%, reg loss: 75.515%
time spent training so far: 1:11:37.916261
train attack total time: 7.097s
train attack init time: 1.868s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.056
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.756
total grad norm: 9.633

==================== evaluation at iteration: 271 ====================
train total loss: 84.260%
train max loss: 19.259%, reg loss: 75.585%
time spent training so far: 1:11:45.971278
train attack total time: 7.655s
train attack init time: 1.993s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: -24.178
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.387
total grad norm: 10.478

==================== evaluation at iteration: 272 ====================
train total loss: 86.633%
train max loss: 18.249%, reg loss: 75.669%
time spent training so far: 1:11:54.141818
train attack total time: 7.822s
train attack init time: 2.029s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.193s


train attack loss increase over inner max: -0.885
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.405
total grad norm: 13.793

==================== evaluation at iteration: 273 ====================
train total loss: 84.287%
train max loss: 18.337%, reg loss: 75.689%
time spent training so far: 1:12:02.533632
train attack total time: 8.083s
train attack init time: 2.014s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: -0.335
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.554
total grad norm: 11.541

==================== evaluation at iteration: 274 ====================
train total loss: 84.048%
train max loss: 18.177%, reg loss: 75.629%
time spent training so far: 1:12:10.313371
train attack total time: 7.467s
train attack init time: 2.074s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.174s


train attack loss increase over inner max: -0.318
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.895
total grad norm: 9.282

==================== evaluation at iteration: 275 ====================
train total loss: 85.509%
train max loss: 17.566%, reg loss: 75.646%
time spent training so far: 1:12:18.468821
train attack total time: 7.858s
train attack init time: 2.039s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: -0.401
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 2.680% of volume
percentage infeasible at boundary: 19.68%
mean, std amount infeasible at boundary: 0.69 +/- 2.63
max amount infeasible at boundary: 94.84

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 1.535
total grad norm: 8.378

==================== evaluation at iteration: 276 ====================
train total loss: 84.240%
train max loss: 17.045%, reg loss: 75.586%
time spent training so far: 1:14:09.106391
train attack total time: 7.881s
train attack init time: 2.012s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.192s


train attack loss increase over inner max: -0.046
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.626
total grad norm: 10.510

==================== evaluation at iteration: 277 ====================
train total loss: 87.672%
train max loss: 23.861%, reg loss: 75.833%
time spent training so far: 1:14:16.509931
train attack total time: 7.101s
train attack init time: 2.002s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.160s


train attack loss increase over inner max: 6.971
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.678
total grad norm: 8.447

==================== evaluation at iteration: 278 ====================
train total loss: 86.992%
train max loss: 25.172%, reg loss: 75.593%
time spent training so far: 1:14:24.252410
train attack total time: 7.351s
train attack init time: 1.975s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: 1.391
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.770
total grad norm: 13.953

==================== evaluation at iteration: 279 ====================
train total loss: 86.483%
train max loss: 24.419%, reg loss: 75.774%
time spent training so far: 1:14:31.752538
train attack total time: 7.157s
train attack init time: 2.152s
train attack avg grad step time: 0.063s
train attack avg reproj time: 0.162s


train attack loss increase over inner max: -0.900
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.602
total grad norm: 8.853

==================== evaluation at iteration: 280 ====================
train total loss: 86.253%
train max loss: 23.917%, reg loss: 75.798%
time spent training so far: 1:14:38.719380
train attack total time: 6.639s
train attack init time: 1.924s
train attack avg grad step time: 0.063s
train attack avg reproj time: 0.148s


train attack loss increase over inner max: -1.067
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.617
total grad norm: 8.280

==================== evaluation at iteration: 281 ====================
train total loss: 88.380%
train max loss: 23.359%, reg loss: 75.828%
time spent training so far: 1:14:45.537047
train attack total time: 6.472s
train attack init time: 1.986s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.133s


train attack loss increase over inner max: -0.157
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.981
total grad norm: 7.674

==================== evaluation at iteration: 282 ====================
train total loss: 86.132%
train max loss: 23.060%, reg loss: 75.761%
time spent training so far: 1:14:53.383478
train attack total time: 7.498s
train attack init time: 1.999s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.179s


train attack loss increase over inner max: 0.514
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.542
total grad norm: 11.204

==================== evaluation at iteration: 283 ====================
train total loss: 85.944%
train max loss: 22.124%, reg loss: 75.801%
time spent training so far: 1:15:01.046837
train attack total time: 7.315s
train attack init time: 2.002s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: -0.142
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.558
total grad norm: 10.800

==================== evaluation at iteration: 284 ====================
train total loss: 86.107%
train max loss: 21.041%, reg loss: 75.828%
time spent training so far: 1:15:08.869037
train attack total time: 7.509s
train attack init time: 2.143s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.173s


train attack loss increase over inner max: -0.477
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.843
total grad norm: 7.783

==================== evaluation at iteration: 285 ====================
train total loss: 85.209%
train max loss: 20.346%, reg loss: 75.754%
time spent training so far: 1:15:17.174832
train attack total time: 7.964s
train attack init time: 2.200s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: -0.326
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.754
total grad norm: 8.082

==================== evaluation at iteration: 286 ====================
train total loss: 84.181%
train max loss: 19.676%, reg loss: 75.829%
time spent training so far: 1:15:24.258844
train attack total time: 6.715s
train attack init time: 2.091s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: -0.510
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.988
total grad norm: 8.949

==================== evaluation at iteration: 287 ====================
train total loss: 85.038%
train max loss: 19.254%, reg loss: 75.862%
time spent training so far: 1:15:31.177292
train attack total time: 6.579s
train attack init time: 2.032s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: -0.181
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.010
total grad norm: 10.726

==================== evaluation at iteration: 288 ====================
train total loss: 86.452%
train max loss: 19.886%, reg loss: 75.950%
time spent training so far: 1:15:39.201095
train attack total time: 7.766s
train attack init time: 1.975s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: 1.330
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.305
total grad norm: 16.937

==================== evaluation at iteration: 289 ====================
train total loss: 86.600%
train max loss: 19.881%, reg loss: 75.943%
time spent training so far: 1:15:47.526283
train attack total time: 8.018s
train attack init time: 2.143s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: 0.152
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.039
total grad norm: 9.750

==================== evaluation at iteration: 290 ====================
train total loss: 84.835%
train max loss: 18.902%, reg loss: 75.899%
time spent training so far: 1:15:55.081600
train attack total time: 7.251s
train attack init time: 1.973s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: -0.548
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.805
total grad norm: 12.061

==================== evaluation at iteration: 291 ====================
train total loss: 83.883%
train max loss: 18.801%, reg loss: 75.846%
time spent training so far: 1:16:02.085440
train attack total time: 6.634s
train attack init time: 1.980s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.143s


train attack loss increase over inner max: 0.015
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.077
total grad norm: 11.829

==================== evaluation at iteration: 292 ====================
train total loss: 84.310%
train max loss: 18.653%, reg loss: 75.939%
time spent training so far: 1:16:09.545101
train attack total time: 7.178s
train attack init time: 2.016s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: -0.451
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.084
total grad norm: 12.326

==================== evaluation at iteration: 293 ====================
train total loss: 85.415%
train max loss: 18.412%, reg loss: 75.910%
time spent training so far: 1:16:16.923654
train attack total time: 7.014s
train attack init time: 2.109s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.151s


train attack loss increase over inner max: 0.965
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.053
total grad norm: 8.611

==================== evaluation at iteration: 294 ====================
train total loss: 84.585%
train max loss: 20.090%, reg loss: 76.006%
time spent training so far: 1:16:24.900000
train attack total time: 7.668s
train attack init time: 2.102s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.176s


train attack loss increase over inner max: 2.645
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.826
total grad norm: 12.015

==================== evaluation at iteration: 295 ====================
train total loss: 84.661%
train max loss: 19.871%, reg loss: 76.124%
time spent training so far: 1:16:33.071359
train attack total time: 7.844s
train attack init time: 2.099s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.188s


train attack loss increase over inner max: -0.363
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.472
total grad norm: 17.287

==================== evaluation at iteration: 296 ====================
train total loss: 83.949%
train max loss: 17.158%, reg loss: 76.091%
time spent training so far: 1:16:42.364048
train attack total time: 8.930s
train attack init time: 1.970s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.245s


train attack loss increase over inner max: -1.369
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.015
total grad norm: 10.680

==================== evaluation at iteration: 297 ====================
train total loss: 84.394%
train max loss: 17.028%, reg loss: 76.042%
time spent training so far: 1:16:50.183982
train attack total time: 7.484s
train attack init time: 2.130s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: -1.039
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.298
total grad norm: 12.074

==================== evaluation at iteration: 298 ====================
train total loss: 82.732%
train max loss: 15.136%, reg loss: 76.051%
time spent training so far: 1:16:57.844289
train attack total time: 7.364s
train attack init time: 2.057s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 0.149
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.122
total grad norm: 10.067

==================== evaluation at iteration: 299 ====================
train total loss: 84.268%
train max loss: 17.596%, reg loss: 76.112%
time spent training so far: 1:17:04.164207
train attack total time: 6.047s
train attack init time: 1.899s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.121s


train attack loss increase over inner max: 2.394
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.431
total grad norm: 9.990

==================== evaluation at iteration: 300 ====================
train total loss: 84.255%
train max loss: 18.407%, reg loss: 76.189%
time spent training so far: 1:17:11.872199
train attack total time: 7.438s
train attack init time: 2.032s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.175s


train attack loss increase over inner max: 0.525
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 1.760% of volume
percentage infeasible at boundary: 13.60%
mean, std amount infeasible at boundary: 0.40 +/- 1.41
max amount infeasible at boundary: 15.94

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 2.334
total grad norm: 11.018

==================== evaluation at iteration: 301 ====================
train total loss: 83.468%
train max loss: 17.343%, reg loss: 76.160%
time spent training so far: 1:19:09.464840
train attack total time: 9.111s
train attack init time: 2.144s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: -0.387
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.392
total grad norm: 11.578

==================== evaluation at iteration: 302 ====================
train total loss: 83.758%
train max loss: 17.335%, reg loss: 76.204%
time spent training so far: 1:19:20.543317
train attack total time: 10.617s
train attack init time: 3.327s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.240s


train attack loss increase over inner max: -0.056
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.458
total grad norm: 13.119

==================== evaluation at iteration: 303 ====================
train total loss: 84.079%
train max loss: 16.222%, reg loss: 76.206%
time spent training so far: 1:19:30.362757
train attack total time: 9.351s
train attack init time: 3.360s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.183s


train attack loss increase over inner max: 0.488
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.710
total grad norm: 9.097

==================== evaluation at iteration: 304 ====================
train total loss: 83.139%
train max loss: 15.467%, reg loss: 76.400%
time spent training so far: 1:19:40.815459
train attack total time: 9.995s
train attack init time: 3.414s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.203s


train attack loss increase over inner max: -0.429
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.269
total grad norm: 11.004

==================== evaluation at iteration: 305 ====================
train total loss: 83.271%
train max loss: 16.035%, reg loss: 76.260%
time spent training so far: 1:19:51.741313
train attack total time: 10.445s
train attack init time: 3.615s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.217s


train attack loss increase over inner max: 2.135
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.450
total grad norm: 9.688

==================== evaluation at iteration: 306 ====================
train total loss: 83.463%
train max loss: 15.239%, reg loss: 76.324%
time spent training so far: 1:20:01.992139
train attack total time: 9.712s
train attack init time: 3.690s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: -0.609
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.472
total grad norm: 10.430

==================== evaluation at iteration: 307 ====================
train total loss: 83.017%
train max loss: 15.824%, reg loss: 76.375%
time spent training so far: 1:20:13.086070
train attack total time: 10.621s
train attack init time: 3.476s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.230s


train attack loss increase over inner max: 0.981
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.636
total grad norm: 13.821

==================== evaluation at iteration: 308 ====================
train total loss: 82.900%
train max loss: 15.288%, reg loss: 76.361%
time spent training so far: 1:20:23.355331
train attack total time: 9.748s
train attack init time: 3.421s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.198s


train attack loss increase over inner max: -0.942
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.885
total grad norm: 16.653

==================== evaluation at iteration: 309 ====================
train total loss: 82.845%
train max loss: 13.289%, reg loss: 76.486%
time spent training so far: 1:20:33.718330
train attack total time: 9.878s
train attack init time: 3.627s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: -2.145
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.602
total grad norm: 14.229

==================== evaluation at iteration: 310 ====================
train total loss: 82.804%
train max loss: 13.244%, reg loss: 76.355%
time spent training so far: 1:20:45.321900
train attack total time: 11.143s
train attack init time: 3.671s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.244s


train attack loss increase over inner max: 1.459
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1831]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1836]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1841]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1845]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1849]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1852]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1855]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1858]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1861]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1863]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1865]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1867]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1868]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1870]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1871]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1871]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1872]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1872]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1872]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1871]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1871]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1870]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1869]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1868]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1867]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1865]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1864]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1863]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1861]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1861]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1860]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1859]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1858]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1857]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1856]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1855]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1854]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pklReg grad norm: 2.991
total grad norm: 9.700

==================== evaluation at iteration: 311 ====================
train total loss: 83.174%
train max loss: 15.728%, reg loss: 76.680%
time spent training so far: 1:20:56.313301
train attack total time: 10.396s
train attack init time: 3.694s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.213s


train attack loss increase over inner max: 3.091
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.762
total grad norm: 15.046

==================== evaluation at iteration: 312 ====================
train total loss: 83.656%
train max loss: 16.902%, reg loss: 76.500%
time spent training so far: 1:21:05.820123
train attack total time: 9.029s
train attack init time: 3.526s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.159s


train attack loss increase over inner max: 1.692
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.623
total grad norm: 10.452

==================== evaluation at iteration: 313 ====================
train total loss: 83.602%
train max loss: 18.135%, reg loss: 76.631%
time spent training so far: 1:21:17.286105
train attack total time: 10.979s
train attack init time: 3.599s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.239s


train attack loss increase over inner max: 1.073
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.483
total grad norm: 12.362

==================== evaluation at iteration: 314 ====================
train total loss: 84.733%
train max loss: 19.240%, reg loss: 76.848%
time spent training so far: 1:21:27.707398
train attack total time: 9.916s
train attack init time: 3.820s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.181s


train attack loss increase over inner max: 1.612
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.419
total grad norm: 14.174

==================== evaluation at iteration: 315 ====================
train total loss: 83.935%
train max loss: 18.763%, reg loss: 76.572%
time spent training so far: 1:21:42.382070
train attack total time: 14.165s
train attack init time: 3.867s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: -0.495
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.849
total grad norm: 10.385

==================== evaluation at iteration: 316 ====================
train total loss: 83.838%
train max loss: 18.495%, reg loss: 76.689%
time spent training so far: 1:21:53.636725
train attack total time: 10.716s
train attack init time: 4.116s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.209s


train attack loss increase over inner max: 0.942
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.627
total grad norm: 10.854

==================== evaluation at iteration: 317 ====================
train total loss: 84.487%
train max loss: 17.411%, reg loss: 76.589%
time spent training so far: 1:22:04.649095
train attack total time: 10.507s
train attack init time: 3.806s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.213s


train attack loss increase over inner max: 0.136
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.085
total grad norm: 8.184

==================== evaluation at iteration: 318 ====================
train total loss: 84.868%
train max loss: 16.487%, reg loss: 76.757%
time spent training so far: 1:22:15.166711
train attack total time: 10.052s
train attack init time: 4.045s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.176s


train attack loss increase over inner max: 0.580
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.592
total grad norm: 14.374

==================== evaluation at iteration: 319 ====================
train total loss: 83.495%
train max loss: 16.422%, reg loss: 76.662%
time spent training so far: 1:22:28.343528
train attack total time: 12.701s
train attack init time: 3.993s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.304s


train attack loss increase over inner max: 1.716
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.007
total grad norm: 12.043

==================== evaluation at iteration: 320 ====================
train total loss: 82.580%
train max loss: 14.319%, reg loss: 76.767%
time spent training so far: 1:22:40.931966
train attack total time: 12.106s
train attack init time: 4.110s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.267s


train attack loss increase over inner max: -1.165
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.705
total grad norm: 10.718

==================== evaluation at iteration: 321 ====================
train total loss: 84.772%
train max loss: 15.909%, reg loss: 76.705%
time spent training so far: 1:22:52.264722
train attack total time: 10.820s
train attack init time: 4.124s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.208s


train attack loss increase over inner max: 1.287
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.205
total grad norm: 15.889

==================== evaluation at iteration: 322 ====================
train total loss: 82.848%
train max loss: 16.467%, reg loss: 76.801%
time spent training so far: 1:23:02.052461
train attack total time: 9.288s
train attack init time: 3.757s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: 0.835
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.842
total grad norm: 12.878

==================== evaluation at iteration: 323 ====================
train total loss: 83.837%
train max loss: 14.668%, reg loss: 76.781%
time spent training so far: 1:23:13.132126
train attack total time: 10.631s
train attack init time: 4.080s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.204s


train attack loss increase over inner max: -1.684
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.009
total grad norm: 16.167

==================== evaluation at iteration: 324 ====================
train total loss: 82.922%
train max loss: 13.857%, reg loss: 76.859%
time spent training so far: 1:23:24.980413
train attack total time: 11.358s
train attack init time: 4.385s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.220s


train attack loss increase over inner max: 0.138
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.189
total grad norm: 17.510

==================== evaluation at iteration: 325 ====================
train total loss: 83.541%
train max loss: 13.627%, reg loss: 76.904%
time spent training so far: 1:23:37.052352
train attack total time: 11.565s
train attack init time: 4.307s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: -0.263
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.320% of volume
percentage infeasible at boundary: 6.32%
mean, std amount infeasible at boundary: 0.15 +/- 0.80
max amount infeasible at boundary: 13.44

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 2.724
total grad norm: 14.590

==================== evaluation at iteration: 326 ====================
train total loss: 82.944%
train max loss: 12.486%, reg loss: 76.847%
time spent training so far: 1:27:28.675187
train attack total time: 9.551s
train attack init time: 4.121s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: 0.546
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.924
total grad norm: 13.978

==================== evaluation at iteration: 327 ====================
train total loss: 83.642%
train max loss: 14.275%, reg loss: 76.878%
time spent training so far: 1:27:38.579792
train attack total time: 9.422s
train attack init time: 4.130s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.148s


train attack loss increase over inner max: 1.172
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.871
total grad norm: 13.185

==================== evaluation at iteration: 328 ====================
train total loss: 83.226%
train max loss: 13.068%, reg loss: 76.830%
time spent training so far: 1:27:48.968031
train attack total time: 9.920s
train attack init time: 4.594s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.150s


train attack loss increase over inner max: 1.078
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.759
total grad norm: 14.409

==================== evaluation at iteration: 329 ====================
train total loss: 83.730%
train max loss: 13.619%, reg loss: 76.792%
time spent training so far: 1:28:00.805142
train attack total time: 11.371s
train attack init time: 4.553s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.218s


train attack loss increase over inner max: -0.066
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.274
total grad norm: 21.317

==================== evaluation at iteration: 330 ====================
train total loss: 85.189%
train max loss: 12.766%, reg loss: 76.997%
time spent training so far: 1:28:13.201326
train attack total time: 11.948s
train attack init time: 5.159s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -0.843
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.046
total grad norm: 11.790

==================== evaluation at iteration: 331 ====================
train total loss: 82.464%
train max loss: 11.905%, reg loss: 76.989%
time spent training so far: 1:28:25.034364
train attack total time: 11.330s
train attack init time: 4.970s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.197s


train attack loss increase over inner max: -0.221
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.855
total grad norm: 15.969

==================== evaluation at iteration: 332 ====================
train total loss: 85.130%
train max loss: 15.829%, reg loss: 76.807%
time spent training so far: 1:28:36.554986
train attack total time: 11.059s
train attack init time: 4.877s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.186s


train attack loss increase over inner max: 5.387
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.231
total grad norm: 14.510

==================== evaluation at iteration: 333 ====================
train total loss: 84.716%
train max loss: 14.286%, reg loss: 77.157%
time spent training so far: 1:28:47.851010
train attack total time: 10.815s
train attack init time: 4.838s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.179s


train attack loss increase over inner max: -0.182
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.266
total grad norm: 14.941

==================== evaluation at iteration: 334 ====================
train total loss: 84.048%
train max loss: 14.135%, reg loss: 77.146%
time spent training so far: 1:29:07.243014
train attack total time: 18.932s
train attack init time: 6.563s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.464s


train attack loss increase over inner max: -0.389
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.229
total grad norm: 9.989

==================== evaluation at iteration: 335 ====================
train total loss: 84.930%
train max loss: 15.546%, reg loss: 77.059%
time spent training so far: 1:29:21.042821
train attack total time: 13.323s
train attack init time: 5.972s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 2.075
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.469
total grad norm: 9.661

==================== evaluation at iteration: 336 ====================
train total loss: 84.116%
train max loss: 14.663%, reg loss: 77.131%
time spent training so far: 1:29:33.537789
train attack total time: 11.962s
train attack init time: 5.513s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.197s


train attack loss increase over inner max: 0.359
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.919
total grad norm: 11.900

==================== evaluation at iteration: 337 ====================
train total loss: 83.151%
train max loss: 13.347%, reg loss: 77.044%
time spent training so far: 1:29:45.992408
train attack total time: 11.985s
train attack init time: 5.399s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.208s


train attack loss increase over inner max: 0.243
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.178
total grad norm: 21.063

==================== evaluation at iteration: 338 ====================
train total loss: 84.717%
train max loss: 12.407%, reg loss: 77.182%
time spent training so far: 1:29:59.918831
train attack total time: 13.420s
train attack init time: 6.399s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.223s


train attack loss increase over inner max: 0.516
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.440
total grad norm: 15.442

==================== evaluation at iteration: 339 ====================
train total loss: 82.854%
train max loss: 11.916%, reg loss: 77.261%
time spent training so far: 1:30:12.300890
train attack total time: 11.883s
train attack init time: 5.877s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: 0.994
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.024
total grad norm: 10.235

==================== evaluation at iteration: 340 ====================
train total loss: 82.446%
train max loss: 10.503%, reg loss: 77.201%
time spent training so far: 1:30:24.204064
train attack total time: 11.403s
train attack init time: 5.442s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: -0.302
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.216
total grad norm: 16.799

==================== evaluation at iteration: 341 ====================
train total loss: 83.040%
train max loss: 15.866%, reg loss: 77.185%
time spent training so far: 1:30:35.520009
train attack total time: 10.741s
train attack init time: 5.667s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.137s


train attack loss increase over inner max: 6.495
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.985
total grad norm: 18.080

==================== evaluation at iteration: 342 ====================
train total loss: 83.870%
train max loss: 15.188%, reg loss: 77.530%
time spent training so far: 1:30:47.536180
train attack total time: 11.512s
train attack init time: 5.111s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.199s


train attack loss increase over inner max: -0.450
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.800
total grad norm: 22.989

==================== evaluation at iteration: 343 ====================
train total loss: 84.662%
train max loss: 14.099%, reg loss: 77.035%
time spent training so far: 1:30:59.193556
train attack total time: 11.149s
train attack init time: 5.480s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.169s


train attack loss increase over inner max: 0.282
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.266
total grad norm: 25.807

==================== evaluation at iteration: 344 ====================
train total loss: 82.379%
train max loss: 12.343%, reg loss: 77.286%
time spent training so far: 1:31:12.468662
train attack total time: 12.824s
train attack init time: 4.998s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.262s


train attack loss increase over inner max: 1.587
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.646
total grad norm: 19.277

==================== evaluation at iteration: 345 ====================
train total loss: 81.443%
train max loss: 12.089%, reg loss: 77.122%
time spent training so far: 1:31:25.748894
train attack total time: 12.807s
train attack init time: 5.547s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: 3.483
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.197
total grad norm: 20.683

==================== evaluation at iteration: 346 ====================
train total loss: 81.963%
train max loss: 9.124%, reg loss: 77.352%
time spent training so far: 1:31:39.814551
train attack total time: 13.526s
train attack init time: 6.364s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: 1.457
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.090
total grad norm: 28.235

==================== evaluation at iteration: 347 ====================
train total loss: 81.954%
train max loss: 8.716%, reg loss: 77.204%
time spent training so far: 1:31:54.879414
train attack total time: 14.521s
train attack init time: 7.045s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.247s


train attack loss increase over inner max: 0.887
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.013
total grad norm: 16.516

==================== evaluation at iteration: 348 ====================
train total loss: 81.160%
train max loss: 7.476%, reg loss: 77.225%
time spent training so far: 1:32:09.275886
train attack total time: 13.884s
train attack init time: 6.543s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.239s


train attack loss increase over inner max: 0.204
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.840
total grad norm: 19.430

==================== evaluation at iteration: 349 ====================
train total loss: 82.898%
train max loss: 12.122%, reg loss: 77.124%
time spent training so far: 1:32:21.683229
train attack total time: 11.936s
train attack init time: 6.144s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.172s


train attack loss increase over inner max: 6.233
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.669
total grad norm: 15.531

==================== evaluation at iteration: 350 ====================
train total loss: 82.187%
train max loss: 12.353%, reg loss: 77.101%
time spent training so far: 1:32:35.059702
train attack total time: 12.867s
train attack init time: 5.980s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.218s


train attack loss increase over inner max: -0.681
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.160% of volume
percentage infeasible at boundary: 4.60%
mean, std amount infeasible at boundary: 0.11 +/- 0.68
max amount infeasible at boundary: 9.44

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 3.383
total grad norm: 16.560

==================== evaluation at iteration: 351 ====================
train total loss: 82.358%
train max loss: 13.667%, reg loss: 77.369%
time spent training so far: 1:38:08.977466
train attack total time: 11.742s
train attack init time: 6.407s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.151s


train attack loss increase over inner max: 0.349
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.187
total grad norm: 19.450

==================== evaluation at iteration: 352 ====================
train total loss: 85.972%
train max loss: 15.479%, reg loss: 77.330%
time spent training so far: 1:38:23.062782
train attack total time: 13.597s
train attack init time: 7.163s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.200s


train attack loss increase over inner max: 2.374
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.744
total grad norm: 17.967

==================== evaluation at iteration: 353 ====================
train total loss: 85.259%
train max loss: 15.112%, reg loss: 77.531%
time spent training so far: 1:38:34.619138
train attack total time: 11.094s
train attack init time: 6.541s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.115s


train attack loss increase over inner max: 0.482
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.112
total grad norm: 20.091

==================== evaluation at iteration: 354 ====================
train total loss: 84.690%
train max loss: 13.951%, reg loss: 77.734%
time spent training so far: 1:38:47.985323
train attack total time: 12.887s
train attack init time: 5.883s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.223s


train attack loss increase over inner max: -0.508
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.154
total grad norm: 20.563

==================== evaluation at iteration: 355 ====================
train total loss: 84.199%
train max loss: 13.275%, reg loss: 77.797%
time spent training so far: 1:39:01.588257
train attack total time: 13.126s
train attack init time: 6.287s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.222s


train attack loss increase over inner max: 0.790
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.304
total grad norm: 18.997

==================== evaluation at iteration: 356 ====================
train total loss: 81.604%
train max loss: 9.368%, reg loss: 77.507%
time spent training so far: 1:39:18.660917
train attack total time: 16.580s
train attack init time: 8.111s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.294s


train attack loss increase over inner max: -0.601
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Max_n_steps: 20
Parameter containing:
tensor([[0.1853]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1851]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1850]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1848]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1846]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1843]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1839]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1836]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1832]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1828]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1824]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1820]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1817]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1813]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1809]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1805]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1801]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1797]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1793]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1788]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1783]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1778]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1773]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1768]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1762]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1757]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1751]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1746]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1740]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1735]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1730]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1725]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1720]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1716]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1712]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1708]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1703]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1698]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1693]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1689]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1685]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1681]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1678]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1675]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1672]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1669]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 3.631
total grad norm: 18.246

==================== evaluation at iteration: 357 ====================
train total loss: 83.828%
train max loss: 13.076%, reg loss: 77.562%
time spent training so far: 1:39:32.813155
train attack total time: 13.694s
train attack init time: 8.581s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: 6.817
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.381
total grad norm: 19.077

==================== evaluation at iteration: 358 ====================
train total loss: 81.686%
train max loss: 10.667%, reg loss: 77.457%
time spent training so far: 1:39:53.142516
train attack total time: 19.820s
train attack init time: 9.225s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.386s


train attack loss increase over inner max: -0.352
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.272
total grad norm: 23.688

==================== evaluation at iteration: 359 ====================
train total loss: 80.085%
train max loss: 5.665%, reg loss: 77.425%
time spent training so far: 1:40:10.406498
train attack total time: 16.723s
train attack init time: 10.548s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.186s


train attack loss increase over inner max: -0.907
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.295
total grad norm: 15.518

==================== evaluation at iteration: 360 ====================
train total loss: 81.082%
train max loss: 6.105%, reg loss: 77.615%
time spent training so far: 1:40:25.964588
train attack total time: 15.074s
train attack init time: 8.776s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: 1.357
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.777
total grad norm: 17.491

==================== evaluation at iteration: 361 ====================
train total loss: 81.941%
train max loss: 9.044%, reg loss: 77.785%
time spent training so far: 1:40:41.714930
train attack total time: 15.167s
train attack init time: 5.713s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.337s


train attack loss increase over inner max: 2.311
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.058
total grad norm: 12.794

==================== evaluation at iteration: 362 ====================
train total loss: 82.059%
train max loss: 10.304%, reg loss: 77.454%
time spent training so far: 1:40:54.422085
train attack total time: 12.226s
train attack init time: 5.491s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.212s


train attack loss increase over inner max: 1.019
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.451
total grad norm: 17.724

==================== evaluation at iteration: 363 ====================
train total loss: 83.073%
train max loss: 11.760%, reg loss: 77.686%
time spent training so far: 1:41:05.594052
train attack total time: 10.687s
train attack init time: 5.424s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.145s


train attack loss increase over inner max: 1.689
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.946
total grad norm: 24.125

==================== evaluation at iteration: 364 ====================
train total loss: 82.194%
train max loss: 10.111%, reg loss: 77.885%
time spent training so far: 1:41:19.867404
train attack total time: 13.770s
train attack init time: 6.782s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: -0.096
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.757
total grad norm: 20.328

==================== evaluation at iteration: 365 ====================
train total loss: 82.655%
train max loss: 9.818%, reg loss: 78.225%
time spent training so far: 1:41:33.930595
train attack total time: 13.614s
train attack init time: 6.970s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: 0.738
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.215
total grad norm: 13.757

==================== evaluation at iteration: 366 ====================
train total loss: 81.866%
train max loss: 8.761%, reg loss: 78.033%
time spent training so far: 1:41:48.138506
train attack total time: 13.686s
train attack init time: 7.330s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.193s


train attack loss increase over inner max: 0.223
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.445
total grad norm: 20.603

==================== evaluation at iteration: 367 ====================
train total loss: 81.351%
train max loss: 7.928%, reg loss: 77.691%
time spent training so far: 1:42:05.351795
train attack total time: 16.741s
train attack init time: 8.541s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.279s


train attack loss increase over inner max: 0.853
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.500
total grad norm: 10.899

==================== evaluation at iteration: 368 ====================
train total loss: 80.720%
train max loss: 7.413%, reg loss: 77.764%
time spent training so far: 1:42:24.629926
train attack total time: 18.758s
train attack init time: 11.538s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: 1.428
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.733
total grad norm: 15.186

==================== evaluation at iteration: 369 ====================
train total loss: 79.728%
train max loss: 3.985%, reg loss: 77.849%
time spent training so far: 1:42:46.047664
train attack total time: 20.931s
train attack init time: 14.158s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.210s


train attack loss increase over inner max: -1.672
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.123
total grad norm: 16.124

==================== evaluation at iteration: 370 ====================
train total loss: 80.201%
train max loss: 8.248%, reg loss: 77.592%
time spent training so far: 1:43:06.203081
train attack total time: 19.698s
train attack init time: 12.024s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.253s


train attack loss increase over inner max: 5.473
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.066
total grad norm: 24.453

==================== evaluation at iteration: 371 ====================
train total loss: 81.040%
train max loss: 9.006%, reg loss: 77.560%
time spent training so far: 1:43:31.830130
train attack total time: 25.080s
train attack init time: 18.240s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: 0.161
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.669
total grad norm: 42.588

==================== evaluation at iteration: 372 ====================
train total loss: 81.403%
train max loss: 5.936%, reg loss: 77.799%
time spent training so far: 1:43:53.159716
train attack total time: 20.829s
train attack init time: 12.460s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.287s


train attack loss increase over inner max: -1.136
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.441
total grad norm: 20.160

==================== evaluation at iteration: 373 ====================
train total loss: 81.129%
train max loss: 7.476%, reg loss: 77.712%
time spent training so far: 1:44:13.615776
train attack total time: 19.994s
train attack init time: 13.038s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.222s


train attack loss increase over inner max: 5.514
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.226
total grad norm: 14.922

==================== evaluation at iteration: 374 ====================
train total loss: 81.381%
train max loss: 7.240%, reg loss: 77.589%
time spent training so far: 1:44:31.758961
train attack total time: 17.680s
train attack init time: 11.776s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.172s


train attack loss increase over inner max: -0.007
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.222
total grad norm: 25.427

==================== evaluation at iteration: 375 ====================
train total loss: 82.754%
train max loss: 12.350%, reg loss: 77.478%
time spent training so far: 1:44:51.876174
train attack total time: 19.664s
train attack init time: 10.060s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.340s


train attack loss increase over inner max: 5.602
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.200% of volume
percentage infeasible at boundary: 2.16%
mean, std amount infeasible at boundary: 0.03 +/- 0.31
max amount infeasible at boundary: 5.48

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.029
total grad norm: 44.620

==================== evaluation at iteration: 376 ====================
train total loss: 83.639%
train max loss: 11.267%, reg loss: 77.961%
time spent training so far: 1:53:48.813771
train attack total time: 23.015s
train attack init time: 10.872s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.457s


train attack loss increase over inner max: -0.134
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.160
total grad norm: 15.700

==================== evaluation at iteration: 377 ====================
train total loss: 84.196%
train max loss: 10.826%, reg loss: 77.981%
time spent training so far: 1:54:05.739498
train attack total time: 16.421s
train attack init time: 9.094s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.238s


train attack loss increase over inner max: 3.348
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.080
total grad norm: 26.342

==================== evaluation at iteration: 378 ====================
train total loss: 80.841%
train max loss: 10.421%, reg loss: 77.536%
time spent training so far: 1:54:20.949407
train attack total time: 14.724s
train attack init time: 7.967s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.213s


train attack loss increase over inner max: 0.413
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.632
total grad norm: 13.817

==================== evaluation at iteration: 379 ====================
train total loss: 81.417%
train max loss: 8.913%, reg loss: 77.698%
time spent training so far: 1:54:37.500440
train attack total time: 16.043s
train attack init time: 10.366s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 2.203
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.243
total grad norm: 19.269

==================== evaluation at iteration: 380 ====================
train total loss: 81.472%
train max loss: 6.950%, reg loss: 77.531%
time spent training so far: 1:54:58.143369
train attack total time: 20.183s
train attack init time: 13.148s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.225s


train attack loss increase over inner max: -1.036
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.647
total grad norm: 19.664

==================== evaluation at iteration: 381 ====================
train total loss: 80.833%
train max loss: 5.847%, reg loss: 77.700%
time spent training so far: 1:55:19.089033
train attack total time: 20.398s
train attack init time: 11.833s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.297s


train attack loss increase over inner max: -0.380
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.304
total grad norm: 27.098

==================== evaluation at iteration: 382 ====================
train total loss: 81.298%
train max loss: 6.578%, reg loss: 77.597%
time spent training so far: 1:55:47.556020
train attack total time: 27.981s
train attack init time: 16.610s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.420s


train attack loss increase over inner max: 2.100
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.742
total grad norm: 19.467

==================== evaluation at iteration: 383 ====================
train total loss: 79.847%
train max loss: 4.079%, reg loss: 77.696%
time spent training so far: 1:56:10.056260
train attack total time: 21.976s
train attack init time: 13.560s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.288s


train attack loss increase over inner max: -1.863
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.325
total grad norm: 19.385

==================== evaluation at iteration: 384 ====================
train total loss: 79.213%
train max loss: 4.593%, reg loss: 77.488%
time spent training so far: 1:56:38.750102
train attack total time: 28.229s
train attack init time: 17.257s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.403s


train attack loss increase over inner max: 0.802
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.399
total grad norm: 33.436

==================== evaluation at iteration: 385 ====================
train total loss: 80.405%
train max loss: 4.723%, reg loss: 77.612%
time spent training so far: 1:57:02.821812
train attack total time: 23.590s
train attack init time: 13.180s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.379s


train attack loss increase over inner max: 2.985
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.557
total grad norm: 10.498

==================== evaluation at iteration: 386 ====================
train total loss: 79.703%
train max loss: 6.212%, reg loss: 77.637%
time spent training so far: 1:57:26.720812
train attack total time: 23.361s
train attack init time: 15.072s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.281s


train attack loss increase over inner max: 1.236
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.006
total grad norm: 23.285

==================== evaluation at iteration: 387 ====================
train total loss: 81.560%
train max loss: 8.705%, reg loss: 77.804%
time spent training so far: 1:57:52.525661
train attack total time: 25.321s
train attack init time: 11.888s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.514s


train attack loss increase over inner max: 2.971
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.484
total grad norm: 21.571

==================== evaluation at iteration: 388 ====================
train total loss: 81.226%
train max loss: 9.175%, reg loss: 77.598%
time spent training so far: 1:58:09.309978
train attack total time: 16.321s
train attack init time: 9.384s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.217s


train attack loss increase over inner max: 0.710
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.486
total grad norm: 34.911

==================== evaluation at iteration: 389 ====================
train total loss: 81.804%
train max loss: 9.226%, reg loss: 77.548%
time spent training so far: 1:58:27.548766
train attack total time: 17.767s
train attack init time: 9.970s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: 0.455
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.473
total grad norm: 25.780

==================== evaluation at iteration: 390 ====================
train total loss: 80.881%
train max loss: 6.687%, reg loss: 77.626%
time spent training so far: 1:58:48.097582
train attack total time: 20.080s
train attack init time: 9.381s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: 0.948
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.822
total grad norm: 30.576

==================== evaluation at iteration: 391 ====================
train total loss: 80.411%
train max loss: 5.813%, reg loss: 77.781%
time spent training so far: 1:59:13.463925
train attack total time: 24.807s
train attack init time: 14.943s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.351s


train attack loss increase over inner max: 1.305
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.889
total grad norm: 32.030

==================== evaluation at iteration: 392 ====================
train total loss: 79.490%
train max loss: 4.640%, reg loss: 77.813%
time spent training so far: 1:59:42.498615
train attack total time: 28.548s
train attack init time: 15.395s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.504s


train attack loss increase over inner max: 0.148
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.489
total grad norm: 45.403

==================== evaluation at iteration: 393 ====================
train total loss: 81.380%
train max loss: 7.354%, reg loss: 77.710%
time spent training so far: 2:00:20.542706
train attack total time: 37.544s
train attack init time: 24.941s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.477s


train attack loss increase over inner max: 4.913
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.543
total grad norm: 39.278

==================== evaluation at iteration: 394 ====================
train total loss: 82.662%
train max loss: 10.052%, reg loss: 77.644%
time spent training so far: 2:00:46.877971
train attack total time: 25.785s
train attack init time: 18.088s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.256s


train attack loss increase over inner max: 5.375
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.388
total grad norm: 34.575

==================== evaluation at iteration: 395 ====================
train total loss: 82.610%
train max loss: 8.955%, reg loss: 77.718%
time spent training so far: 2:01:14.149992
train attack total time: 26.711s
train attack init time: 18.112s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.294s


train attack loss increase over inner max: 4.243
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.594
total grad norm: 14.560

==================== evaluation at iteration: 396 ====================
train total loss: 78.907%
train max loss: 3.206%, reg loss: 77.713%
time spent training so far: 2:01:37.479807
train attack total time: 22.819s
train attack init time: 13.589s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.322s


train attack loss increase over inner max: 1.098
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.873
total grad norm: 25.102

==================== evaluation at iteration: 397 ====================
train total loss: 80.264%
train max loss: 4.309%, reg loss: 77.803%
time spent training so far: 2:01:58.465257
train attack total time: 20.485s
train attack init time: 13.040s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.243s


train attack loss increase over inner max: 0.956
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.129
total grad norm: 14.311

==================== evaluation at iteration: 398 ====================
train total loss: 80.592%
train max loss: 3.974%, reg loss: 78.075%
time spent training so far: 2:02:19.433011
train attack total time: 20.488s
train attack init time: 13.383s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.226s


train attack loss increase over inner max: 0.326
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.409
total grad norm: 18.986

==================== evaluation at iteration: 399 ====================
train total loss: 79.709%
train max loss: 4.125%, reg loss: 77.675%
time spent training so far: 2:02:42.174691
train attack total time: 22.241s
train attack init time: 14.670s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.253s


train attack loss increase over inner max: -0.373
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.583
total grad norm: 23.801

==================== evaluation at iteration: 400 ====================
train total loss: 82.768%
train max loss: 11.923%, reg loss: 77.729%
time spent training so far: 2:03:07.160101
train attack total time: 24.526s
train attack init time: 16.213s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.282s


train attack loss increase over inner max: 7.856
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.040% of volume
percentage infeasible at boundary: 2.48%
mean, std amount infeasible at boundary: 0.04 +/- 0.44
max amount infeasible at boundary: 12.11

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 3.947
total grad norm: 31.318

==================== evaluation at iteration: 401 ====================
train total loss: 83.126%
train max loss: 11.074%, reg loss: 77.926%
time spent training so far: 2:16:15.304157
train attack total time: 20.978s
train attack init time: 12.807s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.273s


train attack loss increase over inner max: 0.037
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1666]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1662]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1659]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1655]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1651]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1648]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1645]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1642]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1639]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1636]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1633]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1629]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1626]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1622]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1617]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1612]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1607]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1603]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1600]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1598]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1596]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1594]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1592]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1589]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1586]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1583]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1580]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1576]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1573]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1569]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1566]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1564]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1561]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1559]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1558]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1557]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1556]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1555]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1553]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1552]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1552]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1553]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 4.294
total grad norm: 33.347

==================== evaluation at iteration: 402 ====================
train total loss: 82.637%
train max loss: 8.021%, reg loss: 78.028%
time spent training so far: 2:16:38.057019
train attack total time: 22.249s
train attack init time: 14.230s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: 0.379
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.109
total grad norm: 21.475

==================== evaluation at iteration: 403 ====================
train total loss: 79.870%
train max loss: 4.849%, reg loss: 78.210%
time spent training so far: 2:17:07.187027
train attack total time: 28.632s
train attack init time: 19.225s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.327s


train attack loss increase over inner max: -0.059
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.033
total grad norm: 30.700

==================== evaluation at iteration: 404 ====================
train total loss: 80.763%
train max loss: 5.318%, reg loss: 78.117%
time spent training so far: 2:17:31.258269
train attack total time: 23.555s
train attack init time: 17.768s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 2.877
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.289
total grad norm: 25.885

==================== evaluation at iteration: 405 ====================
train total loss: 79.210%
train max loss: 2.317%, reg loss: 78.195%
time spent training so far: 2:18:03.923851
train attack total time: 32.121s
train attack init time: 21.127s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.404s


train attack loss increase over inner max: -1.415
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.710
total grad norm: 30.562

==================== evaluation at iteration: 406 ====================
train total loss: 79.396%
train max loss: 2.541%, reg loss: 77.997%
time spent training so far: 2:18:36.207744
train attack total time: 31.716s
train attack init time: 21.497s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.366s


train attack loss increase over inner max: -0.633
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.021
total grad norm: 36.412

==================== evaluation at iteration: 407 ====================
train total loss: 79.637%
train max loss: 2.353%, reg loss: 78.143%
time spent training so far: 2:19:10.732995
train attack total time: 34.032s
train attack init time: 26.446s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.247s


train attack loss increase over inner max: 1.965
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.204
total grad norm: 9.559

==================== evaluation at iteration: 408 ====================
train total loss: 79.670%
train max loss: 1.852%, reg loss: 78.175%
time spent training so far: 2:19:42.980011
train attack total time: 31.769s
train attack init time: 23.774s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.266s


train attack loss increase over inner max: 1.633
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.046
total grad norm: 8.257

==================== evaluation at iteration: 409 ====================
train total loss: 79.505%
train max loss: 3.548%, reg loss: 78.169%
time spent training so far: 2:20:10.123020
train attack total time: 26.672s
train attack init time: 19.118s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.249s


train attack loss increase over inner max: 0.896
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.907
total grad norm: 19.736

==================== evaluation at iteration: 410 ====================
train total loss: 81.415%
train max loss: 7.736%, reg loss: 78.127%
time spent training so far: 2:20:38.868701
train attack total time: 28.267s
train attack init time: 19.631s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.298s


train attack loss increase over inner max: 2.620
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.037
total grad norm: 17.386

==================== evaluation at iteration: 411 ====================
train total loss: 82.658%
train max loss: 8.085%, reg loss: 78.616%
time spent training so far: 2:21:03.262010
train attack total time: 23.834s
train attack init time: 17.111s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.208s


train attack loss increase over inner max: 0.693
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.691
total grad norm: 26.420

==================== evaluation at iteration: 412 ====================
train total loss: 83.863%
train max loss: 7.865%, reg loss: 78.515%
time spent training so far: 2:21:25.494899
train attack total time: 21.727s
train attack init time: 14.823s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -0.114
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.022
total grad norm: 28.216

==================== evaluation at iteration: 413 ====================
train total loss: 82.391%
train max loss: 7.739%, reg loss: 78.262%
time spent training so far: 2:21:47.603686
train attack total time: 21.582s
train attack init time: 14.224s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.236s


train attack loss increase over inner max: 0.172
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.290
total grad norm: 44.913

==================== evaluation at iteration: 414 ====================
train total loss: 82.582%
train max loss: 7.172%, reg loss: 78.218%
time spent training so far: 2:22:08.227908
train attack total time: 20.096s
train attack init time: 12.139s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.265s


train attack loss increase over inner max: 1.189
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.597
total grad norm: 35.930

==================== evaluation at iteration: 415 ====================
train total loss: 82.042%
train max loss: 6.380%, reg loss: 78.316%
time spent training so far: 2:22:31.042678
train attack total time: 22.331s
train attack init time: 14.843s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 2.429
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.252
total grad norm: 35.226

==================== evaluation at iteration: 416 ====================
train total loss: 81.290%
train max loss: 6.777%, reg loss: 78.320%
time spent training so far: 2:23:01.254810
train attack total time: 29.592s
train attack init time: 23.419s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.186s


train attack loss increase over inner max: 1.322
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.524
total grad norm: 24.614

==================== evaluation at iteration: 417 ====================
train total loss: 79.335%
train max loss: 2.182%, reg loss: 78.365%
time spent training so far: 2:23:36.470464
train attack total time: 34.712s
train attack init time: 26.562s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.271s


train attack loss increase over inner max: -1.796
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.903
total grad norm: 21.834

==================== evaluation at iteration: 418 ====================
train total loss: 79.195%
train max loss: 2.041%, reg loss: 78.229%
time spent training so far: 2:24:14.567030
train attack total time: 37.609s
train attack init time: 26.898s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.393s


train attack loss increase over inner max: -0.077
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.585
total grad norm: 22.914

==================== evaluation at iteration: 419 ====================
train total loss: 80.849%
train max loss: 6.538%, reg loss: 78.476%
time spent training so far: 2:24:58.066693
train attack total time: 42.984s
train attack init time: 34.382s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.297s


train attack loss increase over inner max: 5.008
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.675
total grad norm: 34.323

==================== evaluation at iteration: 420 ====================
train total loss: 80.443%
train max loss: 5.245%, reg loss: 78.452%
time spent training so far: 2:25:38.172526
train attack total time: 39.632s
train attack init time: 31.355s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.277s


train attack loss increase over inner max: -0.248
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.934
total grad norm: 12.957

==================== evaluation at iteration: 421 ====================
train total loss: 79.779%
train max loss: 1.412%, reg loss: 78.671%
time spent training so far: 2:26:12.876221
train attack total time: 34.100s
train attack init time: 25.375s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.302s


train attack loss increase over inner max: -0.689
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.083
total grad norm: 26.961

==================== evaluation at iteration: 422 ====================
train total loss: 79.394%
train max loss: 1.814%, reg loss: 78.336%
time spent training so far: 2:26:48.541126
train attack total time: 35.174s
train attack init time: 25.518s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.345s


train attack loss increase over inner max: 0.468
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.041
total grad norm: 14.941

==================== evaluation at iteration: 423 ====================
train total loss: 79.165%
train max loss: 1.279%, reg loss: 78.566%
time spent training so far: 2:27:20.963962
train attack total time: 31.916s
train attack init time: 25.105s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -0.003
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.421
total grad norm: 22.346

==================== evaluation at iteration: 424 ====================
train total loss: 80.527%
train max loss: 6.911%, reg loss: 78.326%
time spent training so far: 2:27:51.460418
train attack total time: 30.054s
train attack init time: 21.391s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.298s


train attack loss increase over inner max: 5.428
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.666
total grad norm: 32.649

==================== evaluation at iteration: 425 ====================
train total loss: 80.833%
train max loss: 5.633%, reg loss: 78.011%
time spent training so far: 2:28:28.520071
train attack total time: 36.536s
train attack init time: 24.427s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.453s


train attack loss increase over inner max: -2.389
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.080% of volume
percentage infeasible at boundary: 0.88%
mean, std amount infeasible at boundary: 0.01 +/- 0.20
max amount infeasible at boundary: 4.58

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.362
total grad norm: 30.396

==================== evaluation at iteration: 426 ====================
train total loss: 82.662%
train max loss: 11.062%, reg loss: 78.199%
time spent training so far: 2:52:24.356826
train attack total time: 40.001s
train attack init time: 31.194s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.306s


train attack loss increase over inner max: 8.182
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.361
total grad norm: 61.396

==================== evaluation at iteration: 427 ====================
train total loss: 81.399%
train max loss: 6.503%, reg loss: 78.209%
time spent training so far: 2:53:09.136586
train attack total time: 44.302s
train attack init time: 33.994s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.374s


train attack loss increase over inner max: -1.449
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.073
total grad norm: 19.017

==================== evaluation at iteration: 428 ====================
train total loss: 78.600%
train max loss: 0.812%, reg loss: 78.183%
time spent training so far: 2:54:12.897619
train attack total time: 63.304s
train attack init time: 40.581s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.935s


train attack loss increase over inner max: 1.970
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.276
total grad norm: 18.929

==================== evaluation at iteration: 429 ====================
train total loss: 78.700%
train max loss: 0.824%, reg loss: 78.287%
time spent training so far: 2:55:03.484703
train attack total time: 50.092s
train attack init time: 41.577s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.294s


train attack loss increase over inner max: 0.108
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.055
total grad norm: 8.442

==================== evaluation at iteration: 430 ====================
train total loss: 78.525%
train max loss: 0.654%, reg loss: 78.083%
time spent training so far: 2:55:59.265043
train attack total time: 55.325s
train attack init time: 46.543s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.302s


train attack loss increase over inner max: 0.442
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

