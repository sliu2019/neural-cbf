nohup: ignoring input
problem          : flying_inv_pend
reg_weight       : 150.0
reg_n_samples    : 250
critic_n_samples : 50
critic_max_n_steps : 20
test_N_volume_samples : 2500
test_N_boundary_samples : 2500
learner_stopping_condition : n_steps
learner_early_stopping_patience : 100
learner_n_steps  : 3000
learner_lr       : 0.001
random_seed      : 1
affix            : repro_after_round1_edits
log_root         : log
model_root       : checkpoint
n_checkpoint_step : 5
n_test_loss_step : 25
gpu              : 0
log_folder       : log/flying_inv_pend_repro_after_round1_edits
model_folder     : checkpoint/flying_inv_pend_repro_after_round1_edits
Reg grad norm: 0.423
total grad norm: 6.321

==================== evaluation at iteration: 0 ====================
train total loss: 95.123%
train max loss: 27.705%, reg loss: 75.574%
time spent training so far: 0:00:26.928259
train attack total time: 26.588s
train attack init time: 2.897s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.656s


train attack loss increase over inner max: 14.579
OOM debug. Mem allocated and reserved: 727552.000000, 2097152.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.680% of volume
percentage infeasible at boundary: 31.40%
mean, std amount infeasible at boundary: 2.63 +/- 5.34
max amount infeasible at boundary: 36.79

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.531
total grad norm: 7.073

==================== evaluation at iteration: 1 ====================
train total loss: 97.868%
train max loss: 35.178%, reg loss: 75.420%
time spent training so far: 0:03:18.968423
train attack total time: 16.707s
train attack init time: 3.016s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.354s


train attack loss increase over inner max: 4.080
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.546
total grad norm: 7.395

==================== evaluation at iteration: 2 ====================
train total loss: 96.530%
train max loss: 30.227%, reg loss: 75.553%
time spent training so far: 0:03:40.838101
train attack total time: 21.471s
train attack init time: 3.132s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.506s


train attack loss increase over inner max: -1.363
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.586
total grad norm: 5.284

==================== evaluation at iteration: 3 ====================
train total loss: 91.956%
train max loss: 26.282%, reg loss: 75.499%
time spent training so far: 0:04:03.256081
train attack total time: 22.060s
train attack init time: 2.991s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.530s


train attack loss increase over inner max: -0.551
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.590
total grad norm: 10.047

==================== evaluation at iteration: 4 ====================
train total loss: 89.201%
train max loss: 24.792%, reg loss: 75.138%
time spent training so far: 0:04:19.773115
train attack total time: 16.200s
train attack init time: 2.849s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.343s


train attack loss increase over inner max: -144.454
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.583
total grad norm: 6.461

==================== evaluation at iteration: 5 ====================
train total loss: 91.001%
train max loss: 24.095%, reg loss: 75.658%
time spent training so far: 0:04:38.180424
train attack total time: 18.067s
train attack init time: 2.770s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.407s


train attack loss increase over inner max: 3.498
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.602
total grad norm: 5.412

==================== evaluation at iteration: 6 ====================
train total loss: 88.917%
train max loss: 21.087%, reg loss: 75.400%
time spent training so far: 0:05:00.371978
train attack total time: 21.847s
train attack init time: 3.023s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.521s


train attack loss increase over inner max: -1.473
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.694
total grad norm: 5.529

==================== evaluation at iteration: 7 ====================
train total loss: 88.021%
train max loss: 22.650%, reg loss: 75.273%
time spent training so far: 0:05:14.647275
train attack total time: 13.884s
train attack init time: 2.611s
train attack avg grad step time: 0.081s
train attack avg reproj time: 0.280s


train attack loss increase over inner max: 3.877
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.645
total grad norm: 5.673

==================== evaluation at iteration: 8 ====================
train total loss: 86.448%
train max loss: 20.647%, reg loss: 75.251%
time spent training so far: 0:05:30.209513
train attack total time: 15.200s
train attack init time: 2.968s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.321s


train attack loss increase over inner max: -0.393
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.706
total grad norm: 4.038

==================== evaluation at iteration: 9 ====================
train total loss: 88.860%
train max loss: 21.069%, reg loss: 75.413%
time spent training so far: 0:05:44.022066
train attack total time: 13.429s
train attack init time: 2.835s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.265s


train attack loss increase over inner max: 0.978
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.723
total grad norm: 5.688

==================== evaluation at iteration: 10 ====================
train total loss: 86.209%
train max loss: 20.196%, reg loss: 75.219%
time spent training so far: 0:05:57.927199
train attack total time: 13.495s
train attack init time: 2.492s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.281s


train attack loss increase over inner max: -1.276
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.775
total grad norm: 5.300

==================== evaluation at iteration: 11 ====================
train total loss: 86.266%
train max loss: 19.387%, reg loss: 75.379%
time spent training so far: 0:06:08.555536
train attack total time: 10.283s
train attack init time: 2.331s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.181s


train attack loss increase over inner max: -0.570
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.702
total grad norm: 6.653

==================== evaluation at iteration: 12 ====================
train total loss: 87.386%
train max loss: 19.507%, reg loss: 75.450%
time spent training so far: 0:06:24.002265
train attack total time: 15.054s
train attack init time: 2.464s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.335s


train attack loss increase over inner max: -0.299
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.952
total grad norm: 6.854

==================== evaluation at iteration: 13 ====================
train total loss: 88.304%
train max loss: 22.238%, reg loss: 75.380%
time spent training so far: 0:06:39.356976
train attack total time: 15.014s
train attack init time: 2.230s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.341s


train attack loss increase over inner max: 2.916
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.008
total grad norm: 10.503

==================== evaluation at iteration: 14 ====================
train total loss: 87.932%
train max loss: 21.576%, reg loss: 75.408%
time spent training so far: 0:06:54.740072
train attack total time: 15.043s
train attack init time: 2.719s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: -0.272
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.737
total grad norm: 9.377

==================== evaluation at iteration: 15 ====================
train total loss: 87.474%
train max loss: 21.218%, reg loss: 75.255%
time spent training so far: 0:07:06.686778
train attack total time: 11.600s
train attack init time: 2.213s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.227s


train attack loss increase over inner max: -0.275
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.803
total grad norm: 12.688

==================== evaluation at iteration: 16 ====================
train total loss: 88.902%
train max loss: 20.127%, reg loss: 75.347%
time spent training so far: 0:07:21.258964
train attack total time: 14.225s
train attack init time: 2.524s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.304s


train attack loss increase over inner max: -0.678
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.663
total grad norm: 6.272

==================== evaluation at iteration: 17 ====================
train total loss: 86.912%
train max loss: 21.365%, reg loss: 75.277%
time spent training so far: 0:07:34.621116
train attack total time: 12.975s
train attack init time: 2.706s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.269s


train attack loss increase over inner max: 0.809
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.667
total grad norm: 7.172

==================== evaluation at iteration: 18 ====================
train total loss: 87.260%
train max loss: 23.249%, reg loss: 75.257%
time spent training so far: 0:07:56.980847
train attack total time: 21.990s
train attack init time: 2.839s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.572s


train attack loss increase over inner max: 1.341
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.592
total grad norm: 6.587

==================== evaluation at iteration: 19 ====================
train total loss: 87.494%
train max loss: 23.954%, reg loss: 75.342%
time spent training so far: 0:08:07.012430
train attack total time: 9.705s
train attack init time: 2.459s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.164s


train attack loss increase over inner max: 2.440
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.586
total grad norm: 10.183

==================== evaluation at iteration: 20 ====================
train total loss: 87.617%
train max loss: 25.611%, reg loss: 75.276%
time spent training so far: 0:08:30.496629
train attack total time: 23.139s
train attack init time: 2.808s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.615s


train attack loss increase over inner max: 2.340
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.530
total grad norm: 7.385

==================== evaluation at iteration: 21 ====================
train total loss: 88.407%
train max loss: 26.318%, reg loss: 75.291%
time spent training so far: 0:08:43.092739
train attack total time: 12.219s
train attack init time: 2.337s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.254s


train attack loss increase over inner max: 2.432
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.411
total grad norm: 7.216

==================== evaluation at iteration: 22 ====================
train total loss: 89.210%
train max loss: 24.659%, reg loss: 75.235%
time spent training so far: 0:08:57.133702
train attack total time: 13.676s
train attack init time: 2.543s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.296s


train attack loss increase over inner max: 0.144
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.447
total grad norm: 5.608

==================== evaluation at iteration: 23 ====================
train total loss: 87.742%
train max loss: 22.391%, reg loss: 75.390%
time spent training so far: 0:09:14.880297
train attack total time: 17.405s
train attack init time: 2.806s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.417s


train attack loss increase over inner max: -48.520
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.376
total grad norm: 6.747

==================== evaluation at iteration: 24 ====================
train total loss: 88.366%
train max loss: 24.428%, reg loss: 75.292%
time spent training so far: 0:09:29.465861
train attack total time: 14.249s
train attack init time: 2.400s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.321s


train attack loss increase over inner max: 2.524
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.387
total grad norm: 5.806

==================== evaluation at iteration: 25 ====================
train total loss: 87.873%
train max loss: 22.538%, reg loss: 75.252%
time spent training so far: 0:09:40.631951
train attack total time: 10.827s
train attack init time: 2.795s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.188s


train attack loss increase over inner max: -1.572
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 5.200% of volume
percentage infeasible at boundary: 28.88%
mean, std amount infeasible at boundary: 1.46 +/- 3.14
max amount infeasible at boundary: 42.19

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.287
total grad norm: 3.512

==================== evaluation at iteration: 26 ====================
train total loss: 86.334%
train max loss: 21.186%, reg loss: 75.042%
time spent training so far: 0:12:05.918000
train attack total time: 16.171s
train attack init time: 2.450s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.385s


train attack loss increase over inner max: -0.550
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.286
total grad norm: 4.399

==================== evaluation at iteration: 27 ====================
train total loss: 86.679%
train max loss: 19.577%, reg loss: 75.365%
time spent training so far: 0:12:20.762565
train attack total time: 14.485s
train attack init time: 2.272s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.349s


train attack loss increase over inner max: -1.871
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.274
total grad norm: 4.909

==================== evaluation at iteration: 28 ====================
train total loss: 86.563%
train max loss: 18.769%, reg loss: 75.087%
time spent training so far: 0:12:35.548324
train attack total time: 14.421s
train attack init time: 2.199s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.349s


train attack loss increase over inner max: -0.953
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.219
total grad norm: 3.897

==================== evaluation at iteration: 29 ====================
train total loss: 87.481%
train max loss: 19.648%, reg loss: 75.120%
time spent training so far: 0:12:48.439550
train attack total time: 12.555s
train attack init time: 2.636s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: 0.901
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.176
total grad norm: 4.011

==================== evaluation at iteration: 30 ====================
train total loss: 87.660%
train max loss: 22.448%, reg loss: 74.962%
time spent training so far: 0:13:04.432798
train attack total time: 15.633s
train attack init time: 2.415s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.385s


train attack loss increase over inner max: 2.955
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.185
total grad norm: 2.947

==================== evaluation at iteration: 31 ====================
train total loss: 86.854%
train max loss: 22.097%, reg loss: 75.259%
time spent training so far: 0:13:18.256128
train attack total time: 13.464s
train attack init time: 2.550s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: -0.278
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.153
total grad norm: 3.766

==================== evaluation at iteration: 32 ====================
train total loss: 87.044%
train max loss: 21.445%, reg loss: 75.091%
time spent training so far: 0:13:31.184782
train attack total time: 12.587s
train attack init time: 2.327s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.278
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.157
total grad norm: 3.826

==================== evaluation at iteration: 33 ====================
train total loss: 88.526%
train max loss: 21.016%, reg loss: 75.068%
time spent training so far: 0:13:44.408565
train attack total time: 12.858s
train attack init time: 2.857s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.481
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.158
total grad norm: 3.847

==================== evaluation at iteration: 34 ====================
train total loss: 87.385%
train max loss: 20.795%, reg loss: 75.076%
time spent training so far: 0:13:56.406718
train attack total time: 11.610s
train attack init time: 2.698s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.231s


train attack loss increase over inner max: 0.016
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.193
total grad norm: 2.926

==================== evaluation at iteration: 35 ====================
train total loss: 86.942%
train max loss: 20.335%, reg loss: 75.063%
time spent training so far: 0:14:08.854254
train attack total time: 12.097s
train attack init time: 2.450s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: -0.184
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.175
total grad norm: 3.259

==================== evaluation at iteration: 36 ====================
train total loss: 86.063%
train max loss: 19.533%, reg loss: 74.988%
time spent training so far: 0:14:20.842594
train attack total time: 11.590s
train attack init time: 2.544s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.237s


train attack loss increase over inner max: -0.430
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.241
total grad norm: 4.673

==================== evaluation at iteration: 37 ====================
train total loss: 86.251%
train max loss: 19.211%, reg loss: 75.338%
time spent training so far: 0:14:32.262721
train attack total time: 11.077s
train attack init time: 2.610s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -0.425
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.251
total grad norm: 3.293

==================== evaluation at iteration: 38 ====================
train total loss: 86.594%
train max loss: 18.767%, reg loss: 75.182%
time spent training so far: 0:14:47.239946
train attack total time: 14.578s
train attack init time: 2.554s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.343s


train attack loss increase over inner max: -0.056
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.259
total grad norm: 4.939

==================== evaluation at iteration: 39 ====================
train total loss: 87.254%
train max loss: 21.311%, reg loss: 75.213%
time spent training so far: 0:14:59.802943
train attack total time: 12.178s
train attack init time: 2.424s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.275s


train attack loss increase over inner max: 2.483
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.311
total grad norm: 2.654

==================== evaluation at iteration: 40 ====================
train total loss: 85.270%
train max loss: 20.444%, reg loss: 75.257%
time spent training so far: 0:15:12.512505
train attack total time: 12.350s
train attack init time: 2.473s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.400
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.327
total grad norm: 3.889

==================== evaluation at iteration: 41 ====================
train total loss: 87.574%
train max loss: 19.580%, reg loss: 75.283%
time spent training so far: 0:15:28.180932
train attack total time: 15.301s
train attack init time: 2.481s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.390s


train attack loss increase over inner max: -0.550
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.354
total grad norm: 7.720

==================== evaluation at iteration: 42 ====================
train total loss: 85.095%
train max loss: 18.528%, reg loss: 75.147%
time spent training so far: 0:15:39.405754
train attack total time: 10.861s
train attack init time: 2.355s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.231s


train attack loss increase over inner max: -0.735
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.354
total grad norm: 4.394

==================== evaluation at iteration: 43 ====================
train total loss: 86.666%
train max loss: 22.874%, reg loss: 75.153%
time spent training so far: 0:15:50.607264
train attack total time: 10.849s
train attack init time: 2.326s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.230s


train attack loss increase over inner max: 4.907
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
At initialization: k0 is 0.002793
Max_n_steps: 30
Parameter containing:
tensor([[0.0038]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0066]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 29
Parameter containing:
tensor([[0.0048]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0056]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0058]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0046]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0068]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0036]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Attack: reprojection exited on timeout, max dist from =0 boundary:  3.6879425048828125
Parameter containing:
tensor([[0.0078]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0026]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0088]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0016]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 29
Parameter containing:
tensor([[0.0098]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.0006]], device='cuda:0', requires_grad=True)
Max_n_steps: 29
Parameter containing:
tensor([[0.0108]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0118]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0128]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0138]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 28
Parameter containing:
tensor([[0.0148]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0157]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0168]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0178]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 28
Parameter containing:
tensor([[0.0188]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 28
Parameter containing:
tensor([[0.0198]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0209]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0219]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0229]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0239]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 27
Parameter containing:
tensor([[0.0248]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0257]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Attack: reprojection exited on timeout, max dist from =0 boundary:  3.576791763305664
Parameter containing:
tensor([[0.0267]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0275]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 27
Parameter containing:
tensor([[0.0284]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 27
Parameter containing:
tensor([[0.0292]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0300]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0307]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0314]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0321]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 26
Parameter containing:
tensor([[0.0328]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0334]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0340]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0345]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0350]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 26
Parameter containing:
tensor([[0.0355]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0360]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 26
Parameter containing:
tensor([[0.0365]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0369]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0373]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 25
Parameter containing:
tensor([[0.0377]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0382]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0386]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.251
total grad norm: 5.458

==================== evaluation at iteration: 44 ====================
train total loss: 86.114%
train max loss: 22.346%, reg loss: 75.073%
time spent training so far: 0:16:04.519015
train attack total time: 13.582s
train attack init time: 2.331s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: -0.992
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.355
total grad norm: 4.315

==================== evaluation at iteration: 45 ====================
train total loss: 85.752%
train max loss: 20.906%, reg loss: 75.088%
time spent training so far: 0:16:17.776688
train attack total time: 12.878s
train attack init time: 2.494s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.295s


train attack loss increase over inner max: -1.323
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.253
total grad norm: 5.949

==================== evaluation at iteration: 46 ====================
train total loss: 87.168%
train max loss: 19.681%, reg loss: 75.056%
time spent training so far: 0:16:32.615146
train attack total time: 14.454s
train attack init time: 2.432s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.359s


train attack loss increase over inner max: -1.455
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.274
total grad norm: 2.407

==================== evaluation at iteration: 47 ====================
train total loss: 85.333%
train max loss: 20.691%, reg loss: 75.102%
time spent training so far: 0:16:47.645925
train attack total time: 14.638s
train attack init time: 2.476s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.363s


train attack loss increase over inner max: 1.164
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.275
total grad norm: 2.362

==================== evaluation at iteration: 48 ====================
train total loss: 86.569%
train max loss: 21.298%, reg loss: 75.194%
time spent training so far: 0:16:59.620104
train attack total time: 11.610s
train attack init time: 2.441s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.254s


train attack loss increase over inner max: 0.295
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.257
total grad norm: 3.086

==================== evaluation at iteration: 49 ====================
train total loss: 85.829%
train max loss: 20.846%, reg loss: 75.113%
time spent training so far: 0:17:09.402219
train attack total time: 9.453s
train attack init time: 2.122s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.185s


train attack loss increase over inner max: -0.290
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.290
total grad norm: 4.132

==================== evaluation at iteration: 50 ====================
train total loss: 86.414%
train max loss: 19.537%, reg loss: 75.175%
time spent training so far: 0:17:25.239987
train attack total time: 15.502s
train attack init time: 2.533s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.395s


train attack loss increase over inner max: -1.590
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 6.360% of volume
percentage infeasible at boundary: 26.88%
mean, std amount infeasible at boundary: 1.29 +/- 2.91
max amount infeasible at boundary: 26.75

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.281
total grad norm: 6.248

==================== evaluation at iteration: 51 ====================
train total loss: 86.625%
train max loss: 20.878%, reg loss: 75.073%
time spent training so far: 0:19:47.592838
train attack total time: 16.750s
train attack init time: 2.642s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.432s


train attack loss increase over inner max: 1.160
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.321
total grad norm: 4.101

==================== evaluation at iteration: 52 ====================
train total loss: 87.526%
train max loss: 20.732%, reg loss: 75.022%
time spent training so far: 0:20:01.639010
train attack total time: 13.633s
train attack init time: 2.594s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.337s


train attack loss increase over inner max: -0.436
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.295
total grad norm: 5.807

==================== evaluation at iteration: 53 ====================
train total loss: 87.487%
train max loss: 20.396%, reg loss: 74.971%
time spent training so far: 0:20:14.592466
train attack total time: 12.587s
train attack init time: 2.901s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.285s


train attack loss increase over inner max: -0.562
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.294
total grad norm: 6.760

==================== evaluation at iteration: 54 ====================
train total loss: 89.026%
train max loss: 25.423%, reg loss: 75.186%
time spent training so far: 0:20:30.071483
train attack total time: 15.080s
train attack init time: 2.495s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.397s


train attack loss increase over inner max: 4.845
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.316
total grad norm: 3.496

==================== evaluation at iteration: 55 ====================
train total loss: 88.131%
train max loss: 25.320%, reg loss: 75.120%
time spent training so far: 0:20:41.603473
train attack total time: 11.192s
train attack init time: 2.213s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: -0.009
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.325
total grad norm: 3.514

==================== evaluation at iteration: 56 ====================
train total loss: 88.635%
train max loss: 24.781%, reg loss: 75.135%
time spent training so far: 0:20:53.933287
train attack total time: 11.990s
train attack init time: 2.556s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.275s


train attack loss increase over inner max: -0.151
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.302
total grad norm: 2.571

==================== evaluation at iteration: 57 ====================
train total loss: 87.489%
train max loss: 24.433%, reg loss: 75.361%
time spent training so far: 0:21:06.231464
train attack total time: 11.943s
train attack init time: 2.316s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.285s


train attack loss increase over inner max: -0.464
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.353
total grad norm: 3.989

==================== evaluation at iteration: 58 ====================
train total loss: 88.105%
train max loss: 23.953%, reg loss: 75.121%
time spent training so far: 0:21:20.046247
train attack total time: 13.443s
train attack init time: 2.318s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.340s


train attack loss increase over inner max: -0.741
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.342
total grad norm: 2.438

==================== evaluation at iteration: 59 ====================
train total loss: 87.710%
train max loss: 22.741%, reg loss: 75.415%
time spent training so far: 0:21:31.315377
train attack total time: 10.943s
train attack init time: 2.014s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.257s


train attack loss increase over inner max: -1.263
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.323
total grad norm: 5.674

==================== evaluation at iteration: 60 ====================
train total loss: 86.548%
train max loss: 21.067%, reg loss: 75.196%
time spent training so far: 0:21:46.106268
train attack total time: 14.440s
train attack init time: 2.598s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.367s


train attack loss increase over inner max: -2.047
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.338
total grad norm: 5.265

==================== evaluation at iteration: 61 ====================
train total loss: 86.446%
train max loss: 19.691%, reg loss: 75.337%
time spent training so far: 0:22:02.727838
train attack total time: 16.241s
train attack init time: 2.843s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.424s


train attack loss increase over inner max: -1.326
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.219
total grad norm: 2.554

==================== evaluation at iteration: 62 ====================
train total loss: 85.583%
train max loss: 19.651%, reg loss: 75.144%
time spent training so far: 0:22:16.754691
train attack total time: 13.675s
train attack init time: 2.955s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.325s


train attack loss increase over inner max: -0.183
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.230
total grad norm: 3.295

==================== evaluation at iteration: 63 ====================
train total loss: 85.473%
train max loss: 18.575%, reg loss: 75.268%
time spent training so far: 0:22:32.828720
train attack total time: 15.694s
train attack init time: 2.699s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.413s


train attack loss increase over inner max: -0.984
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.150
total grad norm: 3.247

==================== evaluation at iteration: 64 ====================
train total loss: 85.978%
train max loss: 18.116%, reg loss: 74.991%
time spent training so far: 0:22:46.511940
train attack total time: 13.344s
train attack init time: 2.410s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.333s


train attack loss increase over inner max: 0.032
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.158
total grad norm: 2.888

==================== evaluation at iteration: 65 ====================
train total loss: 85.866%
train max loss: 17.906%, reg loss: 75.092%
time spent training so far: 0:23:01.494124
train attack total time: 14.631s
train attack init time: 2.431s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.383s


train attack loss increase over inner max: -0.158
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.148
total grad norm: 2.957

==================== evaluation at iteration: 66 ====================
train total loss: 86.603%
train max loss: 19.717%, reg loss: 75.007%
time spent training so far: 0:23:16.005795
train attack total time: 14.162s
train attack init time: 2.217s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: 2.150
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.135
total grad norm: 4.710

==================== evaluation at iteration: 67 ====================
train total loss: 84.885%
train max loss: 19.029%, reg loss: 75.027%
time spent training so far: 0:23:28.249061
train attack total time: 11.902s
train attack init time: 2.137s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.288s


train attack loss increase over inner max: -0.391
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.135
total grad norm: 3.208

==================== evaluation at iteration: 68 ====================
train total loss: 85.171%
train max loss: 18.153%, reg loss: 75.116%
time spent training so far: 0:23:39.511643
train attack total time: 10.919s
train attack init time: 2.484s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.239s


train attack loss increase over inner max: -0.609
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.188
total grad norm: 4.637

==================== evaluation at iteration: 69 ====================
train total loss: 86.948%
train max loss: 20.894%, reg loss: 75.130%
time spent training so far: 0:23:58.499016
train attack total time: 18.654s
train attack init time: 2.165s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.572s


train attack loss increase over inner max: 2.989
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.188
total grad norm: 5.223

==================== evaluation at iteration: 70 ====================
train total loss: 87.166%
train max loss: 23.147%, reg loss: 75.054%
time spent training so far: 0:24:09.523063
train attack total time: 10.698s
train attack init time: 2.140s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.255s


train attack loss increase over inner max: 1.916
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.237
total grad norm: 5.893

==================== evaluation at iteration: 71 ====================
train total loss: 88.667%
train max loss: 23.237%, reg loss: 75.114%
time spent training so far: 0:24:23.936006
train attack total time: 14.021s
train attack init time: 2.233s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.385s


train attack loss increase over inner max: -0.025
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.241
total grad norm: 3.922

==================== evaluation at iteration: 72 ====================
train total loss: 87.652%
train max loss: 22.074%, reg loss: 75.158%
time spent training so far: 0:24:38.605457
train attack total time: 14.314s
train attack init time: 2.294s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: -0.213
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.270
total grad norm: 5.616

==================== evaluation at iteration: 73 ====================
train total loss: 86.984%
train max loss: 21.109%, reg loss: 74.987%
time spent training so far: 0:24:49.932236
train attack total time: 10.955s
train attack init time: 2.184s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: -0.992
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.214
total grad norm: 3.253

==================== evaluation at iteration: 74 ====================
train total loss: 87.043%
train max loss: 20.740%, reg loss: 75.002%
time spent training so far: 0:25:01.109202
train attack total time: 10.836s
train attack init time: 2.174s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: -0.551
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.224
total grad norm: 2.925

==================== evaluation at iteration: 75 ====================
train total loss: 86.525%
train max loss: 20.202%, reg loss: 75.100%
time spent training so far: 0:25:12.172200
train attack total time: 10.671s
train attack init time: 2.266s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.249s


train attack loss increase over inner max: -0.540
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.920% of volume
percentage infeasible at boundary: 25.52%
mean, std amount infeasible at boundary: 1.26 +/- 2.94
max amount infeasible at boundary: 23.41

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.287
total grad norm: 3.250

==================== evaluation at iteration: 76 ====================
train total loss: 87.456%
train max loss: 19.832%, reg loss: 74.995%
time spent training so far: 0:27:32.962834
train attack total time: 12.530s
train attack init time: 2.479s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.314s


train attack loss increase over inner max: -0.789
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.239
total grad norm: 3.942

==================== evaluation at iteration: 77 ====================
train total loss: 84.899%
train max loss: 18.976%, reg loss: 75.105%
time spent training so far: 0:27:44.973952
train attack total time: 11.658s
train attack init time: 2.689s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.827
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.247
total grad norm: 4.524

==================== evaluation at iteration: 78 ====================
train total loss: 85.018%
train max loss: 18.680%, reg loss: 75.061%
time spent training so far: 0:27:58.218599
train attack total time: 12.864s
train attack init time: 3.068s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: -0.583
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.237
total grad norm: 4.808

==================== evaluation at iteration: 79 ====================
train total loss: 87.356%
train max loss: 20.540%, reg loss: 75.071%
time spent training so far: 0:28:10.876819
train attack total time: 12.211s
train attack init time: 2.382s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.300s


train attack loss increase over inner max: 2.400
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.194
total grad norm: 2.660

==================== evaluation at iteration: 80 ====================
train total loss: 86.959%
train max loss: 22.725%, reg loss: 74.974%
time spent training so far: 0:28:24.238566
train attack total time: 12.926s
train attack init time: 2.439s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: 2.656
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.251
total grad norm: 4.517

==================== evaluation at iteration: 81 ====================
train total loss: 85.529%
train max loss: 22.042%, reg loss: 75.132%
time spent training so far: 0:28:35.820341
train attack total time: 11.194s
train attack init time: 2.510s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.260s


train attack loss increase over inner max: -0.297
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.192
total grad norm: 4.245

==================== evaluation at iteration: 82 ====================
train total loss: 87.300%
train max loss: 21.468%, reg loss: 75.255%
time spent training so far: 0:28:48.612358
train attack total time: 12.413s
train attack init time: 2.689s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.298s


train attack loss increase over inner max: -0.461
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.256
total grad norm: 4.755

==================== evaluation at iteration: 83 ====================
train total loss: 88.429%
train max loss: 20.937%, reg loss: 75.198%
time spent training so far: 0:29:01.421951
train attack total time: 12.419s
train attack init time: 2.723s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.299s


train attack loss increase over inner max: -0.113
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.246
total grad norm: 3.607

==================== evaluation at iteration: 84 ====================
train total loss: 86.144%
train max loss: 20.547%, reg loss: 75.078%
time spent training so far: 0:29:14.660892
train attack total time: 12.847s
train attack init time: 2.683s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.316s


train attack loss increase over inner max: -0.602
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.266
total grad norm: 4.078

==================== evaluation at iteration: 85 ====================
train total loss: 86.551%
train max loss: 20.048%, reg loss: 75.213%
time spent training so far: 0:29:26.451866
train attack total time: 11.409s
train attack init time: 2.384s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.271s


train attack loss increase over inner max: -0.403
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.288
total grad norm: 4.470

==================== evaluation at iteration: 86 ====================
train total loss: 87.211%
train max loss: 20.534%, reg loss: 75.220%
time spent training so far: 0:29:38.708904
train attack total time: 11.882s
train attack init time: 2.622s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: 0.771
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.327
total grad norm: 5.097

==================== evaluation at iteration: 87 ====================
train total loss: 88.216%
train max loss: 20.555%, reg loss: 75.270%
time spent training so far: 0:29:51.333679
train attack total time: 12.250s
train attack init time: 2.393s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.305s


train attack loss increase over inner max: -0.563
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000

Max_n_steps: 25
Parameter containing:
tensor([[0.0390]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0395]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 25
Parameter containing:
tensor([[0.0399]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0404]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0408]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Parameter containing:
tensor([[0.0412]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 25
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.11706492304801941
Parameter containing:
tensor([[0.0416]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 25
Parameter containing:
tensor([[0.0420]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0423]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0427]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0431]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0435]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 24
Parameter containing:
tensor([[0.0438]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0441]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0444]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0447]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0449]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 24
Parameter containing:
tensor([[0.0452]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.1189926415681839
Parameter containing:
tensor([[0.0456]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0459]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0462]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0464]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 24
Parameter containing:
tensor([[0.0467]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0470]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 24
Parameter containing:
tensor([[0.0473]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0476]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0478]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Parameter containing:
tensor([[0.0481]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0484]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0486]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0489]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0492]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.14134429395198822
Parameter containing:
tensor([[0.0494]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0496]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.2101573795080185
Parameter containing:
tensor([[0.0499]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0501]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0503]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Parameter containing:
tensor([[0.0505]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0506]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0507]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0508]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0509]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 23
Parameter containing:
tensor([[0.0510]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0511]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.353
total grad norm: 3.105

==================== evaluation at iteration: 88 ====================
train total loss: 87.095%
train max loss: 20.105%, reg loss: 75.092%
time spent training so far: 0:30:06.012284
train attack total time: 14.297s
train attack init time: 2.699s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: 0.140
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.345
total grad norm: 4.387

==================== evaluation at iteration: 89 ====================
train total loss: 86.245%
train max loss: 18.743%, reg loss: 75.154%
time spent training so far: 0:30:17.999934
train attack total time: 11.578s
train attack init time: 2.726s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: -0.691
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.290
total grad norm: 3.715

==================== evaluation at iteration: 90 ====================
train total loss: 85.137%
train max loss: 17.876%, reg loss: 75.074%
time spent training so far: 0:30:30.832258
train attack total time: 12.432s
train attack init time: 3.012s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.287s


train attack loss increase over inner max: -1.179
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.316
total grad norm: 5.308

==================== evaluation at iteration: 91 ====================
train total loss: 86.383%
train max loss: 19.279%, reg loss: 75.071%
time spent training so far: 0:30:41.608646
train attack total time: 10.431s
train attack init time: 2.342s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.251s


train attack loss increase over inner max: 1.071
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.270
total grad norm: 3.308

==================== evaluation at iteration: 92 ====================
train total loss: 86.993%
train max loss: 19.725%, reg loss: 75.062%
time spent training so far: 0:30:51.864165
train attack total time: 9.930s
train attack init time: 2.413s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.225s


train attack loss increase over inner max: 0.844
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.209
total grad norm: 3.588

==================== evaluation at iteration: 93 ====================
train total loss: 87.242%
train max loss: 19.850%, reg loss: 75.010%
time spent training so far: 0:31:04.836728
train attack total time: 12.643s
train attack init time: 2.236s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.347s


train attack loss increase over inner max: -0.245
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.205
total grad norm: 3.681

==================== evaluation at iteration: 94 ====================
train total loss: 85.882%
train max loss: 20.074%, reg loss: 75.082%
time spent training so far: 0:31:18.147766
train attack total time: 12.987s
train attack init time: 2.145s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.360s


train attack loss increase over inner max: 0.384
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.201
total grad norm: 4.145

==================== evaluation at iteration: 95 ====================
train total loss: 85.754%
train max loss: 19.321%, reg loss: 75.104%
time spent training so far: 0:31:29.914110
train attack total time: 11.410s
train attack init time: 2.307s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.292s


train attack loss increase over inner max: -0.501
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.124
total grad norm: 2.009

==================== evaluation at iteration: 96 ====================
train total loss: 87.182%
train max loss: 19.701%, reg loss: 75.018%
time spent training so far: 0:31:40.335681
train attack total time: 10.045s
train attack init time: 2.492s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.228s


train attack loss increase over inner max: 0.581
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.140
total grad norm: 2.370

==================== evaluation at iteration: 97 ====================
train total loss: 87.472%
train max loss: 19.676%, reg loss: 75.168%
time spent training so far: 0:31:50.871713
train attack total time: 10.182s
train attack init time: 2.454s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.234s


train attack loss increase over inner max: 0.378
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.113
total grad norm: 2.406

==================== evaluation at iteration: 98 ====================
train total loss: 85.183%
train max loss: 18.718%, reg loss: 74.850%
time spent training so far: 0:32:03.547526
train attack total time: 12.334s
train attack init time: 2.308s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.332s


train attack loss increase over inner max: -1.011
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.125
total grad norm: 2.524

==================== evaluation at iteration: 99 ====================
train total loss: 88.744%
train max loss: 26.456%, reg loss: 75.116%
time spent training so far: 0:32:14.109262
train attack total time: 10.206s
train attack init time: 2.349s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 5.554
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.139
total grad norm: 2.729

==================== evaluation at iteration: 100 ====================
train total loss: 87.013%
train max loss: 25.899%, reg loss: 74.995%
time spent training so far: 0:32:27.928162
train attack total time: 13.470s
train attack init time: 2.558s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.368s


train attack loss increase over inner max: -0.607
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 6.320% of volume
percentage infeasible at boundary: 26.72%
mean, std amount infeasible at boundary: 1.29 +/- 2.92
max amount infeasible at boundary: 26.53

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.168
total grad norm: 2.537

==================== evaluation at iteration: 101 ====================
train total loss: 88.247%
train max loss: 25.319%, reg loss: 75.290%
time spent training so far: 0:34:37.943185
train attack total time: 10.964s
train attack init time: 2.265s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.276s


train attack loss increase over inner max: -0.827
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.167
total grad norm: 2.953

==================== evaluation at iteration: 102 ====================
train total loss: 88.293%
train max loss: 24.178%, reg loss: 75.144%
time spent training so far: 0:34:52.333982
train attack total time: 14.042s
train attack init time: 2.529s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.392s


train attack loss increase over inner max: -1.308
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.229
total grad norm: 3.084

==================== evaluation at iteration: 103 ====================
train total loss: 87.833%
train max loss: 23.385%, reg loss: 75.084%
time spent training so far: 0:35:05.533479
train attack total time: 12.833s
train attack init time: 2.253s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.352s


train attack loss increase over inner max: -1.281
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.238
total grad norm: 3.371

==================== evaluation at iteration: 104 ====================
train total loss: 89.214%
train max loss: 22.802%, reg loss: 74.980%
time spent training so far: 0:35:18.083604
train attack total time: 12.191s
train attack init time: 2.269s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.327s


train attack loss increase over inner max: -0.731
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.255
total grad norm: 4.922

==================== evaluation at iteration: 105 ====================
train total loss: 87.977%
train max loss: 23.227%, reg loss: 75.161%
time spent training so far: 0:35:30.900861
train attack total time: 12.475s
train attack init time: 2.286s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.338s


train attack loss increase over inner max: 0.706
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.323
total grad norm: 3.257

==================== evaluation at iteration: 106 ====================
train total loss: 87.265%
train max loss: 22.836%, reg loss: 75.136%
time spent training so far: 0:35:40.915168
train attack total time: 9.646s
train attack init time: 2.163s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.225s


train attack loss increase over inner max: -0.120
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.356
total grad norm: 4.261

==================== evaluation at iteration: 107 ====================
train total loss: 87.919%
train max loss: 21.735%, reg loss: 75.074%
time spent training so far: 0:35:53.767382
train attack total time: 12.505s
train attack init time: 2.187s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.342s


train attack loss increase over inner max: -1.320
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.382
total grad norm: 4.060

==================== evaluation at iteration: 108 ====================
train total loss: 87.751%
train max loss: 20.174%, reg loss: 75.135%
time spent training so far: 0:36:04.781415
train attack total time: 10.667s
train attack init time: 2.440s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.256s


train attack loss increase over inner max: -0.791
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.445
total grad norm: 4.903

==================== evaluation at iteration: 109 ====================
train total loss: 87.227%
train max loss: 20.796%, reg loss: 75.117%
time spent training so far: 0:36:19.662697
train attack total time: 14.455s
train attack init time: 2.699s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.402s


train attack loss increase over inner max: 1.463
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.515
total grad norm: 5.390

==================== evaluation at iteration: 110 ====================
train total loss: 86.126%
train max loss: 20.185%, reg loss: 75.213%
time spent training so far: 0:36:29.383920
train attack total time: 9.349s
train attack init time: 2.167s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.212s


train attack loss increase over inner max: -0.089
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.625
total grad norm: 5.515

==================== evaluation at iteration: 111 ====================
train total loss: 88.157%
train max loss: 24.118%, reg loss: 75.185%
time spent training so far: 0:36:39.943827
train attack total time: 10.186s
train attack init time: 2.307s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.242s


train attack loss increase over inner max: 4.548
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.605
total grad norm: 4.700

==================== evaluation at iteration: 112 ====================
train total loss: 88.258%
train max loss: 23.560%, reg loss: 75.259%
time spent training so far: 0:36:51.498825
train attack total time: 11.186s
train attack init time: 2.162s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.289s


train attack loss increase over inner max: 0.018
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.731
total grad norm: 4.425

==================== evaluation at iteration: 113 ====================
train total loss: 89.726%
train max loss: 28.032%, reg loss: 75.271%
time spent training so far: 0:37:07.318938
train attack total time: 15.477s
train attack init time: 2.247s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.464s


train attack loss increase over inner max: 4.049
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.792
total grad norm: 19.004

==================== evaluation at iteration: 114 ====================
train total loss: 90.237%
train max loss: 26.793%, reg loss: 75.215%
time spent training so far: 0:37:19.127037
train attack total time: 11.466s
train attack init time: 2.433s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.290s


train attack loss increase over inner max: -1.340
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.834
total grad norm: 13.687

==================== evaluation at iteration: 115 ====================
train total loss: 87.502%
train max loss: 26.293%, reg loss: 75.069%
time spent training so far: 0:37:32.362352
train attack total time: 12.875s
train attack init time: 2.215s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.356s


train attack loss increase over inner max: -0.951
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.893
total grad norm: 10.050

==================== evaluation at iteration: 116 ====================
train total loss: 88.768%
train max loss: 25.562%, reg loss: 75.277%
time spent training so far: 0:37:42.829158
train attack total time: 10.104s
train attack init time: 2.727s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.219s


train attack loss increase over inner max: -0.260
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.736
total grad norm: 9.610

==================== evaluation at iteration: 117 ====================
train total loss: 87.308%
train max loss: 23.790%, reg loss: 75.185%
time spent training so far: 0:38:01.101025
train attack total time: 17.927s
train attack init time: 2.367s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.562s


train attack loss increase over inner max: -1.044
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.628
total grad norm: 5.195

==================== evaluation at iteration: 118 ====================
train total loss: 86.074%
train max loss: 23.117%, reg loss: 75.228%
time spent training so far: 0:38:15.100525
train attack total time: 13.653s
train attack init time: 2.269s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.386s


train attack loss increase over inner max: -1.233
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.567
total grad norm: 6.259

==================== evaluation at iteration: 119 ====================
train total loss: 88.326%
train max loss: 25.393%, reg loss: 75.240%
time spent training so far: 0:38:25.825231
train attack total time: 10.324s
train attack init time: 2.073s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: 2.269
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.470
total grad norm: 4.759

==================== evaluation at iteration: 120 ====================
train total loss: 87.799%
train max loss: 24.415%, reg loss: 75.172%
time spent training so far: 0:38:39.702117
train attack total time: 13.517s
train attack init time: 2.217s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.382s


train attack loss increase over inner max: -1.175
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.370
total grad norm: 5.343

==================== evaluation at iteration: 121 ====================
train total loss: 87.255%
train max loss: 23.801%, reg loss: 75.259%
time spent training so far: 0:38:51.965837
train attack total time: 11.925s
train attack init time: 2.239s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.333s


train attack loss increase over inner max: -0.765
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.377
total grad norm: 4.210

==================== evaluation at iteration: 122 ====================
train total loss: 87.379%
train max loss: 22.663%, reg loss: 75.156%
time spent training so far: 0:39:02.618571
train attack total time: 10.279s
train attack init time: 2.380s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.257s


train attack loss increase over inner max: -0.823
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.404
total grad norm: 3.313

==================== evaluation at iteration: 123 ====================
train total loss: 87.242%
train max loss: 21.272%, reg loss: 75.231%
time spent training so far: 0:39:17.060976
train attack total time: 14.110s
train attack init time: 2.498s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.418s


train attack loss increase over inner max: -1.725
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.282
total grad norm: 3.837

==================== evaluation at iteration: 124 ====================
train total loss: 85.743%
train max loss: 20.457%, reg loss: 75.009%
time spent training so far: 0:39:31.070016
train attack total time: 13.660s
train attack init time: 2.251s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.410s


train attack loss increase over inner max: -0.957
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.315
total grad norm: 4.029

==================== evaluation at iteration: 125 ====================
train total loss: 85.610%
train max loss: 20.239%, reg loss: 75.181%
time spent training so far: 0:39:42.046384
train attack total time: 10.623s
train attack init time: 2.235s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.532
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.800% of volume
percentage infeasible at boundary: 24.52%
mean, std amount infeasible at boundary: 1.17 +/- 2.75
max amount infeasible at boundary: 17.56

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.297
total grad norm: 4.687

==================== evaluation at iteration: 126 ====================
train total loss: 86.500%
train max loss: 19.569%, reg loss: 75.103%
time spent training so far: 0:41:54.646375
train attack total time: 14.174s
train attack init time: 2.774s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.408s


train attack loss increase over inner max: -0.630
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.298
total grad norm: 3.911

==================== evaluation at iteration: 127 ====================
train total loss: 86.283%
train max loss: 18.498%, reg loss: 75.151%
time spent training so far: 0:42:08.210130
train attack total time: 13.222s
train attack init time: 2.428s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.382s


train attack loss increase over inner max: -0.859
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.377
total grad norm: 5.058

==================== evaluation at iteration: 128 ====================
train total loss: 86.807%
train max loss: 22.023%, reg loss: 75.262%
time spent training so far: 0:42:18.867652
train attack total time: 10.329s
train attack init time: 2.163s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: 3.885
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.334
total grad norm: 4.372

==================== evaluation at iteration: 129 ====================
train total loss: 86.458%
train max loss: 20.339%, reg loss: 75.098%
time spent training so far: 0:42:33.883710
train attack total time: 14.693s
train attack init time: 2.342s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.448s


train attack loss increase over inner max: -1.516
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.357
total grad norm: 4.595

==================== evaluation at iteration: 130 ====================
train total loss: 85.521%
train max loss: 18.140%, reg loss: 75.176%
time spent training so far: 0:42:46.918367
train attack total time: 12.646s
train attack init time: 2.438s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.355s


train attack loss increase over inner max: -2.237
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.424
total grad norm: 5.535

==================== evaluation at iteration: 131 ====================
train total loss: 87.364%
train max loss: 21.702%, reg loss: 75.030%
time spent training so far: 0:42:57.429392
train attack total time: 10.122s
train attack init time: 2.106s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.260s


train attack loss increase over inner max: 3.959
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0513]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0514]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 23
Parameter containing:
tensor([[0.0515]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0517]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0519]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0520]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0521]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0527]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.21827587485313416
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0527]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0531]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0535]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 22
Parameter containing:
tensor([[0.0540]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.27988192439079285
Parameter containing:
tensor([[0.0545]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0551]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0556]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 22
Parameter containing:
tensor([[0.0562]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0566]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0571]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0574]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0577]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0580]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0583]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0585]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0586]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0587]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0589]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0590]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.411
total grad norm: 3.791

==================== evaluation at iteration: 132 ====================
train total loss: 85.971%
train max loss: 19.187%, reg loss: 75.224%
time spent training so far: 0:43:09.447778
train attack total time: 11.659s
train attack init time: 2.404s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.312s


train attack loss increase over inner max: -1.273
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.373
total grad norm: 3.316

==================== evaluation at iteration: 133 ====================
train total loss: 86.138%
train max loss: 19.011%, reg loss: 75.177%
time spent training so far: 0:43:20.736046
train attack total time: 10.902s
train attack init time: 2.697s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.313
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.367
total grad norm: 4.671

==================== evaluation at iteration: 134 ====================
train total loss: 89.377%
train max loss: 26.805%, reg loss: 75.230%
time spent training so far: 0:43:35.096988
train attack total time: 13.992s
train attack init time: 2.436s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.415s


train attack loss increase over inner max: 7.387
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.602
total grad norm: 2.812

==================== evaluation at iteration: 135 ====================
train total loss: 89.282%
train max loss: 28.284%, reg loss: 75.195%
time spent training so far: 0:43:45.182044
train attack total time: 9.727s
train attack init time: 2.045s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.247s


train attack loss increase over inner max: 1.948
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.554
total grad norm: 4.990

==================== evaluation at iteration: 136 ====================
train total loss: 89.375%
train max loss: 25.882%, reg loss: 75.238%
time spent training so far: 0:43:59.028786
train attack total time: 13.474s
train attack init time: 2.294s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.399s


train attack loss increase over inner max: -2.362
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.461
total grad norm: 3.931

==================== evaluation at iteration: 137 ====================
train total loss: 87.457%
train max loss: 25.311%, reg loss: 75.141%
time spent training so far: 0:44:10.047943
train attack total time: 10.622s
train attack init time: 2.373s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.270s


train attack loss increase over inner max: -0.342
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.519
total grad norm: 5.485

==================== evaluation at iteration: 138 ====================
train total loss: 88.190%
train max loss: 23.272%, reg loss: 75.269%
time spent training so far: 0:44:22.750080
train attack total time: 12.344s
train attack init time: 2.354s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.346s


train attack loss increase over inner max: -1.775
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.542
total grad norm: 5.977

==================== evaluation at iteration: 139 ====================
train total loss: 88.012%
train max loss: 23.550%, reg loss: 75.417%
time spent training so far: 0:44:33.698280
train attack total time: 10.573s
train attack init time: 2.519s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: 0.090
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.379
total grad norm: 5.640

==================== evaluation at iteration: 140 ====================
train total loss: 86.459%
train max loss: 21.912%, reg loss: 75.183%
time spent training so far: 0:44:45.593135
train attack total time: 11.539s
train attack init time: 2.121s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: -1.519
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.441
total grad norm: 4.434

==================== evaluation at iteration: 141 ====================
train total loss: 85.619%
train max loss: 20.396%, reg loss: 75.148%
time spent training so far: 0:44:59.313598
train attack total time: 13.355s
train attack init time: 2.037s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.405s


train attack loss increase over inner max: -0.903
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.452
total grad norm: 5.546

==================== evaluation at iteration: 142 ====================
train total loss: 85.027%
train max loss: 20.074%, reg loss: 75.271%
time spent training so far: 0:45:09.507622
train attack total time: 9.816s
train attack init time: 2.237s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: -0.149
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.466
total grad norm: 8.369

==================== evaluation at iteration: 143 ====================
train total loss: 87.335%
train max loss: 23.416%, reg loss: 75.218%
time spent training so far: 0:45:19.499294
train attack total time: 9.642s
train attack init time: 2.542s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.220s


train attack loss increase over inner max: 4.152
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.475
total grad norm: 6.266

==================== evaluation at iteration: 144 ====================
train total loss: 85.724%
train max loss: 21.871%, reg loss: 75.253%
time spent training so far: 0:45:31.048142
train attack total time: 11.182s
train attack init time: 2.134s
train attack avg grad step time: 0.083s
train attack avg reproj time: 0.307s


train attack loss increase over inner max: -2.075
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.462
total grad norm: 6.719

==================== evaluation at iteration: 145 ====================
train total loss: 84.680%
train max loss: 22.178%, reg loss: 75.174%
time spent training so far: 0:45:41.536461
train attack total time: 10.119s
train attack init time: 1.976s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.267s


train attack loss increase over inner max: 0.333
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.530
total grad norm: 6.817

==================== evaluation at iteration: 146 ====================
train total loss: 87.788%
train max loss: 21.765%, reg loss: 75.237%
time spent training so far: 0:45:52.970305
train attack total time: 11.079s
train attack init time: 2.533s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.283s


train attack loss increase over inner max: 0.251
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.615
total grad norm: 9.744

==================== evaluation at iteration: 147 ====================
train total loss: 85.118%
train max loss: 20.289%, reg loss: 75.210%
time spent training so far: 0:46:07.791662
train attack total time: 14.471s
train attack init time: 2.020s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.454s


train attack loss increase over inner max: -1.419
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.577
total grad norm: 6.972

==================== evaluation at iteration: 148 ====================
train total loss: 84.376%
train max loss: 19.698%, reg loss: 75.114%
time spent training so far: 0:46:21.695190
train attack total time: 13.515s
train attack init time: 2.307s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.400s


train attack loss increase over inner max: -0.531
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.480
total grad norm: 4.393

==================== evaluation at iteration: 149 ====================
train total loss: 84.982%
train max loss: 18.492%, reg loss: 75.216%
time spent training so far: 0:46:31.840799
train attack total time: 9.783s
train attack init time: 2.019s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.250s


train attack loss increase over inner max: -0.688
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.673
total grad norm: 5.973

==================== evaluation at iteration: 150 ====================
train total loss: 86.822%
train max loss: 21.439%, reg loss: 75.210%
time spent training so far: 0:46:43.184155
train attack total time: 10.981s
train attack init time: 1.959s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: 0.034
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 4.440% of volume
percentage infeasible at boundary: 23.72%
mean, std amount infeasible at boundary: 1.27 +/- 12.15
max amount infeasible at boundary: 592.76

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.606
total grad norm: 6.682

==================== evaluation at iteration: 151 ====================
train total loss: 86.903%
train max loss: 21.104%, reg loss: 75.261%
time spent training so far: 0:48:44.852654
train attack total time: 11.562s
train attack init time: 2.109s
train attack avg grad step time: 0.084s
train attack avg reproj time: 0.324s


train attack loss increase over inner max: -0.760
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.654
total grad norm: 5.725

==================== evaluation at iteration: 152 ====================
train total loss: 83.927%
train max loss: 19.538%, reg loss: 75.225%
time spent training so far: 0:48:56.076706
train attack total time: 10.886s
train attack init time: 1.866s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.304s


train attack loss increase over inner max: -8.459
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.765
total grad norm: 6.012

==================== evaluation at iteration: 153 ====================
train total loss: 86.078%
train max loss: 19.434%, reg loss: 75.282%
time spent training so far: 0:49:06.717404
train attack total time: 10.293s
train attack init time: 1.994s
train attack avg grad step time: 0.085s
train attack avg reproj time: 0.273s


train attack loss increase over inner max: 0.158
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.719
total grad norm: 6.978

==================== evaluation at iteration: 154 ====================
train total loss: 85.325%
train max loss: 19.297%, reg loss: 75.383%
time spent training so far: 0:49:16.884874
train attack total time: 9.828s
train attack init time: 1.920s
train attack avg grad step time: 0.082s
train attack avg reproj time: 0.258s


train attack loss increase over inner max: 0.660
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.677
total grad norm: 8.596

==================== evaluation at iteration: 155 ====================
train total loss: 84.841%
train max loss: 18.722%, reg loss: 75.371%
time spent training so far: 0:49:26.067744
train attack total time: 8.858s
train attack init time: 2.229s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.218s


train attack loss increase over inner max: -0.204
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.719
total grad norm: 9.770

==================== evaluation at iteration: 156 ====================
train total loss: 86.328%
train max loss: 17.995%, reg loss: 75.102%
time spent training so far: 0:49:34.635798
train attack total time: 8.206s
train attack init time: 2.135s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: 0.061
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.673
total grad norm: 8.745

==================== evaluation at iteration: 157 ====================
train total loss: 86.249%
train max loss: 16.725%, reg loss: 75.408%
time spent training so far: 0:49:45.290401
train attack total time: 10.311s
train attack init time: 2.343s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: -0.790
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.731
total grad norm: 7.668

==================== evaluation at iteration: 158 ====================
train total loss: 86.840%
train max loss: 19.476%, reg loss: 75.264%
time spent training so far: 0:49:55.056052
train attack total time: 9.424s
train attack init time: 2.196s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 2.621
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.746
total grad norm: 8.626

==================== evaluation at iteration: 159 ====================
train total loss: 85.436%
train max loss: 18.945%, reg loss: 75.350%
time spent training so far: 0:50:06.034182
train attack total time: 10.602s
train attack init time: 2.152s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.297s


train attack loss increase over inner max: -140.448
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.915
total grad norm: 5.611

==================== evaluation at iteration: 160 ====================
train total loss: 83.995%
train max loss: 18.368%, reg loss: 75.254%
time spent training so far: 0:50:13.615756
train attack total time: 7.307s
train attack init time: 2.149s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.154s


train attack loss increase over inner max: -0.785
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.874
total grad norm: 8.477

==================== evaluation at iteration: 161 ====================
train total loss: 84.193%
train max loss: 18.035%, reg loss: 75.356%
time spent training so far: 0:50:22.705648
train attack total time: 8.753s
train attack init time: 2.066s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.217s


train attack loss increase over inner max: -0.197
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.736
total grad norm: 9.216

==================== evaluation at iteration: 162 ====================
train total loss: 85.628%
train max loss: 20.041%, reg loss: 75.355%
time spent training so far: 0:50:31.423614
train attack total time: 8.401s
train attack init time: 2.196s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: 1.811
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.875
total grad norm: 19.615

==================== evaluation at iteration: 163 ====================
train total loss: 85.532%
train max loss: 21.141%, reg loss: 75.387%
time spent training so far: 0:50:39.010491
train attack total time: 7.260s
train attack init time: 2.307s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.145s


train attack loss increase over inner max: 1.084
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.946
total grad norm: 6.901

==================== evaluation at iteration: 164 ====================
train total loss: 84.401%
train max loss: 18.508%, reg loss: 75.383%
time spent training so far: 0:50:48.204449
train attack total time: 8.861s
train attack init time: 2.013s
train attack avg grad step time: 0.075s
train attack avg reproj time: 0.220s


train attack loss increase over inner max: -0.492
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.811
total grad norm: 8.119

==================== evaluation at iteration: 165 ====================
train total loss: 84.650%
train max loss: 17.924%, reg loss: 75.420%
time spent training so far: 0:50:56.047485
train attack total time: 7.514s
train attack init time: 2.035s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.165s


train attack loss increase over inner max: -0.113
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.837
total grad norm: 12.087

==================== evaluation at iteration: 166 ====================
train total loss: 85.006%
train max loss: 20.202%, reg loss: 75.388%
time spent training so far: 0:51:03.361078
train attack total time: 6.975s
train attack init time: 2.039s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.140s


train attack loss increase over inner max: 2.876
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.861
total grad norm: 14.728

==================== evaluation at iteration: 167 ====================
train total loss: 82.898%
train max loss: 19.291%, reg loss: 75.434%
time spent training so far: 0:51:09.931848
train attack total time: 6.281s
train attack init time: 2.064s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.114s


train attack loss increase over inner max: -0.174
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.884
total grad norm: 10.832

==================== evaluation at iteration: 168 ====================
train total loss: 85.086%
train max loss: 17.528%, reg loss: 75.362%
time spent training so far: 0:51:17.165923
train attack total time: 6.895s
train attack init time: 2.072s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.141s


train attack loss increase over inner max: -0.108
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.893
total grad norm: 7.059

==================== evaluation at iteration: 169 ====================
train total loss: 84.842%
train max loss: 16.979%, reg loss: 75.404%
time spent training so far: 0:51:25.280363
train attack total time: 7.753s
train attack init time: 2.206s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: -0.437
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.961
total grad norm: 20.351

==================== evaluation at iteration: 170 ====================
train total loss: 85.137%
train max loss: 20.263%, reg loss: 75.321%
time spent training so far: 0:51:32.783560
train attack total time: 7.135s
train attack init time: 2.063s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.149s


train attack loss increase over inner max: 2.883
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.922
total grad norm: 17.257

==================== evaluation at iteration: 171 ====================
train total loss: 83.152%
train max loss: 16.091%, reg loss: 75.310%
time spent training so far: 0:51:40.077348
train attack total time: 6.866s
train attack init time: 2.011s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.140s


train attack loss increase over inner max: -0.748
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.857
total grad norm: 9.169

==================== evaluation at iteration: 172 ====================
train total loss: 83.839%
train max loss: 16.041%, reg loss: 75.481%
time spent training so far: 0:51:47.174969
train attack total time: 6.779s
train attack init time: 1.977s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.133s


train attack loss increase over inner max: 0.459
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.077
total grad norm: 51.567

==================== evaluation at iteration: 173 ====================
train total loss: 86.286%
train max loss: 24.661%, reg loss: 75.345%
time spent training so far: 0:51:54.884225
train attack total time: 7.415s
train attack init time: 1.949s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: 9.449
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.866
total grad norm: 19.128

==================== evaluation at iteration: 174 ====================
train total loss: 82.677%
train max loss: 18.520%, reg loss: 75.283%
time spent training so far: 0:52:03.344416
train attack total time: 8.128s
train attack init time: 2.054s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.204s


train attack loss increase over inner max: 3.047
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.742
total grad norm: 9.703

==================== evaluation at iteration: 175 ====================
train total loss: 87.067%
train max loss: 22.834%, reg loss: 75.345%
time spent training so far: 0:52:10.159731
train attack total time: 6.512s
train attack init time: 2.057s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.131s


train attack loss increase over inner max: 4.336
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 5.040% of volume
percentage infeasible at boundary: 23.20%
mean, std amount infeasible at boundary: 0.82 +/- 2.19
max amount infeasible at boundary: 35.82

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.743
total grad norm: 12.118

==================== evaluation at iteration: 176 ====================
train total loss: 86.454%
train max loss: 22.060%, reg loss: 75.258%
time spent training so far: 0:54:04.271788
train attack total time: 7.212s
train attack init time: 2.047s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.162s


train attack loss increase over inner max: -7.580
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000

Max_n_steps: 21
Parameter containing:
tensor([[0.0591]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0592]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0593]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0594]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0596]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0598]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0602]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0604]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0607]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0611]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0615]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0620]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0626]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0632]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.35346055030822754
Parameter containing:
tensor([[0.0639]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0647]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0656]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0664]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0673]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0684]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0695]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0708]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0722]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0739]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0756]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0773]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.17152349650859833
Parameter containing:
tensor([[0.0791]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0809]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0828]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0847]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0868]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0888]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0908]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.0929]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0951]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0973]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.0994]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.1016]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 21
Parameter containing:
tensor([[0.1037]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 21
Parameter containing:
tensor([[0.1059]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1082]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1105]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1128]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1149]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 0.896
total grad norm: 8.004

==================== evaluation at iteration: 177 ====================
train total loss: 85.842%
train max loss: 21.846%, reg loss: 75.351%
time spent training so far: 0:54:12.137679
train attack total time: 7.535s
train attack init time: 2.114s
train attack avg grad step time: 0.077s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.637
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.773
total grad norm: 10.416

==================== evaluation at iteration: 178 ====================
train total loss: 87.118%
train max loss: 21.074%, reg loss: 75.206%
time spent training so far: 0:54:19.714120
train attack total time: 7.256s
train attack init time: 2.100s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.165s


train attack loss increase over inner max: -0.892
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.796
total grad norm: 7.447

==================== evaluation at iteration: 179 ====================
train total loss: 88.003%
train max loss: 26.446%, reg loss: 75.280%
time spent training so far: 0:54:26.934004
train attack total time: 6.873s
train attack init time: 2.111s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.148s


train attack loss increase over inner max: 5.412
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.811
total grad norm: 12.171

==================== evaluation at iteration: 180 ====================
train total loss: 87.331%
train max loss: 25.831%, reg loss: 75.235%
time spent training so far: 0:54:34.802778
train attack total time: 7.558s
train attack init time: 2.189s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.174s


train attack loss increase over inner max: -0.080
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.830
total grad norm: 7.989

==================== evaluation at iteration: 181 ====================
train total loss: 87.035%
train max loss: 24.918%, reg loss: 75.094%
time spent training so far: 0:54:42.570887
train attack total time: 7.462s
train attack init time: 2.009s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.177s


train attack loss increase over inner max: -2.294
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.803
total grad norm: 11.274

==================== evaluation at iteration: 182 ====================
train total loss: 86.868%
train max loss: 23.606%, reg loss: 75.141%
time spent training so far: 0:54:49.452758
train attack total time: 6.472s
train attack init time: 2.172s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.128s


train attack loss increase over inner max: 0.392
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.784
total grad norm: 7.785

==================== evaluation at iteration: 183 ====================
train total loss: 85.783%
train max loss: 22.039%, reg loss: 75.333%
time spent training so far: 0:54:57.393843
train attack total time: 7.613s
train attack init time: 1.993s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: 0.483
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.773
total grad norm: 7.570

==================== evaluation at iteration: 184 ====================
train total loss: 86.886%
train max loss: 22.713%, reg loss: 75.252%
time spent training so far: 0:55:04.760300
train attack total time: 7.038s
train attack init time: 2.089s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.149s


train attack loss increase over inner max: 1.726
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.873
total grad norm: 6.495

==================== evaluation at iteration: 185 ====================
train total loss: 84.810%
train max loss: 21.147%, reg loss: 75.278%
time spent training so far: 0:55:12.258326
train attack total time: 7.174s
train attack init time: 2.165s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: 0.011
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.776
total grad norm: 52.393

==================== evaluation at iteration: 186 ====================
train total loss: 88.844%
train max loss: 28.302%, reg loss: 75.232%
time spent training so far: 0:55:23.335492
train attack total time: 10.684s
train attack init time: 1.951s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: -482.682
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.919
total grad norm: 21.935

==================== evaluation at iteration: 187 ====================
train total loss: 86.503%
train max loss: 19.857%, reg loss: 75.329%
time spent training so far: 0:55:31.975148
train attack total time: 8.273s
train attack init time: 2.054s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.211s


train attack loss increase over inner max: -0.676
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.776
total grad norm: 9.975

==================== evaluation at iteration: 188 ====================
train total loss: 84.392%
train max loss: 21.188%, reg loss: 75.246%
time spent training so far: 0:55:38.520229
train attack total time: 6.228s
train attack init time: 2.023s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.122s


train attack loss increase over inner max: 1.393
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.859
total grad norm: 9.953

==================== evaluation at iteration: 189 ====================
train total loss: 85.222%
train max loss: 20.170%, reg loss: 75.169%
time spent training so far: 0:55:46.161483
train attack total time: 7.307s
train attack init time: 2.011s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -1.169
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.880
total grad norm: 12.421

==================== evaluation at iteration: 190 ====================
train total loss: 85.360%
train max loss: 17.732%, reg loss: 75.309%
time spent training so far: 0:55:55.230125
train attack total time: 8.799s
train attack init time: 2.104s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: -1.813
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.072
total grad norm: 8.871

==================== evaluation at iteration: 191 ====================
train total loss: 86.565%
train max loss: 18.067%, reg loss: 75.071%
time spent training so far: 0:56:03.336958
train attack total time: 7.665s
train attack init time: 2.103s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.185s


train attack loss increase over inner max: 0.642
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.867
total grad norm: 8.420

==================== evaluation at iteration: 192 ====================
train total loss: 86.139%
train max loss: 17.326%, reg loss: 75.404%
time spent training so far: 0:56:11.687023
train attack total time: 8.031s
train attack init time: 2.157s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: -0.701
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.088
total grad norm: 8.833

==================== evaluation at iteration: 193 ====================
train total loss: 84.086%
train max loss: 17.746%, reg loss: 75.228%
time spent training so far: 0:56:19.277356
train attack total time: 7.269s
train attack init time: 2.065s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.169s


train attack loss increase over inner max: 0.067
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.207
total grad norm: 8.248

==================== evaluation at iteration: 194 ====================
train total loss: 85.904%
train max loss: 20.268%, reg loss: 75.209%
time spent training so far: 0:56:26.063381
train attack total time: 6.483s
train attack init time: 1.900s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.142s


train attack loss increase over inner max: 2.784
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.425
total grad norm: 13.814

==================== evaluation at iteration: 195 ====================
train total loss: 86.457%
train max loss: 20.316%, reg loss: 75.287%
time spent training so far: 0:56:34.596146
train attack total time: 8.234s
train attack init time: 2.176s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: -0.248
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.188
total grad norm: 24.155

==================== evaluation at iteration: 196 ====================
train total loss: 86.833%
train max loss: 22.157%, reg loss: 75.405%
time spent training so far: 0:56:44.715661
train attack total time: 9.712s
train attack init time: 2.101s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.274s


train attack loss increase over inner max: 2.093
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.111
total grad norm: 17.685

==================== evaluation at iteration: 197 ====================
train total loss: 84.453%
train max loss: 18.146%, reg loss: 75.143%
time spent training so far: 0:56:53.238025
train attack total time: 8.161s
train attack init time: 2.054s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.208s


train attack loss increase over inner max: -0.351
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.157
total grad norm: 12.654

==================== evaluation at iteration: 198 ====================
train total loss: 84.932%
train max loss: 17.510%, reg loss: 75.436%
time spent training so far: 0:57:00.491075
train attack total time: 6.902s
train attack init time: 2.213s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.143s


train attack loss increase over inner max: -0.561
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.259
total grad norm: 5.856

==================== evaluation at iteration: 199 ====================
train total loss: 85.254%
train max loss: 21.902%, reg loss: 75.301%
time spent training so far: 0:57:08.437955
train attack total time: 7.638s
train attack init time: 2.004s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.183s


train attack loss increase over inner max: 4.648
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.465
total grad norm: 8.258

==================== evaluation at iteration: 200 ====================
train total loss: 86.010%
train max loss: 20.646%, reg loss: 75.271%
time spent training so far: 0:57:17.100792
train attack total time: 8.394s
train attack init time: 2.286s
train attack avg grad step time: 0.064s
train attack avg reproj time: 0.211s


train attack loss increase over inner max: -1.050
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 3.480% of volume
percentage infeasible at boundary: 20.36%
mean, std amount infeasible at boundary: 0.85 +/- 2.41
max amount infeasible at boundary: 34.00

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 0.926
total grad norm: 10.427

==================== evaluation at iteration: 201 ====================
train total loss: 88.067%
train max loss: 25.155%, reg loss: 75.320%
time spent training so far: 0:59:16.599167
train attack total time: 8.680s
train attack init time: 2.215s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: 4.575
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.958
total grad norm: 10.079

==================== evaluation at iteration: 202 ====================
train total loss: 88.260%
train max loss: 23.816%, reg loss: 75.453%
time spent training so far: 0:59:25.049280
train attack total time: 8.146s
train attack init time: 2.104s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.202s


train attack loss increase over inner max: -1.032
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.054
total grad norm: 5.708

==================== evaluation at iteration: 203 ====================
train total loss: 87.203%
train max loss: 22.811%, reg loss: 75.165%
time spent training so far: 0:59:35.713561
train attack total time: 10.319s
train attack init time: 2.126s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.300s


train attack loss increase over inner max: -23.351
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.938
total grad norm: 17.189

==================== evaluation at iteration: 204 ====================
train total loss: 88.737%
train max loss: 25.179%, reg loss: 75.183%
time spent training so far: 0:59:42.963660
train attack total time: 6.956s
train attack init time: 2.044s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.149s


train attack loss increase over inner max: 2.177
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.355
total grad norm: 34.551

==================== evaluation at iteration: 205 ====================
train total loss: 88.808%
train max loss: 24.796%, reg loss: 75.252%
time spent training so far: 0:59:49.969168
train attack total time: 6.686s
train attack init time: 1.986s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.144s


train attack loss increase over inner max: 2.409
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.834
total grad norm: 8.613

==================== evaluation at iteration: 206 ====================
train total loss: 86.290%
train max loss: 21.698%, reg loss: 75.234%
time spent training so far: 0:59:58.791673
train attack total time: 8.511s
train attack init time: 2.219s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -1.173
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.138
total grad norm: 19.607

==================== evaluation at iteration: 207 ====================
train total loss: 85.386%
train max loss: 20.155%, reg loss: 75.306%
time spent training so far: 1:00:08.394204
train attack total time: 9.253s
train attack init time: 2.046s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.258s


train attack loss increase over inner max: -350.212
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.950
total grad norm: 12.876

==================== evaluation at iteration: 208 ====================
train total loss: 84.636%
train max loss: 19.786%, reg loss: 75.307%
time spent training so far: 1:00:16.124511
train attack total time: 7.392s
train attack init time: 1.985s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: -1.042
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.041
total grad norm: 9.144

==================== evaluation at iteration: 209 ====================
train total loss: 84.968%
train max loss: 18.916%, reg loss: 75.218%
time spent training so far: 1:00:24.106520
train attack total time: 7.666s
train attack init time: 2.105s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.181s


train attack loss increase over inner max: 0.303
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.973
total grad norm: 7.939

==================== evaluation at iteration: 210 ====================
train total loss: 85.539%
train max loss: 18.219%, reg loss: 75.240%
time spent training so far: 1:00:30.314057
train attack total time: 5.880s
train attack init time: 2.061s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.103s


train attack loss increase over inner max: -0.731
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.885
total grad norm: 7.417

==================== evaluation at iteration: 211 ====================
train total loss: 85.264%
train max loss: 19.055%, reg loss: 75.361%
time spent training so far: 1:00:37.772625
train attack total time: 7.107s
train attack init time: 2.345s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.146s


train attack loss increase over inner max: 1.900
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.811
total grad norm: 7.978

==================== evaluation at iteration: 212 ====================
train total loss: 86.049%
train max loss: 18.452%, reg loss: 75.308%
time spent training so far: 1:00:46.152678
train attack total time: 8.059s
train attack init time: 2.316s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: -1.022
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.058
total grad norm: 9.524

==================== evaluation at iteration: 213 ====================
train total loss: 84.137%
train max loss: 17.865%, reg loss: 75.200%
time spent training so far: 1:00:52.911786
train attack total time: 6.431s
train attack init time: 2.076s
train attack avg grad step time: 0.063s
train attack avg reproj time: 0.133s


train attack loss increase over inner max: -1.101
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.952
total grad norm: 8.498

==================== evaluation at iteration: 214 ====================
train total loss: 86.610%
train max loss: 20.512%, reg loss: 75.410%
time spent training so far: 1:00:59.364463
train attack total time: 6.139s
train attack init time: 2.040s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.114s


train attack loss increase over inner max: 1.951
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.139
total grad norm: 9.348

==================== evaluation at iteration: 215 ====================
train total loss: 88.764%
train max loss: 27.472%, reg loss: 75.306%
time spent training so far: 1:01:07.152222
train attack total time: 7.464s
train attack init time: 1.943s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.179s


train attack loss increase over inner max: 6.298
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.059
total grad norm: 6.315

==================== evaluation at iteration: 216 ====================
train total loss: 89.450%
train max loss: 28.015%, reg loss: 75.321%
time spent training so far: 1:01:14.745913
train attack total time: 7.275s
train attack init time: 2.111s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.165s


train attack loss increase over inner max: 0.385
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 0.976
total grad norm: 6.077

==================== evaluation at iteration: 217 ====================
train total loss: 88.287%
train max loss: 26.146%, reg loss: 75.289%
time spent training so far: 1:01:21.834144
train attack total time: 6.752s
train attack init time: 2.228s
train attack avg grad step time: 0.064s
train attack avg reproj time: 0.139s


train attack loss increase over inner max: -0.663
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.005
total grad norm: 8.780

==================== evaluation at iteration: 218 ====================
train total loss: 87.952%
train max loss: 26.393%, reg loss: 75.221%
time spent training so far: 1:01:28.839066
train attack total time: 6.694s
train attack init time: 2.066s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: -0.406
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.151
total grad norm: 7.028

==================== evaluation at iteration: 219 ====================
train total loss: 86.821%
train max loss: 24.840%, reg loss: 75.401%
time spent training so far: 1:01:36.656845
train attack total time: 7.515s
train attack init time: 2.075s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.176s


train attack loss increase over inner max: -1.659
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.156
total grad norm: 8.381

==================== evaluation at iteration: 220 ====================
train total loss: 86.002%
train max loss: 24.609%, reg loss: 75.413%
time spent training so far: 1:01:43.992276
train attack total time: 7.001s
train attack init time: 1.946s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.159s


train attack loss increase over inner max: -0.724
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1170]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1189]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1206]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1222]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1236]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1249]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1260]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1271]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1280]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  397.8602294921875
Parameter containing:
tensor([[0.1293]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1308]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1322]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1335]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1347]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1359]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1369]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1379]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1388]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1398]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1408]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1418]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1428]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1436]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1445]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1452]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1459]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.7717676162719727
Parameter containing:
tensor([[0.1464]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1471]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1480]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1490]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  132.5690155029297
Parameter containing:
tensor([[0.1501]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1512]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1532]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1541]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1549]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1556]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1562]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1568]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1574]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1578]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1581]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1585]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1588]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 1.029
total grad norm: 8.402

==================== evaluation at iteration: 221 ====================
train total loss: 89.465%
train max loss: 23.718%, reg loss: 75.396%
time spent training so far: 1:01:51.834936
train attack total time: 7.473s
train attack init time: 2.215s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: 0.236
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.242
total grad norm: 7.793

==================== evaluation at iteration: 222 ====================
train total loss: 88.345%
train max loss: 23.274%, reg loss: 75.364%
time spent training so far: 1:01:58.033169
train attack total time: 5.869s
train attack init time: 1.955s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.107s


train attack loss increase over inner max: -0.552
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.023
total grad norm: 9.406

==================== evaluation at iteration: 223 ====================
train total loss: 87.441%
train max loss: 22.327%, reg loss: 75.440%
time spent training so far: 1:02:05.993489
train attack total time: 7.645s
train attack init time: 1.992s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.189s


train attack loss increase over inner max: -0.221
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.368
total grad norm: 9.492

==================== evaluation at iteration: 224 ====================
train total loss: 85.375%
train max loss: 21.663%, reg loss: 75.671%
time spent training so far: 1:02:12.809892
train attack total time: 6.467s
train attack init time: 2.061s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.127s


train attack loss increase over inner max: 0.193
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.285
total grad norm: 9.678

==================== evaluation at iteration: 225 ====================
train total loss: 86.237%
train max loss: 20.588%, reg loss: 75.390%
time spent training so far: 1:02:20.304112
train attack total time: 7.192s
train attack init time: 2.058s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.851
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 2.960% of volume
percentage infeasible at boundary: 17.48%
mean, std amount infeasible at boundary: 0.67 +/- 2.57
max amount infeasible at boundary: 80.07

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 1.230
total grad norm: 112.177

==================== evaluation at iteration: 226 ====================
train total loss: 105.469%
train max loss: 43.475%, reg loss: 75.381%
time spent training so far: 1:04:13.902666
train attack total time: 7.556s
train attack init time: 2.156s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.175s


train attack loss increase over inner max: 22.665
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.165
total grad norm: 72.347

==================== evaluation at iteration: 227 ====================
train total loss: 94.164%
train max loss: 34.603%, reg loss: 75.345%
time spent training so far: 1:04:21.190232
train attack total time: 6.912s
train attack init time: 2.027s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.150s


train attack loss increase over inner max: 9.647
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.233
total grad norm: 20.287

==================== evaluation at iteration: 228 ====================
train total loss: 85.999%
train max loss: 19.146%, reg loss: 75.460%
time spent training so far: 1:04:28.979618
train attack total time: 7.440s
train attack init time: 2.148s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.173s


train attack loss increase over inner max: -0.061
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.445
total grad norm: 22.556

==================== evaluation at iteration: 229 ====================
train total loss: 84.366%
train max loss: 18.287%, reg loss: 75.525%
time spent training so far: 1:04:37.266615
train attack total time: 7.950s
train attack init time: 2.153s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.189s


train attack loss increase over inner max: -0.012
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.212
total grad norm: 12.582

==================== evaluation at iteration: 230 ====================
train total loss: 83.903%
train max loss: 17.781%, reg loss: 75.428%
time spent training so far: 1:04:44.911774
train attack total time: 7.323s
train attack init time: 1.869s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: -0.739
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.543
total grad norm: 9.278

==================== evaluation at iteration: 231 ====================
train total loss: 85.600%
train max loss: 17.513%, reg loss: 75.495%
time spent training so far: 1:04:53.054167
train attack total time: 7.769s
train attack init time: 2.133s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: -0.476
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.558
total grad norm: 7.159

==================== evaluation at iteration: 232 ====================
train total loss: 84.753%
train max loss: 17.680%, reg loss: 75.481%
time spent training so far: 1:05:00.838367
train attack total time: 7.488s
train attack init time: 2.249s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.162s


train attack loss increase over inner max: -0.067
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.836
total grad norm: 6.839

==================== evaluation at iteration: 233 ====================
train total loss: 87.186%
train max loss: 19.218%, reg loss: 75.634%
time spent training so far: 1:05:08.330517
train attack total time: 7.157s
train attack init time: 2.250s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.154s


train attack loss increase over inner max: 1.699
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.067
total grad norm: 6.592

==================== evaluation at iteration: 234 ====================
train total loss: 86.585%
train max loss: 19.902%, reg loss: 75.718%
time spent training so far: 1:05:15.748668
train attack total time: 7.117s
train attack init time: 2.095s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: -0.018
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.861
total grad norm: 7.924

==================== evaluation at iteration: 235 ====================
train total loss: 85.124%
train max loss: 18.694%, reg loss: 75.619%
time spent training so far: 1:05:22.769619
train attack total time: 6.736s
train attack init time: 2.113s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.139s


train attack loss increase over inner max: -0.670
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.022
total grad norm: 9.773

==================== evaluation at iteration: 236 ====================
train total loss: 89.963%
train max loss: 28.274%, reg loss: 75.610%
time spent training so far: 1:05:30.191979
train attack total time: 7.089s
train attack init time: 2.124s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.154s


train attack loss increase over inner max: 8.804
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.866
total grad norm: 6.193

==================== evaluation at iteration: 237 ====================
train total loss: 89.685%
train max loss: 27.887%, reg loss: 75.533%
time spent training so far: 1:05:37.644340
train attack total time: 7.115s
train attack init time: 1.956s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.604
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.719
total grad norm: 7.130

==================== evaluation at iteration: 238 ====================
train total loss: 88.361%
train max loss: 27.331%, reg loss: 75.515%
time spent training so far: 1:05:45.491167
train attack total time: 7.542s
train attack init time: 2.304s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.169s


train attack loss increase over inner max: -0.267
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.740
total grad norm: 5.598

==================== evaluation at iteration: 239 ====================
train total loss: 89.318%
train max loss: 26.792%, reg loss: 75.425%
time spent training so far: 1:05:53.254998
train attack total time: 7.477s
train attack init time: 2.181s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: 0.803
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.442
total grad norm: 7.241

==================== evaluation at iteration: 240 ====================
train total loss: 87.146%
train max loss: 25.388%, reg loss: 75.474%
time spent training so far: 1:06:00.804749
train attack total time: 7.226s
train attack init time: 2.069s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.160s


train attack loss increase over inner max: -0.656
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.727
total grad norm: 6.869

==================== evaluation at iteration: 241 ====================
train total loss: 87.621%
train max loss: 24.599%, reg loss: 75.447%
time spent training so far: 1:06:08.299045
train attack total time: 7.096s
train attack init time: 2.301s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.147s


train attack loss increase over inner max: 0.489
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.929
total grad norm: 9.096

==================== evaluation at iteration: 242 ====================
train total loss: 87.335%
train max loss: 23.605%, reg loss: 75.246%
time spent training so far: 1:06:15.986647
train attack total time: 7.298s
train attack init time: 2.164s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.863
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.778
total grad norm: 9.992

==================== evaluation at iteration: 243 ====================
train total loss: 86.421%
train max loss: 21.539%, reg loss: 75.590%
time spent training so far: 1:06:28.140105
train attack total time: 11.832s
train attack init time: 2.009s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.375s


train attack loss increase over inner max: 0.137
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.573
total grad norm: 26.034

==================== evaluation at iteration: 244 ====================
train total loss: 86.868%
train max loss: 21.135%, reg loss: 75.443%
time spent training so far: 1:06:37.310295
train attack total time: 8.895s
train attack init time: 2.323s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.230s


train attack loss increase over inner max: 0.242
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.552
total grad norm: 15.550

==================== evaluation at iteration: 245 ====================
train total loss: 85.916%
train max loss: 20.741%, reg loss: 75.460%
time spent training so far: 1:06:45.515506
train attack total time: 7.905s
train attack init time: 2.097s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.191s


train attack loss increase over inner max: 1.428
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.597
total grad norm: 9.719

==================== evaluation at iteration: 246 ====================
train total loss: 86.414%
train max loss: 20.083%, reg loss: 75.529%
time spent training so far: 1:06:52.676158
train attack total time: 6.797s
train attack init time: 2.204s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.141s


train attack loss increase over inner max: 0.110
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.578
total grad norm: 10.816

==================== evaluation at iteration: 247 ====================
train total loss: 87.382%
train max loss: 19.581%, reg loss: 75.417%
time spent training so far: 1:07:00.558025
train attack total time: 7.512s
train attack init time: 2.033s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: -0.419
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.526
total grad norm: 8.359

==================== evaluation at iteration: 248 ====================
train total loss: 84.668%
train max loss: 18.527%, reg loss: 75.522%
time spent training so far: 1:07:08.227457
train attack total time: 7.340s
train attack init time: 2.098s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.822
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.270
total grad norm: 8.975

==================== evaluation at iteration: 249 ====================
train total loss: 84.133%
train max loss: 17.088%, reg loss: 75.437%
time spent training so far: 1:07:14.922483
train attack total time: 6.383s
train attack init time: 1.974s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.130s


train attack loss increase over inner max: -1.021
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.431
total grad norm: 7.923

==================== evaluation at iteration: 250 ====================
train total loss: 83.515%
train max loss: 17.015%, reg loss: 75.492%
time spent training so far: 1:07:22.511794
train attack total time: 7.258s
train attack init time: 1.985s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.171s


train attack loss increase over inner max: 0.631
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 3.520% of volume
percentage infeasible at boundary: 18.20%
mean, std amount infeasible at boundary: 0.64 +/- 1.87
max amount infeasible at boundary: 16.34

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 1.861
total grad norm: 7.514

==================== evaluation at iteration: 251 ====================
train total loss: 86.123%
train max loss: 21.129%, reg loss: 75.481%
time spent training so far: 1:09:12.779387
train attack total time: 7.244s
train attack init time: 1.895s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 4.586
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.552
total grad norm: 9.698

==================== evaluation at iteration: 252 ====================
train total loss: 86.163%
train max loss: 21.431%, reg loss: 75.473%
time spent training so far: 1:09:19.759512
train attack total time: 6.703s
train attack init time: 1.980s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.145s


train attack loss increase over inner max: 1.919
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.401
total grad norm: 6.852

==================== evaluation at iteration: 253 ====================
train total loss: 84.802%
train max loss: 20.180%, reg loss: 75.460%
time spent training so far: 1:09:26.575535
train attack total time: 6.476s
train attack init time: 2.024s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.134s


train attack loss increase over inner max: -1.197
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.298
total grad norm: 36.507

==================== evaluation at iteration: 254 ====================
train total loss: 89.014%
train max loss: 27.219%, reg loss: 75.552%
time spent training so far: 1:09:33.963760
train attack total time: 7.037s
train attack init time: 2.046s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.155s


train attack loss increase over inner max: 7.654
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.249
total grad norm: 11.338

==================== evaluation at iteration: 255 ====================
train total loss: 82.860%
train max loss: 18.934%, reg loss: 75.536%
time spent training so far: 1:09:41.957862
train attack total time: 7.658s
train attack init time: 1.939s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.191s


train attack loss increase over inner max: -1.802
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.375
total grad norm: 9.272

==================== evaluation at iteration: 256 ====================
train total loss: 86.009%
train max loss: 19.909%, reg loss: 75.500%
time spent training so far: 1:09:49.413432
train attack total time: 7.068s
train attack init time: 1.869s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: 3.665
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.408
total grad norm: 12.083

==================== evaluation at iteration: 257 ====================
train total loss: 86.031%
train max loss: 19.514%, reg loss: 75.542%
time spent training so far: 1:09:56.658434
train attack total time: 6.943s
train attack init time: 2.055s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.150s


train attack loss increase over inner max: 0.468
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.185
total grad norm: 16.957

==================== evaluation at iteration: 258 ====================
train total loss: 84.978%
train max loss: 20.377%, reg loss: 75.480%
time spent training so far: 1:10:04.243790
train attack total time: 7.234s
train attack init time: 2.146s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.158s


train attack loss increase over inner max: 2.907
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.294
total grad norm: 17.336

==================== evaluation at iteration: 259 ====================
train total loss: 86.102%
train max loss: 23.959%, reg loss: 75.401%
time spent training so far: 1:10:12.343420
train attack total time: 7.793s
train attack init time: 2.019s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: 5.153
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.461
total grad norm: 16.343

==================== evaluation at iteration: 260 ====================
train total loss: 87.412%
train max loss: 22.371%, reg loss: 75.580%
time spent training so far: 1:10:20.062388
train attack total time: 7.357s
train attack init time: 2.047s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: 1.561
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.417
total grad norm: 13.468

==================== evaluation at iteration: 261 ====================
train total loss: 84.501%
train max loss: 20.269%, reg loss: 75.470%
time spent training so far: 1:10:27.092556
train attack total time: 6.624s
train attack init time: 2.184s
train attack avg grad step time: 0.073s
train attack avg reproj time: 0.125s


train attack loss increase over inner max: -1.011
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.423
total grad norm: 16.965

==================== evaluation at iteration: 262 ====================
train total loss: 85.637%
train max loss: 19.553%, reg loss: 75.538%
time spent training so far: 1:10:35.335403
train attack total time: 7.904s
train attack init time: 2.151s
train attack avg grad step time: 0.075s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: 0.117
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.350
total grad norm: 12.285

==================== evaluation at iteration: 263 ====================
train total loss: 85.264%
train max loss: 18.418%, reg loss: 75.541%
time spent training so far: 1:10:43.441052
train attack total time: 7.823s
train attack init time: 1.998s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.191s


train attack loss increase over inner max: -0.413
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.478
total grad norm: 9.977

==================== evaluation at iteration: 264 ====================
train total loss: 83.482%
train max loss: 18.107%, reg loss: 75.470%
time spent training so far: 1:10:51.218583
train attack total time: 7.468s
train attack init time: 2.052s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.171s


train attack loss increase over inner max: 1.617
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.226
total grad norm: 10.473

==================== evaluation at iteration: 265 ====================
train total loss: 85.105%
train max loss: 17.424%, reg loss: 75.556%
time spent training so far: 1:10:59.450899
train attack total time: 7.926s
train attack init time: 1.999s
train attack avg grad step time: 0.077s
train attack avg reproj time: 0.190s


train attack loss increase over inner max: 0.888
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000

Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1590]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1593]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1595]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1597]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1612]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1628]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1644]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1660]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1676]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1690]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1702]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1713]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1721]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1729]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1736]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1741]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1745]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1748]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1750]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1752]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1753]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  1.7979050874710083
Parameter containing:
tensor([[0.1754]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1756]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1757]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1759]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1761]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1763]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1764]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1766]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1767]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1768]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1769]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1771]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1773]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1774]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1776]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1779]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1783]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1789]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1795]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1802]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1810]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1817]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1824]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 1.379
total grad norm: 13.527

==================== evaluation at iteration: 266 ====================
train total loss: 86.366%
train max loss: 18.693%, reg loss: 75.503%
time spent training so far: 1:11:07.369003
train attack total time: 7.617s
train attack init time: 1.967s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: -4.574
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.425
total grad norm: 8.434

==================== evaluation at iteration: 267 ====================
train total loss: 84.865%
train max loss: 20.703%, reg loss: 75.489%
time spent training so far: 1:11:15.350316
train attack total time: 7.641s
train attack init time: 2.098s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: 1.683
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.588
total grad norm: 11.649

==================== evaluation at iteration: 268 ====================
train total loss: 86.479%
train max loss: 19.952%, reg loss: 75.544%
time spent training so far: 1:11:23.340390
train attack total time: 7.673s
train attack init time: 2.042s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: -1.097
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.495
total grad norm: 8.546

==================== evaluation at iteration: 269 ====================
train total loss: 85.891%
train max loss: 19.923%, reg loss: 75.552%
time spent training so far: 1:11:30.486216
train attack total time: 6.812s
train attack init time: 2.129s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.141s


train attack loss increase over inner max: 0.005
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.347
total grad norm: 7.748

==================== evaluation at iteration: 270 ====================
train total loss: 85.563%
train max loss: 19.166%, reg loss: 75.515%
time spent training so far: 1:11:37.916261
train attack total time: 7.097s
train attack init time: 1.868s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.166s


train attack loss increase over inner max: -0.056
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.756
total grad norm: 9.633

==================== evaluation at iteration: 271 ====================
train total loss: 84.260%
train max loss: 19.259%, reg loss: 75.585%
time spent training so far: 1:11:45.971278
train attack total time: 7.655s
train attack init time: 1.993s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: -24.178
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.387
total grad norm: 10.478

==================== evaluation at iteration: 272 ====================
train total loss: 86.633%
train max loss: 18.249%, reg loss: 75.669%
time spent training so far: 1:11:54.141818
train attack total time: 7.822s
train attack init time: 2.029s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.193s


train attack loss increase over inner max: -0.885
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.405
total grad norm: 13.793

==================== evaluation at iteration: 273 ====================
train total loss: 84.287%
train max loss: 18.337%, reg loss: 75.689%
time spent training so far: 1:12:02.533632
train attack total time: 8.083s
train attack init time: 2.014s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: -0.335
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.554
total grad norm: 11.541

==================== evaluation at iteration: 274 ====================
train total loss: 84.048%
train max loss: 18.177%, reg loss: 75.629%
time spent training so far: 1:12:10.313371
train attack total time: 7.467s
train attack init time: 2.074s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.174s


train attack loss increase over inner max: -0.318
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.895
total grad norm: 9.282

==================== evaluation at iteration: 275 ====================
train total loss: 85.509%
train max loss: 17.566%, reg loss: 75.646%
time spent training so far: 1:12:18.468821
train attack total time: 7.858s
train attack init time: 2.039s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: -0.401
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 2.680% of volume
percentage infeasible at boundary: 19.68%
mean, std amount infeasible at boundary: 0.69 +/- 2.63
max amount infeasible at boundary: 94.84

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 1.535
total grad norm: 8.378

==================== evaluation at iteration: 276 ====================
train total loss: 84.240%
train max loss: 17.045%, reg loss: 75.586%
time spent training so far: 1:14:09.106391
train attack total time: 7.881s
train attack init time: 2.012s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.192s


train attack loss increase over inner max: -0.046
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.626
total grad norm: 10.510

==================== evaluation at iteration: 277 ====================
train total loss: 87.672%
train max loss: 23.861%, reg loss: 75.833%
time spent training so far: 1:14:16.509931
train attack total time: 7.101s
train attack init time: 2.002s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.160s


train attack loss increase over inner max: 6.971
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.678
total grad norm: 8.447

==================== evaluation at iteration: 278 ====================
train total loss: 86.992%
train max loss: 25.172%, reg loss: 75.593%
time spent training so far: 1:14:24.252410
train attack total time: 7.351s
train attack init time: 1.975s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: 1.391
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.770
total grad norm: 13.953

==================== evaluation at iteration: 279 ====================
train total loss: 86.483%
train max loss: 24.419%, reg loss: 75.774%
time spent training so far: 1:14:31.752538
train attack total time: 7.157s
train attack init time: 2.152s
train attack avg grad step time: 0.063s
train attack avg reproj time: 0.162s


train attack loss increase over inner max: -0.900
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.602
total grad norm: 8.853

==================== evaluation at iteration: 280 ====================
train total loss: 86.253%
train max loss: 23.917%, reg loss: 75.798%
time spent training so far: 1:14:38.719380
train attack total time: 6.639s
train attack init time: 1.924s
train attack avg grad step time: 0.063s
train attack avg reproj time: 0.148s


train attack loss increase over inner max: -1.067
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.617
total grad norm: 8.280

==================== evaluation at iteration: 281 ====================
train total loss: 88.380%
train max loss: 23.359%, reg loss: 75.828%
time spent training so far: 1:14:45.537047
train attack total time: 6.472s
train attack init time: 1.986s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.133s


train attack loss increase over inner max: -0.157
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.981
total grad norm: 7.674

==================== evaluation at iteration: 282 ====================
train total loss: 86.132%
train max loss: 23.060%, reg loss: 75.761%
time spent training so far: 1:14:53.383478
train attack total time: 7.498s
train attack init time: 1.999s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.179s


train attack loss increase over inner max: 0.514
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.542
total grad norm: 11.204

==================== evaluation at iteration: 283 ====================
train total loss: 85.944%
train max loss: 22.124%, reg loss: 75.801%
time spent training so far: 1:15:01.046837
train attack total time: 7.315s
train attack init time: 2.002s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: -0.142
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.558
total grad norm: 10.800

==================== evaluation at iteration: 284 ====================
train total loss: 86.107%
train max loss: 21.041%, reg loss: 75.828%
time spent training so far: 1:15:08.869037
train attack total time: 7.509s
train attack init time: 2.143s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.173s


train attack loss increase over inner max: -0.477
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.843
total grad norm: 7.783

==================== evaluation at iteration: 285 ====================
train total loss: 85.209%
train max loss: 20.346%, reg loss: 75.754%
time spent training so far: 1:15:17.174832
train attack total time: 7.964s
train attack init time: 2.200s
train attack avg grad step time: 0.072s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: -0.326
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.754
total grad norm: 8.082

==================== evaluation at iteration: 286 ====================
train total loss: 84.181%
train max loss: 19.676%, reg loss: 75.829%
time spent training so far: 1:15:24.258844
train attack total time: 6.715s
train attack init time: 2.091s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: -0.510
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.988
total grad norm: 8.949

==================== evaluation at iteration: 287 ====================
train total loss: 85.038%
train max loss: 19.254%, reg loss: 75.862%
time spent training so far: 1:15:31.177292
train attack total time: 6.579s
train attack init time: 2.032s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: -0.181
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.010
total grad norm: 10.726

==================== evaluation at iteration: 288 ====================
train total loss: 86.452%
train max loss: 19.886%, reg loss: 75.950%
time spent training so far: 1:15:39.201095
train attack total time: 7.766s
train attack init time: 1.975s
train attack avg grad step time: 0.067s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: 1.330
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.305
total grad norm: 16.937

==================== evaluation at iteration: 289 ====================
train total loss: 86.600%
train max loss: 19.881%, reg loss: 75.943%
time spent training so far: 1:15:47.526283
train attack total time: 8.018s
train attack init time: 2.143s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: 0.152
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.039
total grad norm: 9.750

==================== evaluation at iteration: 290 ====================
train total loss: 84.835%
train max loss: 18.902%, reg loss: 75.899%
time spent training so far: 1:15:55.081600
train attack total time: 7.251s
train attack init time: 1.973s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: -0.548
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.805
total grad norm: 12.061

==================== evaluation at iteration: 291 ====================
train total loss: 83.883%
train max loss: 18.801%, reg loss: 75.846%
time spent training so far: 1:16:02.085440
train attack total time: 6.634s
train attack init time: 1.980s
train attack avg grad step time: 0.066s
train attack avg reproj time: 0.143s


train attack loss increase over inner max: 0.015
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.077
total grad norm: 11.829

==================== evaluation at iteration: 292 ====================
train total loss: 84.310%
train max loss: 18.653%, reg loss: 75.939%
time spent training so far: 1:16:09.545101
train attack total time: 7.178s
train attack init time: 2.016s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.167s


train attack loss increase over inner max: -0.451
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.084
total grad norm: 12.326

==================== evaluation at iteration: 293 ====================
train total loss: 85.415%
train max loss: 18.412%, reg loss: 75.910%
time spent training so far: 1:16:16.923654
train attack total time: 7.014s
train attack init time: 2.109s
train attack avg grad step time: 0.070s
train attack avg reproj time: 0.151s


train attack loss increase over inner max: 0.965
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.053
total grad norm: 8.611

==================== evaluation at iteration: 294 ====================
train total loss: 84.585%
train max loss: 20.090%, reg loss: 76.006%
time spent training so far: 1:16:24.900000
train attack total time: 7.668s
train attack init time: 2.102s
train attack avg grad step time: 0.074s
train attack avg reproj time: 0.176s


train attack loss increase over inner max: 2.645
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 1.826
total grad norm: 12.015

==================== evaluation at iteration: 295 ====================
train total loss: 84.661%
train max loss: 19.871%, reg loss: 76.124%
time spent training so far: 1:16:33.071359
train attack total time: 7.844s
train attack init time: 2.099s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.188s


train attack loss increase over inner max: -0.363
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.472
total grad norm: 17.287

==================== evaluation at iteration: 296 ====================
train total loss: 83.949%
train max loss: 17.158%, reg loss: 76.091%
time spent training so far: 1:16:42.364048
train attack total time: 8.930s
train attack init time: 1.970s
train attack avg grad step time: 0.069s
train attack avg reproj time: 0.245s


train attack loss increase over inner max: -1.369
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.015
total grad norm: 10.680

==================== evaluation at iteration: 297 ====================
train total loss: 84.394%
train max loss: 17.028%, reg loss: 76.042%
time spent training so far: 1:16:50.183982
train attack total time: 7.484s
train attack init time: 2.130s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: -1.039
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.298
total grad norm: 12.074

==================== evaluation at iteration: 298 ====================
train total loss: 82.732%
train max loss: 15.136%, reg loss: 76.051%
time spent training so far: 1:16:57.844289
train attack total time: 7.364s
train attack init time: 2.057s
train attack avg grad step time: 0.071s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 0.149
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.122
total grad norm: 10.067

==================== evaluation at iteration: 299 ====================
train total loss: 84.268%
train max loss: 17.596%, reg loss: 76.112%
time spent training so far: 1:17:04.164207
train attack total time: 6.047s
train attack init time: 1.899s
train attack avg grad step time: 0.065s
train attack avg reproj time: 0.121s


train attack loss increase over inner max: 2.394
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.431
total grad norm: 9.990

==================== evaluation at iteration: 300 ====================
train total loss: 84.255%
train max loss: 18.407%, reg loss: 76.189%
time spent training so far: 1:17:11.872199
train attack total time: 7.438s
train attack init time: 2.032s
train attack avg grad step time: 0.068s
train attack avg reproj time: 0.175s


train attack loss increase over inner max: 0.525
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 1.760% of volume
percentage infeasible at boundary: 13.60%
mean, std amount infeasible at boundary: 0.40 +/- 1.41
max amount infeasible at boundary: 15.94

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 2.334
total grad norm: 11.018

==================== evaluation at iteration: 301 ====================
train total loss: 83.468%
train max loss: 17.343%, reg loss: 76.160%
time spent training so far: 1:19:09.464840
train attack total time: 9.111s
train attack init time: 2.144s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: -0.387
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.392
total grad norm: 11.578

==================== evaluation at iteration: 302 ====================
train total loss: 83.758%
train max loss: 17.335%, reg loss: 76.204%
time spent training so far: 1:19:20.543317
train attack total time: 10.617s
train attack init time: 3.327s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.240s


train attack loss increase over inner max: -0.056
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.458
total grad norm: 13.119

==================== evaluation at iteration: 303 ====================
train total loss: 84.079%
train max loss: 16.222%, reg loss: 76.206%
time spent training so far: 1:19:30.362757
train attack total time: 9.351s
train attack init time: 3.360s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.183s


train attack loss increase over inner max: 0.488
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.710
total grad norm: 9.097

==================== evaluation at iteration: 304 ====================
train total loss: 83.139%
train max loss: 15.467%, reg loss: 76.400%
time spent training so far: 1:19:40.815459
train attack total time: 9.995s
train attack init time: 3.414s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.203s


train attack loss increase over inner max: -0.429
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.269
total grad norm: 11.004

==================== evaluation at iteration: 305 ====================
train total loss: 83.271%
train max loss: 16.035%, reg loss: 76.260%
time spent training so far: 1:19:51.741313
train attack total time: 10.445s
train attack init time: 3.615s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.217s


train attack loss increase over inner max: 2.135
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.450
total grad norm: 9.688

==================== evaluation at iteration: 306 ====================
train total loss: 83.463%
train max loss: 15.239%, reg loss: 76.324%
time spent training so far: 1:20:01.992139
train attack total time: 9.712s
train attack init time: 3.690s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.184s


train attack loss increase over inner max: -0.609
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.472
total grad norm: 10.430

==================== evaluation at iteration: 307 ====================
train total loss: 83.017%
train max loss: 15.824%, reg loss: 76.375%
time spent training so far: 1:20:13.086070
train attack total time: 10.621s
train attack init time: 3.476s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.230s


train attack loss increase over inner max: 0.981
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.636
total grad norm: 13.821

==================== evaluation at iteration: 308 ====================
train total loss: 82.900%
train max loss: 15.288%, reg loss: 76.361%
time spent training so far: 1:20:23.355331
train attack total time: 9.748s
train attack init time: 3.421s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.198s


train attack loss increase over inner max: -0.942
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.885
total grad norm: 16.653

==================== evaluation at iteration: 309 ====================
train total loss: 82.845%
train max loss: 13.289%, reg loss: 76.486%
time spent training so far: 1:20:33.718330
train attack total time: 9.878s
train attack init time: 3.627s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.195s


train attack loss increase over inner max: -2.145
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.602
total grad norm: 14.229

==================== evaluation at iteration: 310 ====================
train total loss: 82.804%
train max loss: 13.244%, reg loss: 76.355%
time spent training so far: 1:20:45.321900
train attack total time: 11.143s
train attack init time: 3.671s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.244s


train attack loss increase over inner max: 1.459
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1831]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1836]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1841]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1845]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1849]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1852]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1855]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1858]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1861]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1863]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1865]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1867]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1868]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1870]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1871]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1871]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1872]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1872]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1872]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1871]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1871]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1870]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1869]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1868]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1867]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1865]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1864]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1863]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1861]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1862]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1861]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1860]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1859]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1858]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1857]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1856]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1855]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1854]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pklReg grad norm: 2.991
total grad norm: 9.700

==================== evaluation at iteration: 311 ====================
train total loss: 83.174%
train max loss: 15.728%, reg loss: 76.680%
time spent training so far: 1:20:56.313301
train attack total time: 10.396s
train attack init time: 3.694s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.213s


train attack loss increase over inner max: 3.091
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.762
total grad norm: 15.046

==================== evaluation at iteration: 312 ====================
train total loss: 83.656%
train max loss: 16.902%, reg loss: 76.500%
time spent training so far: 1:21:05.820123
train attack total time: 9.029s
train attack init time: 3.526s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.159s


train attack loss increase over inner max: 1.692
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.623
total grad norm: 10.452

==================== evaluation at iteration: 313 ====================
train total loss: 83.602%
train max loss: 18.135%, reg loss: 76.631%
time spent training so far: 1:21:17.286105
train attack total time: 10.979s
train attack init time: 3.599s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.239s


train attack loss increase over inner max: 1.073
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.483
total grad norm: 12.362

==================== evaluation at iteration: 314 ====================
train total loss: 84.733%
train max loss: 19.240%, reg loss: 76.848%
time spent training so far: 1:21:27.707398
train attack total time: 9.916s
train attack init time: 3.820s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.181s


train attack loss increase over inner max: 1.612
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.419
total grad norm: 14.174

==================== evaluation at iteration: 315 ====================
train total loss: 83.935%
train max loss: 18.763%, reg loss: 76.572%
time spent training so far: 1:21:42.382070
train attack total time: 14.165s
train attack init time: 3.867s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: -0.495
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.849
total grad norm: 10.385

==================== evaluation at iteration: 316 ====================
train total loss: 83.838%
train max loss: 18.495%, reg loss: 76.689%
time spent training so far: 1:21:53.636725
train attack total time: 10.716s
train attack init time: 4.116s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.209s


train attack loss increase over inner max: 0.942
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.627
total grad norm: 10.854

==================== evaluation at iteration: 317 ====================
train total loss: 84.487%
train max loss: 17.411%, reg loss: 76.589%
time spent training so far: 1:22:04.649095
train attack total time: 10.507s
train attack init time: 3.806s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.213s


train attack loss increase over inner max: 0.136
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.085
total grad norm: 8.184

==================== evaluation at iteration: 318 ====================
train total loss: 84.868%
train max loss: 16.487%, reg loss: 76.757%
time spent training so far: 1:22:15.166711
train attack total time: 10.052s
train attack init time: 4.045s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.176s


train attack loss increase over inner max: 0.580
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.592
total grad norm: 14.374

==================== evaluation at iteration: 319 ====================
train total loss: 83.495%
train max loss: 16.422%, reg loss: 76.662%
time spent training so far: 1:22:28.343528
train attack total time: 12.701s
train attack init time: 3.993s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.304s


train attack loss increase over inner max: 1.716
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.007
total grad norm: 12.043

==================== evaluation at iteration: 320 ====================
train total loss: 82.580%
train max loss: 14.319%, reg loss: 76.767%
time spent training so far: 1:22:40.931966
train attack total time: 12.106s
train attack init time: 4.110s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.267s


train attack loss increase over inner max: -1.165
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.705
total grad norm: 10.718

==================== evaluation at iteration: 321 ====================
train total loss: 84.772%
train max loss: 15.909%, reg loss: 76.705%
time spent training so far: 1:22:52.264722
train attack total time: 10.820s
train attack init time: 4.124s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.208s


train attack loss increase over inner max: 1.287
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.205
total grad norm: 15.889

==================== evaluation at iteration: 322 ====================
train total loss: 82.848%
train max loss: 16.467%, reg loss: 76.801%
time spent training so far: 1:23:02.052461
train attack total time: 9.288s
train attack init time: 3.757s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: 0.835
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.842
total grad norm: 12.878

==================== evaluation at iteration: 323 ====================
train total loss: 83.837%
train max loss: 14.668%, reg loss: 76.781%
time spent training so far: 1:23:13.132126
train attack total time: 10.631s
train attack init time: 4.080s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.204s


train attack loss increase over inner max: -1.684
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.009
total grad norm: 16.167

==================== evaluation at iteration: 324 ====================
train total loss: 82.922%
train max loss: 13.857%, reg loss: 76.859%
time spent training so far: 1:23:24.980413
train attack total time: 11.358s
train attack init time: 4.385s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.220s


train attack loss increase over inner max: 0.138
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.189
total grad norm: 17.510

==================== evaluation at iteration: 325 ====================
train total loss: 83.541%
train max loss: 13.627%, reg loss: 76.904%
time spent training so far: 1:23:37.052352
train attack total time: 11.565s
train attack init time: 4.307s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: -0.263
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.320% of volume
percentage infeasible at boundary: 6.32%
mean, std amount infeasible at boundary: 0.15 +/- 0.80
max amount infeasible at boundary: 13.44

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 2.724
total grad norm: 14.590

==================== evaluation at iteration: 326 ====================
train total loss: 82.944%
train max loss: 12.486%, reg loss: 76.847%
time spent training so far: 1:27:28.675187
train attack total time: 9.551s
train attack init time: 4.121s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.157s


train attack loss increase over inner max: 0.546
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.924
total grad norm: 13.978

==================== evaluation at iteration: 327 ====================
train total loss: 83.642%
train max loss: 14.275%, reg loss: 76.878%
time spent training so far: 1:27:38.579792
train attack total time: 9.422s
train attack init time: 4.130s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.148s


train attack loss increase over inner max: 1.172
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.871
total grad norm: 13.185

==================== evaluation at iteration: 328 ====================
train total loss: 83.226%
train max loss: 13.068%, reg loss: 76.830%
time spent training so far: 1:27:48.968031
train attack total time: 9.920s
train attack init time: 4.594s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.150s


train attack loss increase over inner max: 1.078
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.759
total grad norm: 14.409

==================== evaluation at iteration: 329 ====================
train total loss: 83.730%
train max loss: 13.619%, reg loss: 76.792%
time spent training so far: 1:28:00.805142
train attack total time: 11.371s
train attack init time: 4.553s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.218s


train attack loss increase over inner max: -0.066
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.274
total grad norm: 21.317

==================== evaluation at iteration: 330 ====================
train total loss: 85.189%
train max loss: 12.766%, reg loss: 76.997%
time spent training so far: 1:28:13.201326
train attack total time: 11.948s
train attack init time: 5.159s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -0.843
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.046
total grad norm: 11.790

==================== evaluation at iteration: 331 ====================
train total loss: 82.464%
train max loss: 11.905%, reg loss: 76.989%
time spent training so far: 1:28:25.034364
train attack total time: 11.330s
train attack init time: 4.970s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.197s


train attack loss increase over inner max: -0.221
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.855
total grad norm: 15.969

==================== evaluation at iteration: 332 ====================
train total loss: 85.130%
train max loss: 15.829%, reg loss: 76.807%
time spent training so far: 1:28:36.554986
train attack total time: 11.059s
train attack init time: 4.877s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.186s


train attack loss increase over inner max: 5.387
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.231
total grad norm: 14.510

==================== evaluation at iteration: 333 ====================
train total loss: 84.716%
train max loss: 14.286%, reg loss: 77.157%
time spent training so far: 1:28:47.851010
train attack total time: 10.815s
train attack init time: 4.838s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.179s


train attack loss increase over inner max: -0.182
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.266
total grad norm: 14.941

==================== evaluation at iteration: 334 ====================
train total loss: 84.048%
train max loss: 14.135%, reg loss: 77.146%
time spent training so far: 1:29:07.243014
train attack total time: 18.932s
train attack init time: 6.563s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.464s


train attack loss increase over inner max: -0.389
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.229
total grad norm: 9.989

==================== evaluation at iteration: 335 ====================
train total loss: 84.930%
train max loss: 15.546%, reg loss: 77.059%
time spent training so far: 1:29:21.042821
train attack total time: 13.323s
train attack init time: 5.972s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 2.075
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.469
total grad norm: 9.661

==================== evaluation at iteration: 336 ====================
train total loss: 84.116%
train max loss: 14.663%, reg loss: 77.131%
time spent training so far: 1:29:33.537789
train attack total time: 11.962s
train attack init time: 5.513s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.197s


train attack loss increase over inner max: 0.359
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.919
total grad norm: 11.900

==================== evaluation at iteration: 337 ====================
train total loss: 83.151%
train max loss: 13.347%, reg loss: 77.044%
time spent training so far: 1:29:45.992408
train attack total time: 11.985s
train attack init time: 5.399s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.208s


train attack loss increase over inner max: 0.243
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.178
total grad norm: 21.063

==================== evaluation at iteration: 338 ====================
train total loss: 84.717%
train max loss: 12.407%, reg loss: 77.182%
time spent training so far: 1:29:59.918831
train attack total time: 13.420s
train attack init time: 6.399s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.223s


train attack loss increase over inner max: 0.516
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.440
total grad norm: 15.442

==================== evaluation at iteration: 339 ====================
train total loss: 82.854%
train max loss: 11.916%, reg loss: 77.261%
time spent training so far: 1:30:12.300890
train attack total time: 11.883s
train attack init time: 5.877s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.180s


train attack loss increase over inner max: 0.994
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.024
total grad norm: 10.235

==================== evaluation at iteration: 340 ====================
train total loss: 82.446%
train max loss: 10.503%, reg loss: 77.201%
time spent training so far: 1:30:24.204064
train attack total time: 11.403s
train attack init time: 5.442s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.178s


train attack loss increase over inner max: -0.302
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.216
total grad norm: 16.799

==================== evaluation at iteration: 341 ====================
train total loss: 83.040%
train max loss: 15.866%, reg loss: 77.185%
time spent training so far: 1:30:35.520009
train attack total time: 10.741s
train attack init time: 5.667s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.137s


train attack loss increase over inner max: 6.495
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.985
total grad norm: 18.080

==================== evaluation at iteration: 342 ====================
train total loss: 83.870%
train max loss: 15.188%, reg loss: 77.530%
time spent training so far: 1:30:47.536180
train attack total time: 11.512s
train attack init time: 5.111s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.199s


train attack loss increase over inner max: -0.450
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.800
total grad norm: 22.989

==================== evaluation at iteration: 343 ====================
train total loss: 84.662%
train max loss: 14.099%, reg loss: 77.035%
time spent training so far: 1:30:59.193556
train attack total time: 11.149s
train attack init time: 5.480s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.169s


train attack loss increase over inner max: 0.282
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.266
total grad norm: 25.807

==================== evaluation at iteration: 344 ====================
train total loss: 82.379%
train max loss: 12.343%, reg loss: 77.286%
time spent training so far: 1:31:12.468662
train attack total time: 12.824s
train attack init time: 4.998s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.262s


train attack loss increase over inner max: 1.587
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.646
total grad norm: 19.277

==================== evaluation at iteration: 345 ====================
train total loss: 81.443%
train max loss: 12.089%, reg loss: 77.122%
time spent training so far: 1:31:25.748894
train attack total time: 12.807s
train attack init time: 5.547s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: 3.483
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.197
total grad norm: 20.683

==================== evaluation at iteration: 346 ====================
train total loss: 81.963%
train max loss: 9.124%, reg loss: 77.352%
time spent training so far: 1:31:39.814551
train attack total time: 13.526s
train attack init time: 6.364s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: 1.457
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.090
total grad norm: 28.235

==================== evaluation at iteration: 347 ====================
train total loss: 81.954%
train max loss: 8.716%, reg loss: 77.204%
time spent training so far: 1:31:54.879414
train attack total time: 14.521s
train attack init time: 7.045s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.247s


train attack loss increase over inner max: 0.887
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.013
total grad norm: 16.516

==================== evaluation at iteration: 348 ====================
train total loss: 81.160%
train max loss: 7.476%, reg loss: 77.225%
time spent training so far: 1:32:09.275886
train attack total time: 13.884s
train attack init time: 6.543s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.239s


train attack loss increase over inner max: 0.204
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.840
total grad norm: 19.430

==================== evaluation at iteration: 349 ====================
train total loss: 82.898%
train max loss: 12.122%, reg loss: 77.124%
time spent training so far: 1:32:21.683229
train attack total time: 11.936s
train attack init time: 6.144s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.172s


train attack loss increase over inner max: 6.233
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 2.669
total grad norm: 15.531

==================== evaluation at iteration: 350 ====================
train total loss: 82.187%
train max loss: 12.353%, reg loss: 77.101%
time spent training so far: 1:32:35.059702
train attack total time: 12.867s
train attack init time: 5.980s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.218s


train attack loss increase over inner max: -0.681
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.160% of volume
percentage infeasible at boundary: 4.60%
mean, std amount infeasible at boundary: 0.11 +/- 0.68
max amount infeasible at boundary: 9.44

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 3.383
total grad norm: 16.560

==================== evaluation at iteration: 351 ====================
train total loss: 82.358%
train max loss: 13.667%, reg loss: 77.369%
time spent training so far: 1:38:08.977466
train attack total time: 11.742s
train attack init time: 6.407s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.151s


train attack loss increase over inner max: 0.349
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.187
total grad norm: 19.450

==================== evaluation at iteration: 352 ====================
train total loss: 85.972%
train max loss: 15.479%, reg loss: 77.330%
time spent training so far: 1:38:23.062782
train attack total time: 13.597s
train attack init time: 7.163s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.200s


train attack loss increase over inner max: 2.374
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.744
total grad norm: 17.967

==================== evaluation at iteration: 353 ====================
train total loss: 85.259%
train max loss: 15.112%, reg loss: 77.531%
time spent training so far: 1:38:34.619138
train attack total time: 11.094s
train attack init time: 6.541s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.115s


train attack loss increase over inner max: 0.482
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.112
total grad norm: 20.091

==================== evaluation at iteration: 354 ====================
train total loss: 84.690%
train max loss: 13.951%, reg loss: 77.734%
time spent training so far: 1:38:47.985323
train attack total time: 12.887s
train attack init time: 5.883s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.223s


train attack loss increase over inner max: -0.508
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.154
total grad norm: 20.563

==================== evaluation at iteration: 355 ====================
train total loss: 84.199%
train max loss: 13.275%, reg loss: 77.797%
time spent training so far: 1:39:01.588257
train attack total time: 13.126s
train attack init time: 6.287s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.222s


train attack loss increase over inner max: 0.790
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.304
total grad norm: 18.997

==================== evaluation at iteration: 356 ====================
train total loss: 81.604%
train max loss: 9.368%, reg loss: 77.507%
time spent training so far: 1:39:18.660917
train attack total time: 16.580s
train attack init time: 8.111s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.294s


train attack loss increase over inner max: -0.601
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Max_n_steps: 20
Parameter containing:
tensor([[0.1853]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1851]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1850]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1848]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1846]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1843]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1839]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1836]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1832]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1828]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1824]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1820]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1817]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1813]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1809]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1805]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1801]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1797]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1793]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1788]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1783]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1778]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1773]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1768]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1762]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1757]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1751]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1746]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1740]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1735]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1730]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1725]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1720]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1716]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1712]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1708]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1703]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1698]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1693]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1689]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1685]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1681]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1678]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1675]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1672]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1669]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 3.631
total grad norm: 18.246

==================== evaluation at iteration: 357 ====================
train total loss: 83.828%
train max loss: 13.076%, reg loss: 77.562%
time spent training so far: 1:39:32.813155
train attack total time: 13.694s
train attack init time: 8.581s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.138s


train attack loss increase over inner max: 6.817
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.381
total grad norm: 19.077

==================== evaluation at iteration: 358 ====================
train total loss: 81.686%
train max loss: 10.667%, reg loss: 77.457%
time spent training so far: 1:39:53.142516
train attack total time: 19.820s
train attack init time: 9.225s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.386s


train attack loss increase over inner max: -0.352
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.272
total grad norm: 23.688

==================== evaluation at iteration: 359 ====================
train total loss: 80.085%
train max loss: 5.665%, reg loss: 77.425%
time spent training so far: 1:40:10.406498
train attack total time: 16.723s
train attack init time: 10.548s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.186s


train attack loss increase over inner max: -0.907
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.295
total grad norm: 15.518

==================== evaluation at iteration: 360 ====================
train total loss: 81.082%
train max loss: 6.105%, reg loss: 77.615%
time spent training so far: 1:40:25.964588
train attack total time: 15.074s
train attack init time: 8.776s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.194s


train attack loss increase over inner max: 1.357
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.777
total grad norm: 17.491

==================== evaluation at iteration: 361 ====================
train total loss: 81.941%
train max loss: 9.044%, reg loss: 77.785%
time spent training so far: 1:40:41.714930
train attack total time: 15.167s
train attack init time: 5.713s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.337s


train attack loss increase over inner max: 2.311
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.058
total grad norm: 12.794

==================== evaluation at iteration: 362 ====================
train total loss: 82.059%
train max loss: 10.304%, reg loss: 77.454%
time spent training so far: 1:40:54.422085
train attack total time: 12.226s
train attack init time: 5.491s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.212s


train attack loss increase over inner max: 1.019
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.451
total grad norm: 17.724

==================== evaluation at iteration: 363 ====================
train total loss: 83.073%
train max loss: 11.760%, reg loss: 77.686%
time spent training so far: 1:41:05.594052
train attack total time: 10.687s
train attack init time: 5.424s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.145s


train attack loss increase over inner max: 1.689
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.946
total grad norm: 24.125

==================== evaluation at iteration: 364 ====================
train total loss: 82.194%
train max loss: 10.111%, reg loss: 77.885%
time spent training so far: 1:41:19.867404
train attack total time: 13.770s
train attack init time: 6.782s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: -0.096
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.757
total grad norm: 20.328

==================== evaluation at iteration: 365 ====================
train total loss: 82.655%
train max loss: 9.818%, reg loss: 78.225%
time spent training so far: 1:41:33.930595
train attack total time: 13.614s
train attack init time: 6.970s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: 0.738
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.215
total grad norm: 13.757

==================== evaluation at iteration: 366 ====================
train total loss: 81.866%
train max loss: 8.761%, reg loss: 78.033%
time spent training so far: 1:41:48.138506
train attack total time: 13.686s
train attack init time: 7.330s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.193s


train attack loss increase over inner max: 0.223
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.445
total grad norm: 20.603

==================== evaluation at iteration: 367 ====================
train total loss: 81.351%
train max loss: 7.928%, reg loss: 77.691%
time spent training so far: 1:42:05.351795
train attack total time: 16.741s
train attack init time: 8.541s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.279s


train attack loss increase over inner max: 0.853
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.500
total grad norm: 10.899

==================== evaluation at iteration: 368 ====================
train total loss: 80.720%
train max loss: 7.413%, reg loss: 77.764%
time spent training so far: 1:42:24.629926
train attack total time: 18.758s
train attack init time: 11.538s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: 1.428
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.733
total grad norm: 15.186

==================== evaluation at iteration: 369 ====================
train total loss: 79.728%
train max loss: 3.985%, reg loss: 77.849%
time spent training so far: 1:42:46.047664
train attack total time: 20.931s
train attack init time: 14.158s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.210s


train attack loss increase over inner max: -1.672
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.123
total grad norm: 16.124

==================== evaluation at iteration: 370 ====================
train total loss: 80.201%
train max loss: 8.248%, reg loss: 77.592%
time spent training so far: 1:43:06.203081
train attack total time: 19.698s
train attack init time: 12.024s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.253s


train attack loss increase over inner max: 5.473
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.066
total grad norm: 24.453

==================== evaluation at iteration: 371 ====================
train total loss: 81.040%
train max loss: 9.006%, reg loss: 77.560%
time spent training so far: 1:43:31.830130
train attack total time: 25.080s
train attack init time: 18.240s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: 0.161
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.669
total grad norm: 42.588

==================== evaluation at iteration: 372 ====================
train total loss: 81.403%
train max loss: 5.936%, reg loss: 77.799%
time spent training so far: 1:43:53.159716
train attack total time: 20.829s
train attack init time: 12.460s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.287s


train attack loss increase over inner max: -1.136
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.441
total grad norm: 20.160

==================== evaluation at iteration: 373 ====================
train total loss: 81.129%
train max loss: 7.476%, reg loss: 77.712%
time spent training so far: 1:44:13.615776
train attack total time: 19.994s
train attack init time: 13.038s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.222s


train attack loss increase over inner max: 5.514
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.226
total grad norm: 14.922

==================== evaluation at iteration: 374 ====================
train total loss: 81.381%
train max loss: 7.240%, reg loss: 77.589%
time spent training so far: 1:44:31.758961
train attack total time: 17.680s
train attack init time: 11.776s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.172s


train attack loss increase over inner max: -0.007
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.222
total grad norm: 25.427

==================== evaluation at iteration: 375 ====================
train total loss: 82.754%
train max loss: 12.350%, reg loss: 77.478%
time spent training so far: 1:44:51.876174
train attack total time: 19.664s
train attack init time: 10.060s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.340s


train attack loss increase over inner max: 5.602
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.200% of volume
percentage infeasible at boundary: 2.16%
mean, std amount infeasible at boundary: 0.03 +/- 0.31
max amount infeasible at boundary: 5.48

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.029
total grad norm: 44.620

==================== evaluation at iteration: 376 ====================
train total loss: 83.639%
train max loss: 11.267%, reg loss: 77.961%
time spent training so far: 1:53:48.813771
train attack total time: 23.015s
train attack init time: 10.872s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.457s


train attack loss increase over inner max: -0.134
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.160
total grad norm: 15.700

==================== evaluation at iteration: 377 ====================
train total loss: 84.196%
train max loss: 10.826%, reg loss: 77.981%
time spent training so far: 1:54:05.739498
train attack total time: 16.421s
train attack init time: 9.094s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.238s


train attack loss increase over inner max: 3.348
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.080
total grad norm: 26.342

==================== evaluation at iteration: 378 ====================
train total loss: 80.841%
train max loss: 10.421%, reg loss: 77.536%
time spent training so far: 1:54:20.949407
train attack total time: 14.724s
train attack init time: 7.967s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.213s


train attack loss increase over inner max: 0.413
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.632
total grad norm: 13.817

==================== evaluation at iteration: 379 ====================
train total loss: 81.417%
train max loss: 8.913%, reg loss: 77.698%
time spent training so far: 1:54:37.500440
train attack total time: 16.043s
train attack init time: 10.366s
train attack avg grad step time: 0.086s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 2.203
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.243
total grad norm: 19.269

==================== evaluation at iteration: 380 ====================
train total loss: 81.472%
train max loss: 6.950%, reg loss: 77.531%
time spent training so far: 1:54:58.143369
train attack total time: 20.183s
train attack init time: 13.148s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.225s


train attack loss increase over inner max: -1.036
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.647
total grad norm: 19.664

==================== evaluation at iteration: 381 ====================
train total loss: 80.833%
train max loss: 5.847%, reg loss: 77.700%
time spent training so far: 1:55:19.089033
train attack total time: 20.398s
train attack init time: 11.833s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.297s


train attack loss increase over inner max: -0.380
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.304
total grad norm: 27.098

==================== evaluation at iteration: 382 ====================
train total loss: 81.298%
train max loss: 6.578%, reg loss: 77.597%
time spent training so far: 1:55:47.556020
train attack total time: 27.981s
train attack init time: 16.610s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.420s


train attack loss increase over inner max: 2.100
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.742
total grad norm: 19.467

==================== evaluation at iteration: 383 ====================
train total loss: 79.847%
train max loss: 4.079%, reg loss: 77.696%
time spent training so far: 1:56:10.056260
train attack total time: 21.976s
train attack init time: 13.560s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.288s


train attack loss increase over inner max: -1.863
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.325
total grad norm: 19.385

==================== evaluation at iteration: 384 ====================
train total loss: 79.213%
train max loss: 4.593%, reg loss: 77.488%
time spent training so far: 1:56:38.750102
train attack total time: 28.229s
train attack init time: 17.257s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.403s


train attack loss increase over inner max: 0.802
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.399
total grad norm: 33.436

==================== evaluation at iteration: 385 ====================
train total loss: 80.405%
train max loss: 4.723%, reg loss: 77.612%
time spent training so far: 1:57:02.821812
train attack total time: 23.590s
train attack init time: 13.180s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.379s


train attack loss increase over inner max: 2.985
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.557
total grad norm: 10.498

==================== evaluation at iteration: 386 ====================
train total loss: 79.703%
train max loss: 6.212%, reg loss: 77.637%
time spent training so far: 1:57:26.720812
train attack total time: 23.361s
train attack init time: 15.072s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.281s


train attack loss increase over inner max: 1.236
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.006
total grad norm: 23.285

==================== evaluation at iteration: 387 ====================
train total loss: 81.560%
train max loss: 8.705%, reg loss: 77.804%
time spent training so far: 1:57:52.525661
train attack total time: 25.321s
train attack init time: 11.888s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.514s


train attack loss increase over inner max: 2.971
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.484
total grad norm: 21.571

==================== evaluation at iteration: 388 ====================
train total loss: 81.226%
train max loss: 9.175%, reg loss: 77.598%
time spent training so far: 1:58:09.309978
train attack total time: 16.321s
train attack init time: 9.384s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.217s


train attack loss increase over inner max: 0.710
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.486
total grad norm: 34.911

==================== evaluation at iteration: 389 ====================
train total loss: 81.804%
train max loss: 9.226%, reg loss: 77.548%
time spent training so far: 1:58:27.548766
train attack total time: 17.767s
train attack init time: 9.970s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: 0.455
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.473
total grad norm: 25.780

==================== evaluation at iteration: 390 ====================
train total loss: 80.881%
train max loss: 6.687%, reg loss: 77.626%
time spent training so far: 1:58:48.097582
train attack total time: 20.080s
train attack init time: 9.381s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: 0.948
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.822
total grad norm: 30.576

==================== evaluation at iteration: 391 ====================
train total loss: 80.411%
train max loss: 5.813%, reg loss: 77.781%
time spent training so far: 1:59:13.463925
train attack total time: 24.807s
train attack init time: 14.943s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.351s


train attack loss increase over inner max: 1.305
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.889
total grad norm: 32.030

==================== evaluation at iteration: 392 ====================
train total loss: 79.490%
train max loss: 4.640%, reg loss: 77.813%
time spent training so far: 1:59:42.498615
train attack total time: 28.548s
train attack init time: 15.395s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.504s


train attack loss increase over inner max: 0.148
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.489
total grad norm: 45.403

==================== evaluation at iteration: 393 ====================
train total loss: 81.380%
train max loss: 7.354%, reg loss: 77.710%
time spent training so far: 2:00:20.542706
train attack total time: 37.544s
train attack init time: 24.941s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.477s


train attack loss increase over inner max: 4.913
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.543
total grad norm: 39.278

==================== evaluation at iteration: 394 ====================
train total loss: 82.662%
train max loss: 10.052%, reg loss: 77.644%
time spent training so far: 2:00:46.877971
train attack total time: 25.785s
train attack init time: 18.088s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.256s


train attack loss increase over inner max: 5.375
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.388
total grad norm: 34.575

==================== evaluation at iteration: 395 ====================
train total loss: 82.610%
train max loss: 8.955%, reg loss: 77.718%
time spent training so far: 2:01:14.149992
train attack total time: 26.711s
train attack init time: 18.112s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.294s


train attack loss increase over inner max: 4.243
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.594
total grad norm: 14.560

==================== evaluation at iteration: 396 ====================
train total loss: 78.907%
train max loss: 3.206%, reg loss: 77.713%
time spent training so far: 2:01:37.479807
train attack total time: 22.819s
train attack init time: 13.589s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.322s


train attack loss increase over inner max: 1.098
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.873
total grad norm: 25.102

==================== evaluation at iteration: 397 ====================
train total loss: 80.264%
train max loss: 4.309%, reg loss: 77.803%
time spent training so far: 2:01:58.465257
train attack total time: 20.485s
train attack init time: 13.040s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.243s


train attack loss increase over inner max: 0.956
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.129
total grad norm: 14.311

==================== evaluation at iteration: 398 ====================
train total loss: 80.592%
train max loss: 3.974%, reg loss: 78.075%
time spent training so far: 2:02:19.433011
train attack total time: 20.488s
train attack init time: 13.383s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.226s


train attack loss increase over inner max: 0.326
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.409
total grad norm: 18.986

==================== evaluation at iteration: 399 ====================
train total loss: 79.709%
train max loss: 4.125%, reg loss: 77.675%
time spent training so far: 2:02:42.174691
train attack total time: 22.241s
train attack init time: 14.670s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.253s


train attack loss increase over inner max: -0.373
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.583
total grad norm: 23.801

==================== evaluation at iteration: 400 ====================
train total loss: 82.768%
train max loss: 11.923%, reg loss: 77.729%
time spent training so far: 2:03:07.160101
train attack total time: 24.526s
train attack init time: 16.213s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.282s


train attack loss increase over inner max: 7.856
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.040% of volume
percentage infeasible at boundary: 2.48%
mean, std amount infeasible at boundary: 0.04 +/- 0.44
max amount infeasible at boundary: 12.11

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 3.947
total grad norm: 31.318

==================== evaluation at iteration: 401 ====================
train total loss: 83.126%
train max loss: 11.074%, reg loss: 77.926%
time spent training so far: 2:16:15.304157
train attack total time: 20.978s
train attack init time: 12.807s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.273s


train attack loss increase over inner max: 0.037
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1666]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1662]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1659]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1655]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1651]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1648]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1645]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1642]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1639]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1636]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1633]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1629]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1626]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1622]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1617]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1612]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1607]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1603]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1600]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1599]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1598]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1596]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1594]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1592]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1589]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1586]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1583]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1580]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1576]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1573]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1569]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1566]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1564]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1561]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1559]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1558]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1557]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1556]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1555]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1553]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1552]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1552]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1553]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 4.294
total grad norm: 33.347

==================== evaluation at iteration: 402 ====================
train total loss: 82.637%
train max loss: 8.021%, reg loss: 78.028%
time spent training so far: 2:16:38.057019
train attack total time: 22.249s
train attack init time: 14.230s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: 0.379
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.109
total grad norm: 21.475

==================== evaluation at iteration: 403 ====================
train total loss: 79.870%
train max loss: 4.849%, reg loss: 78.210%
time spent training so far: 2:17:07.187027
train attack total time: 28.632s
train attack init time: 19.225s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.327s


train attack loss increase over inner max: -0.059
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.033
total grad norm: 30.700

==================== evaluation at iteration: 404 ====================
train total loss: 80.763%
train max loss: 5.318%, reg loss: 78.117%
time spent training so far: 2:17:31.258269
train attack total time: 23.555s
train attack init time: 17.768s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.168s


train attack loss increase over inner max: 2.877
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.289
total grad norm: 25.885

==================== evaluation at iteration: 405 ====================
train total loss: 79.210%
train max loss: 2.317%, reg loss: 78.195%
time spent training so far: 2:18:03.923851
train attack total time: 32.121s
train attack init time: 21.127s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.404s


train attack loss increase over inner max: -1.415
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.710
total grad norm: 30.562

==================== evaluation at iteration: 406 ====================
train total loss: 79.396%
train max loss: 2.541%, reg loss: 77.997%
time spent training so far: 2:18:36.207744
train attack total time: 31.716s
train attack init time: 21.497s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.366s


train attack loss increase over inner max: -0.633
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.021
total grad norm: 36.412

==================== evaluation at iteration: 407 ====================
train total loss: 79.637%
train max loss: 2.353%, reg loss: 78.143%
time spent training so far: 2:19:10.732995
train attack total time: 34.032s
train attack init time: 26.446s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.247s


train attack loss increase over inner max: 1.965
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.204
total grad norm: 9.559

==================== evaluation at iteration: 408 ====================
train total loss: 79.670%
train max loss: 1.852%, reg loss: 78.175%
time spent training so far: 2:19:42.980011
train attack total time: 31.769s
train attack init time: 23.774s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.266s


train attack loss increase over inner max: 1.633
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.046
total grad norm: 8.257

==================== evaluation at iteration: 409 ====================
train total loss: 79.505%
train max loss: 3.548%, reg loss: 78.169%
time spent training so far: 2:20:10.123020
train attack total time: 26.672s
train attack init time: 19.118s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.249s


train attack loss increase over inner max: 0.896
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.907
total grad norm: 19.736

==================== evaluation at iteration: 410 ====================
train total loss: 81.415%
train max loss: 7.736%, reg loss: 78.127%
time spent training so far: 2:20:38.868701
train attack total time: 28.267s
train attack init time: 19.631s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.298s


train attack loss increase over inner max: 2.620
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.037
total grad norm: 17.386

==================== evaluation at iteration: 411 ====================
train total loss: 82.658%
train max loss: 8.085%, reg loss: 78.616%
time spent training so far: 2:21:03.262010
train attack total time: 23.834s
train attack init time: 17.111s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.208s


train attack loss increase over inner max: 0.693
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.691
total grad norm: 26.420

==================== evaluation at iteration: 412 ====================
train total loss: 83.863%
train max loss: 7.865%, reg loss: 78.515%
time spent training so far: 2:21:25.494899
train attack total time: 21.727s
train attack init time: 14.823s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -0.114
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.022
total grad norm: 28.216

==================== evaluation at iteration: 413 ====================
train total loss: 82.391%
train max loss: 7.739%, reg loss: 78.262%
time spent training so far: 2:21:47.603686
train attack total time: 21.582s
train attack init time: 14.224s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.236s


train attack loss increase over inner max: 0.172
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.290
total grad norm: 44.913

==================== evaluation at iteration: 414 ====================
train total loss: 82.582%
train max loss: 7.172%, reg loss: 78.218%
time spent training so far: 2:22:08.227908
train attack total time: 20.096s
train attack init time: 12.139s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.265s


train attack loss increase over inner max: 1.189
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.597
total grad norm: 35.930

==================== evaluation at iteration: 415 ====================
train total loss: 82.042%
train max loss: 6.380%, reg loss: 78.316%
time spent training so far: 2:22:31.042678
train attack total time: 22.331s
train attack init time: 14.843s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 2.429
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.252
total grad norm: 35.226

==================== evaluation at iteration: 416 ====================
train total loss: 81.290%
train max loss: 6.777%, reg loss: 78.320%
time spent training so far: 2:23:01.254810
train attack total time: 29.592s
train attack init time: 23.419s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.186s


train attack loss increase over inner max: 1.322
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.524
total grad norm: 24.614

==================== evaluation at iteration: 417 ====================
train total loss: 79.335%
train max loss: 2.182%, reg loss: 78.365%
time spent training so far: 2:23:36.470464
train attack total time: 34.712s
train attack init time: 26.562s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.271s


train attack loss increase over inner max: -1.796
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.903
total grad norm: 21.834

==================== evaluation at iteration: 418 ====================
train total loss: 79.195%
train max loss: 2.041%, reg loss: 78.229%
time spent training so far: 2:24:14.567030
train attack total time: 37.609s
train attack init time: 26.898s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.393s


train attack loss increase over inner max: -0.077
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.585
total grad norm: 22.914

==================== evaluation at iteration: 419 ====================
train total loss: 80.849%
train max loss: 6.538%, reg loss: 78.476%
time spent training so far: 2:24:58.066693
train attack total time: 42.984s
train attack init time: 34.382s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.297s


train attack loss increase over inner max: 5.008
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.675
total grad norm: 34.323

==================== evaluation at iteration: 420 ====================
train total loss: 80.443%
train max loss: 5.245%, reg loss: 78.452%
time spent training so far: 2:25:38.172526
train attack total time: 39.632s
train attack init time: 31.355s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.277s


train attack loss increase over inner max: -0.248
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.934
total grad norm: 12.957

==================== evaluation at iteration: 421 ====================
train total loss: 79.779%
train max loss: 1.412%, reg loss: 78.671%
time spent training so far: 2:26:12.876221
train attack total time: 34.100s
train attack init time: 25.375s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.302s


train attack loss increase over inner max: -0.689
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.083
total grad norm: 26.961

==================== evaluation at iteration: 422 ====================
train total loss: 79.394%
train max loss: 1.814%, reg loss: 78.336%
time spent training so far: 2:26:48.541126
train attack total time: 35.174s
train attack init time: 25.518s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.345s


train attack loss increase over inner max: 0.468
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.041
total grad norm: 14.941

==================== evaluation at iteration: 423 ====================
train total loss: 79.165%
train max loss: 1.279%, reg loss: 78.566%
time spent training so far: 2:27:20.963962
train attack total time: 31.916s
train attack init time: 25.105s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -0.003
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.421
total grad norm: 22.346

==================== evaluation at iteration: 424 ====================
train total loss: 80.527%
train max loss: 6.911%, reg loss: 78.326%
time spent training so far: 2:27:51.460418
train attack total time: 30.054s
train attack init time: 21.391s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.298s


train attack loss increase over inner max: 5.428
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.666
total grad norm: 32.649

==================== evaluation at iteration: 425 ====================
train total loss: 80.833%
train max loss: 5.633%, reg loss: 78.011%
time spent training so far: 2:28:28.520071
train attack total time: 36.536s
train attack init time: 24.427s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.453s


train attack loss increase over inner max: -2.389
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.080% of volume
percentage infeasible at boundary: 0.88%
mean, std amount infeasible at boundary: 0.01 +/- 0.20
max amount infeasible at boundary: 4.58

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.362
total grad norm: 30.396

==================== evaluation at iteration: 426 ====================
train total loss: 82.662%
train max loss: 11.062%, reg loss: 78.199%
time spent training so far: 2:52:24.356826
train attack total time: 40.001s
train attack init time: 31.194s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.306s


train attack loss increase over inner max: 8.182
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.361
total grad norm: 61.396

==================== evaluation at iteration: 427 ====================
train total loss: 81.399%
train max loss: 6.503%, reg loss: 78.209%
time spent training so far: 2:53:09.136586
train attack total time: 44.302s
train attack init time: 33.994s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.374s


train attack loss increase over inner max: -1.449
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.073
total grad norm: 19.017

==================== evaluation at iteration: 428 ====================
train total loss: 78.600%
train max loss: 0.812%, reg loss: 78.183%
time spent training so far: 2:54:12.897619
train attack total time: 63.304s
train attack init time: 40.581s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.935s


train attack loss increase over inner max: 1.970
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.276
total grad norm: 18.929

==================== evaluation at iteration: 429 ====================
train total loss: 78.700%
train max loss: 0.824%, reg loss: 78.287%
time spent training so far: 2:55:03.484703
train attack total time: 50.092s
train attack init time: 41.577s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.294s


train attack loss increase over inner max: 0.108
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.055
total grad norm: 8.442

==================== evaluation at iteration: 430 ====================
train total loss: 78.525%
train max loss: 0.654%, reg loss: 78.083%
time spent training so far: 2:55:59.265043
train attack total time: 55.325s
train attack init time: 46.543s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.302s


train attack loss increase over inner max: 0.442
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.406
total grad norm: 12.430

==================== evaluation at iteration: 431 ====================
train total loss: 78.687%
train max loss: 1.263%, reg loss: 78.219%
time spent training so far: 2:57:06.962959
train attack total time: 67.168s
train attack init time: 58.865s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.284s


train attack loss increase over inner max: 0.858
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.233
total grad norm: 20.777

==================== evaluation at iteration: 432 ====================
train total loss: 79.279%
train max loss: 1.312%, reg loss: 78.178%
time spent training so far: 2:58:02.319435
train attack total time: 54.852s
train attack init time: 46.656s
train attack avg grad step time: 0.098s
train attack avg reproj time: 0.272s


train attack loss increase over inner max: 1.241
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.914
total grad norm: 34.503

==================== evaluation at iteration: 433 ====================
train total loss: 78.746%
train max loss: 1.370%, reg loss: 78.094%
time spent training so far: 2:58:56.214859
train attack total time: 53.386s
train attack init time: 42.717s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.391s


train attack loss increase over inner max: -0.366
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.693
total grad norm: 58.541

==================== evaluation at iteration: 434 ====================
train total loss: 81.436%
train max loss: 7.199%, reg loss: 78.334%
time spent training so far: 2:59:29.427942
train attack total time: 32.704s
train attack init time: 24.518s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.276s


train attack loss increase over inner max: 6.260
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.873
total grad norm: 38.619

==================== evaluation at iteration: 435 ====================
train total loss: 84.204%
train max loss: 10.247%, reg loss: 77.989%
time spent training so far: 3:00:09.551412
train attack total time: 39.660s
train attack init time: 31.977s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.255s


train attack loss increase over inner max: 4.874
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.356
total grad norm: 28.514

==================== evaluation at iteration: 436 ====================
train total loss: 85.237%
train max loss: 13.417%, reg loss: 78.250%
time spent training so far: 3:00:51.271159
train attack total time: 41.165s
train attack init time: 33.336s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.260s


train attack loss increase over inner max: 4.237
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.051
total grad norm: 67.394

==================== evaluation at iteration: 437 ====================
train total loss: 88.289%
train max loss: 12.727%, reg loss: 78.076%
time spent training so far: 3:01:41.349468
train attack total time: 49.531s
train attack init time: 42.004s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.244s


train attack loss increase over inner max: 1.182
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.402
total grad norm: 85.125

==================== evaluation at iteration: 438 ====================
train total loss: 83.183%
train max loss: 7.204%, reg loss: 78.267%
time spent training so far: 3:02:26.572880
train attack total time: 44.703s
train attack init time: 35.293s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.334s


train attack loss increase over inner max: -0.925
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.011
total grad norm: 51.474

==================== evaluation at iteration: 439 ====================
train total loss: 78.554%
train max loss: 0.382%, reg loss: 78.172%
time spent training so far: 3:03:29.610532
train attack total time: 62.520s
train attack init time: 51.749s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: 0.265
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.138
total grad norm: 49.983

==================== evaluation at iteration: 440 ====================
train total loss: 78.398%
train max loss: 0.298%, reg loss: 78.100%
time spent training so far: 3:04:20.435018
train attack total time: 50.346s
train attack init time: 41.601s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: 0.569
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.442
total grad norm: 32.053

==================== evaluation at iteration: 441 ====================
train total loss: 78.895%
train max loss: 0.469%, reg loss: 78.426%
time spent training so far: 3:05:06.852252
train attack total time: 45.865s
train attack init time: 36.533s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.330s


train attack loss increase over inner max: 0.080
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.451
total grad norm: 26.675

==================== evaluation at iteration: 442 ====================
train total loss: 78.745%
train max loss: 1.161%, reg loss: 78.280%
time spent training so far: 3:06:10.048511
train attack total time: 62.681s
train attack init time: 52.794s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.354s


train attack loss increase over inner max: 1.346
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.457
total grad norm: 17.327

==================== evaluation at iteration: 443 ====================
train total loss: 78.599%
train max loss: 0.497%, reg loss: 78.302%
time spent training so far: 3:07:00.971223
train attack total time: 50.459s
train attack init time: 41.941s
train attack avg grad step time: 0.098s
train attack avg reproj time: 0.287s


train attack loss increase over inner max: -0.314
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.301
total grad norm: 9.569

==================== evaluation at iteration: 444 ====================
train total loss: 78.746%
train max loss: 0.562%, reg loss: 78.299%
time spent training so far: 3:07:56.648863
train attack total time: 55.172s
train attack init time: 46.266s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.312s


train attack loss increase over inner max: 0.363
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.411
total grad norm: 27.272

==================== evaluation at iteration: 445 ====================
train total loss: 78.974%
train max loss: 0.696%, reg loss: 78.327%
time spent training so far: 3:08:55.675980
train attack total time: 58.546s
train attack init time: 49.917s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.300s


train attack loss increase over inner max: 0.480
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.962
total grad norm: 13.893

==================== evaluation at iteration: 446 ====================
train total loss: 78.884%
train max loss: 1.311%, reg loss: 78.253%
time spent training so far: 3:09:48.648107
train attack total time: 52.425s
train attack init time: 44.086s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.284s


train attack loss increase over inner max: 0.903
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.776
total grad norm: 5.275

==================== evaluation at iteration: 447 ====================
train total loss: 78.688%
train max loss: 0.663%, reg loss: 78.025%
time spent training so far: 3:10:36.067846
train attack total time: 46.881s
train attack init time: 38.190s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.298s


train attack loss increase over inner max: -0.798
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Max_n_steps: 20
Parameter containing:
tensor([[0.1555]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1557]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1558]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1558]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1557]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1556]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1553]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1551]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1548]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1544]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1540]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1536]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1532]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1528]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1524]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1521]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1517]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1513]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1509]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1505]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1501]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1496]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1493]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1490]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1488]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1486]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1483]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1481]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1478]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1475]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1471]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1467]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1465]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1466]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1470]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1477]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1488]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1497]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1505]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1511]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1517]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1526]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1528]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1530]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1531]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 4.339
total grad norm: 18.777

==================== evaluation at iteration: 448 ====================
train total loss: 78.869%
train max loss: 1.523%, reg loss: 78.233%
time spent training so far: 3:11:41.994960
train attack total time: 65.478s
train attack init time: 54.052s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.425s


train attack loss increase over inner max: 0.362
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.147
total grad norm: 8.556

==================== evaluation at iteration: 449 ====================
train total loss: 79.225%
train max loss: 1.460%, reg loss: 78.190%
time spent training so far: 3:12:36.163882
train attack total time: 53.655s
train attack init time: 42.030s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.431s


train attack loss increase over inner max: 0.421
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.045
total grad norm: 11.639

==================== evaluation at iteration: 450 ====================
train total loss: 78.946%
train max loss: 0.992%, reg loss: 78.097%
time spent training so far: 3:13:24.938054
train attack total time: 48.294s
train attack init time: 40.364s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: -0.104
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.040% of volume
percentage infeasible at boundary: 0.04%
mean, std amount infeasible at boundary: 0.00 +/- 0.00
max amount infeasible at boundary: 0.05

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 3.940
total grad norm: 18.207

==================== evaluation at iteration: 451 ====================
train total loss: 78.467%
train max loss: 1.044%, reg loss: 77.968%
time spent training so far: 3:50:25.757053
train attack total time: 52.518s
train attack init time: 41.849s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.389s


train attack loss increase over inner max: 0.526
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.224
total grad norm: 11.157

==================== evaluation at iteration: 452 ====================
train total loss: 78.909%
train max loss: 1.546%, reg loss: 78.101%
time spent training so far: 3:51:19.038095
train attack total time: 52.806s
train attack init time: 44.692s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.270s


train attack loss increase over inner max: 0.965
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.297
total grad norm: 15.256

==================== evaluation at iteration: 453 ====================
train total loss: 78.806%
train max loss: 1.107%, reg loss: 78.047%
time spent training so far: 3:52:11.818398
train attack total time: 52.251s
train attack init time: 43.372s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.309s


train attack loss increase over inner max: 0.128
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.315
total grad norm: 20.943

==================== evaluation at iteration: 454 ====================
train total loss: 78.480%
train max loss: 1.100%, reg loss: 77.968%
time spent training so far: 3:52:57.181207
train attack total time: 44.808s
train attack init time: 37.363s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.244s


train attack loss increase over inner max: 0.391
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.203
total grad norm: 32.490

==================== evaluation at iteration: 455 ====================
train total loss: 79.115%
train max loss: 1.156%, reg loss: 77.959%
time spent training so far: 3:53:58.390131
train attack total time: 60.670s
train attack init time: 50.047s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.388s


train attack loss increase over inner max: -0.235
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.391
total grad norm: 42.814

==================== evaluation at iteration: 456 ====================
train total loss: 78.808%
train max loss: 2.339%, reg loss: 77.561%
time spent training so far: 3:54:44.998721
train attack total time: 46.076s
train attack init time: 39.698s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.193s


train attack loss increase over inner max: 2.019
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.522
total grad norm: 60.135

==================== evaluation at iteration: 457 ====================
train total loss: 79.050%
train max loss: 3.608%, reg loss: 77.618%
time spent training so far: 3:55:33.012008
train attack total time: 47.509s
train attack init time: 39.704s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.259s


train attack loss increase over inner max: 1.542
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.525
total grad norm: 27.875

==================== evaluation at iteration: 458 ====================
train total loss: 78.230%
train max loss: 2.053%, reg loss: 77.491%
time spent training so far: 3:56:18.984996
train attack total time: 45.497s
train attack init time: 30.720s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.576s


train attack loss increase over inner max: 2.137
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.620
total grad norm: 81.919

==================== evaluation at iteration: 459 ====================
train total loss: 84.670%
train max loss: 9.808%, reg loss: 77.530%
time spent training so far: 3:56:57.527323
train attack total time: 38.029s
train attack init time: 28.821s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: 8.722
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.793
total grad norm: 58.624

==================== evaluation at iteration: 460 ====================
train total loss: 82.730%
train max loss: 9.409%, reg loss: 77.633%
time spent training so far: 3:57:42.696087
train attack total time: 44.685s
train attack init time: 31.830s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.488s


train attack loss increase over inner max: 3.530
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.487
total grad norm: 25.639

==================== evaluation at iteration: 461 ====================
train total loss: 79.421%
train max loss: 4.283%, reg loss: 77.493%
time spent training so far: 3:58:30.590910
train attack total time: 47.317s
train attack init time: 39.289s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.270s


train attack loss increase over inner max: 1.898
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.557
total grad norm: 32.966

==================== evaluation at iteration: 462 ====================
train total loss: 80.995%
train max loss: 9.733%, reg loss: 77.613%
time spent training so far: 3:59:16.186147
train attack total time: 45.076s
train attack init time: 33.483s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.429s


train attack loss increase over inner max: 5.869
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.131
total grad norm: 70.984

==================== evaluation at iteration: 463 ====================
train total loss: 88.471%
train max loss: 15.256%, reg loss: 77.781%
time spent training so far: 4:00:05.905394
train attack total time: 49.195s
train attack init time: 38.420s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.390s


train attack loss increase over inner max: 9.413
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.706
total grad norm: 82.176

==================== evaluation at iteration: 464 ====================
train total loss: 85.861%
train max loss: 11.197%, reg loss: 77.796%
time spent training so far: 4:00:53.096385
train attack total time: 46.678s
train attack init time: 30.881s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.623s


train attack loss increase over inner max: 2.037
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.910
total grad norm: 83.754

==================== evaluation at iteration: 465 ====================
train total loss: 84.655%
train max loss: 11.283%, reg loss: 77.840%
time spent training so far: 4:01:31.985459
train attack total time: 38.436s
train attack init time: 30.309s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.274s


train attack loss increase over inner max: 5.185
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.943
total grad norm: 93.466

==================== evaluation at iteration: 466 ====================
train total loss: 79.480%
train max loss: 2.610%, reg loss: 77.798%
time spent training so far: 4:02:30.026039
train attack total time: 57.495s
train attack init time: 32.513s
train attack avg grad step time: 0.094s
train attack avg reproj time: 1.037s


train attack loss increase over inner max: -2.182
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.234
total grad norm: 32.051

==================== evaluation at iteration: 467 ====================
train total loss: 79.175%
train max loss: 2.462%, reg loss: 77.933%
time spent training so far: 4:03:14.085140
train attack total time: 43.596s
train attack init time: 34.585s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.311s


train attack loss increase over inner max: 5.503
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.753
total grad norm: 44.145

==================== evaluation at iteration: 468 ====================
train total loss: 78.299%
train max loss: 1.193%, reg loss: 77.846%
time spent training so far: 4:03:48.411251
train attack total time: 33.825s
train attack init time: 25.299s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.291s


train attack loss increase over inner max: -2.271
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.042
total grad norm: 63.267

==================== evaluation at iteration: 469 ====================
train total loss: 83.221%
train max loss: 8.567%, reg loss: 78.045%
time spent training so far: 4:04:31.606593
train attack total time: 42.684s
train attack init time: 34.565s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.272s


train attack loss increase over inner max: 8.276
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.440
total grad norm: 31.428

==================== evaluation at iteration: 470 ====================
train total loss: 82.327%
train max loss: 8.555%, reg loss: 78.235%
time spent training so far: 4:05:03.647528
train attack total time: 31.519s
train attack init time: 22.679s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.308s


train attack loss increase over inner max: 5.783
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.489
total grad norm: 36.466

==================== evaluation at iteration: 471 ====================
train total loss: 83.481%
train max loss: 10.265%, reg loss: 78.313%
time spent training so far: 4:05:36.546520
train attack total time: 32.344s
train attack init time: 24.763s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.250s


train attack loss increase over inner max: 1.508
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.610
total grad norm: 43.239

==================== evaluation at iteration: 472 ====================
train total loss: 86.346%
train max loss: 15.432%, reg loss: 78.389%
time spent training so far: 4:06:03.497452
train attack total time: 26.462s
train attack init time: 19.274s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.233s


train attack loss increase over inner max: 5.344
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.072
total grad norm: 61.526

==================== evaluation at iteration: 473 ====================
train total loss: 84.572%
train max loss: 13.735%, reg loss: 78.162%
time spent training so far: 4:06:36.039075
train attack total time: 32.063s
train attack init time: 22.765s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.325s


train attack loss increase over inner max: -0.058
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.159
total grad norm: 58.373

==================== evaluation at iteration: 474 ====================
train total loss: 87.884%
train max loss: 15.497%, reg loss: 78.312%
time spent training so far: 4:07:09.039838
train attack total time: 32.527s
train attack init time: 25.087s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.243s


train attack loss increase over inner max: 7.533
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.956
total grad norm: 29.211

==================== evaluation at iteration: 475 ====================
train total loss: 79.357%
train max loss: 1.113%, reg loss: 78.791%
time spent training so far: 4:07:50.722825
train attack total time: 41.217s
train attack init time: 29.412s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.439s


train attack loss increase over inner max: -8.636
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.32%
mean, std amount infeasible at boundary: 0.00 +/- 0.04
max amount infeasible at boundary: 1.56

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 5.515
total grad norm: 15.206

==================== evaluation at iteration: 476 ====================
train total loss: 79.793%
train max loss: 1.729%, reg loss: 79.063%
time spent training so far: 4:35:08.959179
train attack total time: 37.928s
train attack init time: 29.340s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.296s


train attack loss increase over inner max: 0.797
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.340
total grad norm: 30.102

==================== evaluation at iteration: 477 ====================
train total loss: 79.412%
train max loss: 1.273%, reg loss: 78.592%
time spent training so far: 4:35:50.973074
train attack total time: 41.558s
train attack init time: 30.100s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.427s


train attack loss increase over inner max: -0.162
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.203
total grad norm: 12.575

==================== evaluation at iteration: 478 ====================
train total loss: 79.766%
train max loss: 1.053%, reg loss: 79.067%
time spent training so far: 4:36:35.812258
train attack total time: 44.365s
train attack init time: 33.270s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.405s


train attack loss increase over inner max: 0.980
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.161
total grad norm: 13.351

==================== evaluation at iteration: 479 ====================
train total loss: 80.303%
train max loss: 2.251%, reg loss: 79.020%
time spent training so far: 4:37:16.979849
train attack total time: 40.695s
train attack init time: 29.842s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.399s


train attack loss increase over inner max: 1.022
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.772
total grad norm: 23.988

==================== evaluation at iteration: 480 ====================
train total loss: 80.607%
train max loss: 4.749%, reg loss: 78.939%
time spent training so far: 4:38:04.045031
train attack total time: 46.522s
train attack init time: 34.089s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.469s


train attack loss increase over inner max: 2.170
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.969
total grad norm: 69.637

==================== evaluation at iteration: 481 ====================
train total loss: 80.292%
train max loss: 1.938%, reg loss: 78.949%
time spent training so far: 4:38:51.309190
train attack total time: 46.697s
train attack init time: 36.434s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: -2.857
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.123
total grad norm: 26.812

==================== evaluation at iteration: 482 ====================
train total loss: 80.553%
train max loss: 2.474%, reg loss: 79.129%
time spent training so far: 4:39:31.983521
train attack total time: 40.223s
train attack init time: 34.113s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: 2.498
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.062
total grad norm: 52.079

==================== evaluation at iteration: 483 ====================
train total loss: 79.287%
train max loss: 1.185%, reg loss: 78.611%
time spent training so far: 4:40:24.769778
train attack total time: 52.322s
train attack init time: 42.234s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.368s


train attack loss increase over inner max: 0.841
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.529
total grad norm: 12.724

==================== evaluation at iteration: 484 ====================
train total loss: 79.931%
train max loss: 1.134%, reg loss: 79.262%
time spent training so far: 4:41:10.863257
train attack total time: 45.617s
train attack init time: 36.242s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.333s


train attack loss increase over inner max: 0.113
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.386
total grad norm: 13.073

==================== evaluation at iteration: 485 ====================
train total loss: 79.996%
train max loss: 1.617%, reg loss: 79.118%
time spent training so far: 4:42:07.142521
train attack total time: 55.812s
train attack init time: 47.677s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.273s


train attack loss increase over inner max: 0.607
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.833
total grad norm: 26.361

==================== evaluation at iteration: 486 ====================
train total loss: 79.853%
train max loss: 1.361%, reg loss: 79.006%
time spent training so far: 4:43:03.266549
train attack total time: 55.543s
train attack init time: 37.049s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.741s


train attack loss increase over inner max: 0.009
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.975
total grad norm: 31.423

==================== evaluation at iteration: 487 ====================
train total loss: 81.417%
train max loss: 2.949%, reg loss: 79.485%
time spent training so far: 4:43:52.156731
train attack total time: 48.378s
train attack init time: 40.455s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.266s


train attack loss increase over inner max: 0.925
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.178
total grad norm: 24.308

==================== evaluation at iteration: 488 ====================
train total loss: 80.969%
train max loss: 5.732%, reg loss: 79.038%
time spent training so far: 4:44:30.949180
train attack total time: 38.316s
train attack init time: 28.892s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.335s


train attack loss increase over inner max: 2.211
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.241
total grad norm: 24.415

==================== evaluation at iteration: 489 ====================
train total loss: 81.660%
train max loss: 4.271%, reg loss: 79.577%
time spent training so far: 4:45:05.249632
train attack total time: 33.874s
train attack init time: 27.175s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.214s


train attack loss increase over inner max: -1.085
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.403
total grad norm: 26.095

==================== evaluation at iteration: 490 ====================
train total loss: 81.352%
train max loss: 5.197%, reg loss: 79.253%
time spent training so far: 4:45:41.369585
train attack total time: 35.654s
train attack init time: 27.267s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.283s


train attack loss increase over inner max: -0.030
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.605
total grad norm: 14.391

==================== evaluation at iteration: 491 ====================
train total loss: 80.923%
train max loss: 4.040%, reg loss: 78.757%
time spent training so far: 4:46:18.595043
train attack total time: 36.723s
train attack init time: 27.095s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.344s


train attack loss increase over inner max: 0.300
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.846
total grad norm: 32.122

==================== evaluation at iteration: 492 ====================
train total loss: 83.017%
train max loss: 5.782%, reg loss: 78.807%
time spent training so far: 4:47:15.094541
train attack total time: 56.002s
train attack init time: 47.808s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.276s


train attack loss increase over inner max: 2.373
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1532]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1532]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1531]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1530]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1529]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1527]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1525]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1522]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1519]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1517]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1515]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1518]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1523]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1529]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1535]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1545]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1558]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1572]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1586]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1600]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1612]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1623]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1635]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1646]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1658]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1669]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1682]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1693]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1702]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1710]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1717]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1722]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1727]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1731]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1735]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1738]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1741]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1742]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1743]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1743]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1742]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1741]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1739]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1737]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1735]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 4.670
total grad norm: 47.396

==================== evaluation at iteration: 493 ====================
train total loss: 82.308%
train max loss: 3.655%, reg loss: 78.798%
time spent training so far: 4:47:56.621132
train attack total time: 41.027s
train attack init time: 32.821s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.275s


train attack loss increase over inner max: -0.961
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.927
total grad norm: 13.351

==================== evaluation at iteration: 494 ====================
train total loss: 81.454%
train max loss: 3.505%, reg loss: 79.352%
time spent training so far: 4:48:42.304847
train attack total time: 45.198s
train attack init time: 37.627s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.252s


train attack loss increase over inner max: 1.293
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.649
total grad norm: 50.058

==================== evaluation at iteration: 495 ====================
train total loss: 79.157%
train max loss: 0.279%, reg loss: 78.937%
time spent training so far: 4:49:30.859451
train attack total time: 48.080s
train attack init time: 33.715s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.556s


train attack loss increase over inner max: -2.072
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.423
total grad norm: 53.309

==================== evaluation at iteration: 496 ====================
train total loss: 79.935%
train max loss: 0.893%, reg loss: 79.043%
time spent training so far: 4:50:18.264517
train attack total time: 46.837s
train attack init time: 38.778s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.271s


train attack loss increase over inner max: 0.936
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.214
total grad norm: 31.023

==================== evaluation at iteration: 497 ====================
train total loss: 79.407%
train max loss: 0.906%, reg loss: 78.942%
time spent training so far: 4:51:11.898948
train attack total time: 53.118s
train attack init time: 38.660s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.561s


train attack loss increase over inner max: 1.677
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.762
total grad norm: 51.263

==================== evaluation at iteration: 498 ====================
train total loss: 79.486%
train max loss: 0.999%, reg loss: 78.755%
time spent training so far: 4:52:07.867090
train attack total time: 55.464s
train attack init time: 46.089s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.331s


train attack loss increase over inner max: -0.767
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.804
total grad norm: 4.804

==================== evaluation at iteration: 499 ====================
train total loss: 78.590%
train max loss: -0.480%, reg loss: 78.590%
time spent training so far: 4:53:05.029385
train attack total time: 56.683s
train attack init time: 46.974s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.348s


train attack loss increase over inner max: 0.141
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.059
total grad norm: 42.567

==================== evaluation at iteration: 500 ====================
train total loss: 78.479%
train max loss: 0.309%, reg loss: 78.272%
time spent training so far: 4:53:54.455536
train attack total time: 48.939s
train attack init time: 39.654s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.328s


train attack loss increase over inner max: 0.573
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.40%
mean, std amount infeasible at boundary: 0.00 +/- 0.13
max amount infeasible at boundary: 5.45

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.491
total grad norm: 4.491

==================== evaluation at iteration: 501 ====================
train total loss: 78.481%
train max loss: -0.009%, reg loss: 78.481%
time spent training so far: 5:24:45.331141
train attack total time: 49.173s
train attack init time: 39.610s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.341s


train attack loss increase over inner max: -0.219
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.322
total grad norm: 27.421

==================== evaluation at iteration: 502 ====================
train total loss: 79.980%
train max loss: 1.775%, reg loss: 78.767%
time spent training so far: 5:25:22.090749
train attack total time: 36.261s
train attack init time: 27.321s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.312s


train attack loss increase over inner max: 1.998
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.586
total grad norm: 26.580

==================== evaluation at iteration: 503 ====================
train total loss: 81.732%
train max loss: 7.240%, reg loss: 78.362%
time spent training so far: 5:26:09.915611
train attack total time: 47.329s
train attack init time: 33.628s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.527s


train attack loss increase over inner max: 6.226
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.179
total grad norm: 23.311

==================== evaluation at iteration: 504 ====================
train total loss: 81.508%
train max loss: 6.411%, reg loss: 78.195%
time spent training so far: 5:26:54.452434
train attack total time: 44.058s
train attack init time: 30.353s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.528s


train attack loss increase over inner max: -0.391
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.201
total grad norm: 22.318

==================== evaluation at iteration: 505 ====================
train total loss: 83.808%
train max loss: 13.324%, reg loss: 78.152%
time spent training so far: 5:27:30.412337
train attack total time: 35.452s
train attack init time: 23.831s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.431s


train attack loss increase over inner max: 7.315
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.004
total grad norm: 28.854

==================== evaluation at iteration: 506 ====================
train total loss: 84.450%
train max loss: 12.753%, reg loss: 78.369%
time spent training so far: 5:28:05.356862
train attack total time: 34.410s
train attack init time: 25.318s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.319s


train attack loss increase over inner max: -0.738
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.679
total grad norm: 25.681

==================== evaluation at iteration: 507 ====================
train total loss: 83.350%
train max loss: 12.715%, reg loss: 78.265%
time spent training so far: 5:28:36.285372
train attack total time: 30.443s
train attack init time: 23.157s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: 2.529
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.034
total grad norm: 37.616

==================== evaluation at iteration: 508 ====================
train total loss: 83.542%
train max loss: 12.355%, reg loss: 77.993%
time spent training so far: 5:29:06.932471
train attack total time: 30.193s
train attack init time: 22.860s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.240s


train attack loss increase over inner max: 2.998
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.338
total grad norm: 45.285

==================== evaluation at iteration: 509 ====================
train total loss: 86.432%
train max loss: 17.701%, reg loss: 78.077%
time spent training so far: 5:29:51.265171
train attack total time: 43.875s
train attack init time: 36.903s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.226s


train attack loss increase over inner max: 7.203
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.067
total grad norm: 77.824

==================== evaluation at iteration: 510 ====================
train total loss: 85.478%
train max loss: 13.277%, reg loss: 78.029%
time spent training so far: 5:30:25.704265
train attack total time: 33.977s
train attack init time: 26.055s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.262s


train attack loss increase over inner max: 2.727
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.707
total grad norm: 38.996

==================== evaluation at iteration: 511 ====================
train total loss: 84.366%
train max loss: 13.024%, reg loss: 78.277%
time spent training so far: 5:31:14.749690
train attack total time: 48.467s
train attack init time: 39.882s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.293s


train attack loss increase over inner max: 1.060
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.010
total grad norm: 38.994

==================== evaluation at iteration: 512 ====================
train total loss: 83.906%
train max loss: 13.666%, reg loss: 78.052%
time spent training so far: 5:31:52.454886
train attack total time: 37.241s
train attack init time: 26.934s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.370s


train attack loss increase over inner max: 0.170
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.222
total grad norm: 23.850

==================== evaluation at iteration: 513 ====================
train total loss: 84.442%
train max loss: 14.237%, reg loss: 78.042%
time spent training so far: 5:32:27.444292
train attack total time: 34.517s
train attack init time: 26.148s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.286s


train attack loss increase over inner max: -0.057
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.834
total grad norm: 36.226

==================== evaluation at iteration: 514 ====================
train total loss: 84.723%
train max loss: 13.217%, reg loss: 78.255%
time spent training so far: 5:33:00.476136
train attack total time: 32.516s
train attack init time: 25.597s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.219s


train attack loss increase over inner max: 0.216
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.510
total grad norm: 37.626

==================== evaluation at iteration: 515 ====================
train total loss: 87.010%
train max loss: 17.790%, reg loss: 78.172%
time spent training so far: 5:33:35.076836
train attack total time: 34.090s
train attack init time: 28.182s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.173s


train attack loss increase over inner max: 4.771
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.333
total grad norm: 41.522

==================== evaluation at iteration: 516 ====================
train total loss: 86.666%
train max loss: 15.418%, reg loss: 78.081%
time spent training so far: 5:34:08.697204
train attack total time: 33.048s
train attack init time: 22.979s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.363s


train attack loss increase over inner max: -0.459
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.456
total grad norm: 42.058

==================== evaluation at iteration: 517 ====================
train total loss: 83.717%
train max loss: 11.908%, reg loss: 78.104%
time spent training so far: 5:34:42.368344
train attack total time: 33.119s
train attack init time: 25.970s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.228s


train attack loss increase over inner max: -0.314
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.269
total grad norm: 28.248

==================== evaluation at iteration: 518 ====================
train total loss: 80.506%
train max loss: 5.496%, reg loss: 78.088%
time spent training so far: 5:35:13.215814
train attack total time: 30.338s
train attack init time: 21.301s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.318s


train attack loss increase over inner max: -1.740
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.672
total grad norm: 18.771

==================== evaluation at iteration: 519 ====================
train total loss: 83.007%
train max loss: 9.427%, reg loss: 78.156%
time spent training so far: 5:35:42.600635
train attack total time: 28.918s
train attack init time: 20.857s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.271s


train attack loss increase over inner max: 4.464
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.502
total grad norm: 29.988

==================== evaluation at iteration: 520 ====================
train total loss: 86.934%
train max loss: 20.137%, reg loss: 78.200%
time spent training so far: 5:36:09.621351
train attack total time: 26.533s
train attack init time: 19.183s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.238s


train attack loss increase over inner max: 11.623
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.797
total grad norm: 65.473

==================== evaluation at iteration: 521 ====================
train total loss: 88.643%
train max loss: 17.493%, reg loss: 78.260%
time spent training so far: 5:36:41.765388
train attack total time: 31.629s
train attack init time: 22.211s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.333s


train attack loss increase over inner max: -2.165
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.609
total grad norm: 27.103

==================== evaluation at iteration: 522 ====================
train total loss: 83.211%
train max loss: 10.310%, reg loss: 78.223%
time spent training so far: 5:37:08.166859
train attack total time: 25.869s
train attack init time: 18.295s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.251s


train attack loss increase over inner max: -2.333
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.881
total grad norm: 35.964

==================== evaluation at iteration: 523 ====================
train total loss: 84.948%
train max loss: 13.323%, reg loss: 78.341%
time spent training so far: 5:37:42.729016
train attack total time: 34.134s
train attack init time: 27.530s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.209s


train attack loss increase over inner max: 8.983
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.546
total grad norm: 29.808

==================== evaluation at iteration: 524 ====================
train total loss: 85.174%
train max loss: 11.472%, reg loss: 78.246%
time spent training so far: 5:38:17.877385
train attack total time: 34.648s
train attack init time: 24.966s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.345s


train attack loss increase over inner max: 0.212
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.156
total grad norm: 71.865

==================== evaluation at iteration: 525 ====================
train total loss: 86.680%
train max loss: 8.823%, reg loss: 78.452%
time spent training so far: 5:39:04.723082
train attack total time: 46.381s
train attack init time: 37.255s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.319s


train attack loss increase over inner max: -2.215
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.12%
mean, std amount infeasible at boundary: 0.00 +/- 0.03
max amount infeasible at boundary: 1.13

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.778
total grad norm: 38.551

==================== evaluation at iteration: 526 ====================
train total loss: 80.201%
train max loss: 5.051%, reg loss: 78.265%
time spent training so far: 6:12:00.771748
train attack total time: 46.948s
train attack init time: 38.076s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.309s


train attack loss increase over inner max: -1.179
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.013
total grad norm: 76.156

==================== evaluation at iteration: 527 ====================
train total loss: 78.575%
train max loss: 0.478%, reg loss: 78.097%
time spent training so far: 6:12:40.405707
train attack total time: 39.146s
train attack init time: 28.245s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.399s


train attack loss increase over inner max: -1.501
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.003
total grad norm: 13.486

==================== evaluation at iteration: 528 ====================
train total loss: 78.366%
train max loss: 0.255%, reg loss: 78.111%
time spent training so far: 6:13:37.637751
train attack total time: 56.732s
train attack init time: 47.954s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.304s


train attack loss increase over inner max: 0.210
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.052
total grad norm: 30.362

==================== evaluation at iteration: 529 ====================
train total loss: 78.550%
train max loss: 0.425%, reg loss: 78.125%
time spent training so far: 6:14:34.172325
train attack total time: 56.023s
train attack init time: 45.850s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.364s


train attack loss increase over inner max: 0.633
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.908
total grad norm: 29.009

==================== evaluation at iteration: 530 ====================
train total loss: 82.990%
train max loss: 6.373%, reg loss: 78.504%
time spent training so far: 6:15:21.328688
train attack total time: 46.664s
train attack init time: 37.677s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.312s


train attack loss increase over inner max: 6.784
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.743
total grad norm: 31.036

==================== evaluation at iteration: 531 ====================
train total loss: 82.719%
train max loss: 8.420%, reg loss: 78.290%
time spent training so far: 6:16:02.228452
train attack total time: 40.274s
train attack init time: 29.088s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.411s


train attack loss increase over inner max: 2.791
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.370
total grad norm: 40.445

==================== evaluation at iteration: 532 ====================
train total loss: 84.270%
train max loss: 9.540%, reg loss: 78.194%
time spent training so far: 6:16:42.607012
train attack total time: 39.871s
train attack init time: 29.756s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.365s


train attack loss increase over inner max: 0.878
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.368
total grad norm: 24.411

==================== evaluation at iteration: 533 ====================
train total loss: 85.512%
train max loss: 19.172%, reg loss: 78.206%
time spent training so far: 6:17:12.957846
train attack total time: 29.871s
train attack init time: 22.739s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.229s


train attack loss increase over inner max: 10.311
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.598
total grad norm: 30.475

==================== evaluation at iteration: 534 ====================
train total loss: 86.338%
train max loss: 17.266%, reg loss: 78.324%
time spent training so far: 6:17:47.256547
train attack total time: 33.779s
train attack init time: 22.794s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.405s


train attack loss increase over inner max: -2.152
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.037
total grad norm: 24.126

==================== evaluation at iteration: 535 ====================
train total loss: 87.402%
train max loss: 17.212%, reg loss: 78.463%
time spent training so far: 6:18:12.181074
train attack total time: 24.434s
train attack init time: 17.025s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.242s


train attack loss increase over inner max: 2.767
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.011
total grad norm: 44.806

==================== evaluation at iteration: 536 ====================
train total loss: 91.291%
train max loss: 22.311%, reg loss: 78.588%
time spent training so far: 6:18:44.139810
train attack total time: 31.360s
train attack init time: 16.339s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.584s


train attack loss increase over inner max: 5.978
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.650
total grad norm: 35.684

==================== evaluation at iteration: 537 ====================
train total loss: 88.801%
train max loss: 18.733%, reg loss: 78.456%
time spent training so far: 6:19:10.052360
train attack total time: 25.422s
train attack init time: 18.367s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: 0.003
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.183
total grad norm: 60.626

==================== evaluation at iteration: 538 ====================
train total loss: 89.945%
train max loss: 18.751%, reg loss: 78.240%
time spent training so far: 6:19:40.936922
train attack total time: 30.375s
train attack init time: 19.561s
train attack avg grad step time: 0.099s
train attack avg reproj time: 0.390s


train attack loss increase over inner max: 4.626
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000

Max_n_steps: 20
Parameter containing:
tensor([[0.1732]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1730]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1728]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1725]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1722]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1718]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1715]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1712]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1710]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1707]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1704]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1702]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1700]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1699]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1699]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1700]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1703]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1707]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1712]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1717]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1722]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1727]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1731]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1735]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1739]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1742]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1745]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1749]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1755]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1760]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1766]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1771]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1776]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1779]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1782]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1785]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1786]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1788]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1789]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1788]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1789]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1790]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1793]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1799]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1805]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1814]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 4.987
total grad norm: 71.910

==================== evaluation at iteration: 539 ====================
train total loss: 87.724%
train max loss: 16.751%, reg loss: 78.448%
time spent training so far: 6:20:08.662412
train attack total time: 27.253s
train attack init time: 17.969s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.325s


train attack loss increase over inner max: -1.163
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.637
total grad norm: 32.854

==================== evaluation at iteration: 540 ====================
train total loss: 83.191%
train max loss: 10.193%, reg loss: 78.318%
time spent training so far: 6:20:45.776141
train attack total time: 36.617s
train attack init time: 21.011s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.611s


train attack loss increase over inner max: -0.183
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.484
total grad norm: 23.009

==================== evaluation at iteration: 541 ====================
train total loss: 84.040%
train max loss: 11.266%, reg loss: 78.217%
time spent training so far: 6:21:15.809588
train attack total time: 29.400s
train attack init time: 21.091s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.280s


train attack loss increase over inner max: -0.425
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.484
total grad norm: 26.962

==================== evaluation at iteration: 542 ====================
train total loss: 84.453%
train max loss: 11.882%, reg loss: 78.204%
time spent training so far: 6:21:41.450430
train attack total time: 25.124s
train attack init time: 19.746s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.145s


train attack loss increase over inner max: 0.302
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.284
total grad norm: 33.724

==================== evaluation at iteration: 543 ====================
train total loss: 85.501%
train max loss: 11.012%, reg loss: 78.632%
time spent training so far: 6:22:07.543222
train attack total time: 25.574s
train attack init time: 18.254s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.235s


train attack loss increase over inner max: -0.451
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.295
total grad norm: 37.606

==================== evaluation at iteration: 544 ====================
train total loss: 84.378%
train max loss: 11.270%, reg loss: 78.173%
time spent training so far: 6:22:37.938912
train attack total time: 29.864s
train attack init time: 22.344s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.244s


train attack loss increase over inner max: -0.085
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.178
total grad norm: 45.261

==================== evaluation at iteration: 545 ====================
train total loss: 85.082%
train max loss: 8.467%, reg loss: 78.084%
time spent training so far: 6:23:06.862592
train attack total time: 28.430s
train attack init time: 19.468s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.314s


train attack loss increase over inner max: -0.874
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.792
total grad norm: 42.454

==================== evaluation at iteration: 546 ====================
train total loss: 81.276%
train max loss: 4.949%, reg loss: 77.928%
time spent training so far: 6:23:45.351156
train attack total time: 37.926s
train attack init time: 29.922s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.266s


train attack loss increase over inner max: -1.327
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.466
total grad norm: 74.243

==================== evaluation at iteration: 547 ====================
train total loss: 78.860%
train max loss: 1.267%, reg loss: 78.172%
time spent training so far: 6:24:26.447657
train attack total time: 40.615s
train attack init time: 31.716s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.310s


train attack loss increase over inner max: -1.153
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.636
total grad norm: 36.930

==================== evaluation at iteration: 548 ====================
train total loss: 78.936%
train max loss: 1.642%, reg loss: 78.237%
time spent training so far: 6:25:01.449022
train attack total time: 34.497s
train attack init time: 25.819s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.300s


train attack loss increase over inner max: 2.234
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.890
total grad norm: 50.353

==================== evaluation at iteration: 549 ====================
train total loss: 78.888%
train max loss: 0.441%, reg loss: 78.447%
time spent training so far: 6:25:45.553746
train attack total time: 43.566s
train attack init time: 31.593s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.452s


train attack loss increase over inner max: 0.356
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.613
total grad norm: 19.235

==================== evaluation at iteration: 550 ====================
train total loss: 79.508%
train max loss: 1.714%, reg loss: 78.274%
time spent training so far: 6:26:27.682275
train attack total time: 41.659s
train attack init time: 34.527s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.232s


train attack loss increase over inner max: 2.261
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.76%
mean, std amount infeasible at boundary: 0.02 +/- 0.22
max amount infeasible at boundary: 6.16

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.325
total grad norm: 25.608

==================== evaluation at iteration: 551 ====================
train total loss: 80.940%
train max loss: 4.781%, reg loss: 78.347%
time spent training so far: 6:53:05.162354
train attack total time: 41.471s
train attack init time: 33.237s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.280s


train attack loss increase over inner max: 2.400
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.841
total grad norm: 25.704

==================== evaluation at iteration: 552 ====================
train total loss: 81.707%
train max loss: 5.085%, reg loss: 78.406%
time spent training so far: 6:53:43.274096
train attack total time: 37.621s
train attack init time: 28.831s
train attack avg grad step time: 0.099s
train attack avg reproj time: 0.297s


train attack loss increase over inner max: 0.184
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.391
total grad norm: 24.565

==================== evaluation at iteration: 553 ====================
train total loss: 82.850%
train max loss: 9.679%, reg loss: 78.716%
time spent training so far: 6:54:15.848578
train attack total time: 32.058s
train attack init time: 22.795s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.326s


train attack loss increase over inner max: 4.982
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.090
total grad norm: 19.231

==================== evaluation at iteration: 554 ====================
train total loss: 82.860%
train max loss: 9.905%, reg loss: 78.459%
time spent training so far: 6:54:46.310627
train attack total time: 30.008s
train attack init time: 22.392s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.251s


train attack loss increase over inner max: 0.001
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.040
total grad norm: 36.860

==================== evaluation at iteration: 555 ====================
train total loss: 86.346%
train max loss: 12.706%, reg loss: 78.427%
time spent training so far: 6:55:14.885057
train attack total time: 28.102s
train attack init time: 21.228s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.217s


train attack loss increase over inner max: 2.415
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.746
total grad norm: 26.718

==================== evaluation at iteration: 556 ====================
train total loss: 84.781%
train max loss: 12.659%, reg loss: 78.439%
time spent training so far: 6:55:46.995180
train attack total time: 31.555s
train attack init time: 25.659s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.170s


train attack loss increase over inner max: 0.891
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.231
total grad norm: 38.150

==================== evaluation at iteration: 557 ====================
train total loss: 82.941%
train max loss: 11.116%, reg loss: 78.185%
time spent training so far: 6:56:16.138567
train attack total time: 28.649s
train attack init time: 22.091s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.200s


train attack loss increase over inner max: 0.820
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.978
total grad norm: 45.756

==================== evaluation at iteration: 558 ====================
train total loss: 81.391%
train max loss: 7.375%, reg loss: 78.457%
time spent training so far: 6:56:48.161621
train attack total time: 31.526s
train attack init time: 24.881s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: -0.001
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.899
total grad norm: 22.008

==================== evaluation at iteration: 559 ====================
train total loss: 80.183%
train max loss: 2.606%, reg loss: 78.395%
time spent training so far: 6:57:26.665181
train attack total time: 38.019s
train attack init time: 29.971s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.270s


train attack loss increase over inner max: 0.719
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.303
total grad norm: 23.756

==================== evaluation at iteration: 560 ====================
train total loss: 78.815%
train max loss: 0.859%, reg loss: 78.266%
time spent training so far: 6:58:18.881525
train attack total time: 51.774s
train attack init time: 38.142s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.527s


train attack loss increase over inner max: -1.129
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.088
total grad norm: 25.493

==================== evaluation at iteration: 561 ====================
train total loss: 78.577%
train max loss: 0.843%, reg loss: 78.117%
time spent training so far: 6:59:03.540863
train attack total time: 44.090s
train attack init time: 35.019s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.314s


train attack loss increase over inner max: 0.279
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.915
total grad norm: 40.615

==================== evaluation at iteration: 562 ====================
train total loss: 78.717%
train max loss: 0.349%, reg loss: 78.368%
time spent training so far: 6:59:55.140765
train attack total time: 51.092s
train attack init time: 41.303s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.350s


train attack loss increase over inner max: -0.409
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.205
total grad norm: 47.015

==================== evaluation at iteration: 563 ====================
train total loss: 78.890%
train max loss: 1.051%, reg loss: 78.225%
time spent training so far: 7:00:41.396891
train attack total time: 45.745s
train attack init time: 37.014s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.301s


train attack loss increase over inner max: 0.555
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.554
total grad norm: 35.082

==================== evaluation at iteration: 564 ====================
train total loss: 78.714%
train max loss: 0.738%, reg loss: 78.158%
time spent training so far: 7:01:31.096799
train attack total time: 49.204s
train attack init time: 35.694s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.522s


train attack loss increase over inner max: 2.253
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.709
total grad norm: 4.709

==================== evaluation at iteration: 565 ====================
train total loss: 78.405%
train max loss: -0.084%, reg loss: 78.405%
time spent training so far: 7:02:18.083514
train attack total time: 46.470s
train attack init time: 38.466s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.268s


train attack loss increase over inner max: -0.061
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.975
total grad norm: 29.733

==================== evaluation at iteration: 566 ====================
train total loss: 79.575%
train max loss: 1.574%, reg loss: 78.467%
time spent training so far: 7:02:54.925390
train attack total time: 36.275s
train attack init time: 30.066s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.185s


train attack loss increase over inner max: 2.196
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.699
total grad norm: 29.614

==================== evaluation at iteration: 567 ====================
train total loss: 79.219%
train max loss: 1.627%, reg loss: 78.228%
time spent training so far: 7:03:29.696716
train attack total time: 34.304s
train attack init time: 26.158s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.273s


train attack loss increase over inner max: 1.067
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.672
total grad norm: 21.797

==================== evaluation at iteration: 568 ====================
train total loss: 79.781%
train max loss: 4.513%, reg loss: 78.258%
time spent training so far: 7:04:08.504673
train attack total time: 38.305s
train attack init time: 30.807s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.244s


train attack loss increase over inner max: 2.775
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.622
total grad norm: 20.945

==================== evaluation at iteration: 569 ====================
train total loss: 80.933%
train max loss: 7.518%, reg loss: 78.171%
time spent training so far: 7:04:41.724423
train attack total time: 32.689s
train attack init time: 27.317s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.148s


train attack loss increase over inner max: 2.842
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.124
total grad norm: 33.373

==================== evaluation at iteration: 570 ====================
train total loss: 81.683%
train max loss: 6.860%, reg loss: 78.432%
time spent training so far: 7:05:21.186127
train attack total time: 38.985s
train attack init time: 23.542s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.609s


train attack loss increase over inner max: 0.009
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.851
total grad norm: 18.639

==================== evaluation at iteration: 571 ====================
train total loss: 81.073%
train max loss: 6.526%, reg loss: 78.370%
time spent training so far: 7:05:54.409538
train attack total time: 32.666s
train attack init time: 24.409s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.279s


train attack loss increase over inner max: 0.707
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.185
total grad norm: 33.485

==================== evaluation at iteration: 572 ====================
train total loss: 81.774%
train max loss: 8.482%, reg loss: 77.899%
time spent training so far: 7:06:29.415768
train attack total time: 34.462s
train attack init time: 24.444s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.356s


train attack loss increase over inner max: 3.467
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.289
total grad norm: 25.082

==================== evaluation at iteration: 573 ====================
train total loss: 80.053%
train max loss: 4.122%, reg loss: 78.189%
time spent training so far: 7:07:04.988523
train attack total time: 35.063s
train attack init time: 25.407s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.344s


train attack loss increase over inner max: -2.492
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.640
total grad norm: 29.267

==================== evaluation at iteration: 574 ====================
train total loss: 79.028%
train max loss: 1.207%, reg loss: 78.164%
time spent training so far: 7:07:44.300307
train attack total time: 38.791s
train attack init time: 31.999s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.216s


train attack loss increase over inner max: -0.764
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.913
total grad norm: 53.452

==================== evaluation at iteration: 575 ====================
train total loss: 79.683%
train max loss: 2.421%, reg loss: 78.321%
time spent training so far: 7:08:29.707344
train attack total time: 44.908s
train attack init time: 34.544s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.374s


train attack loss increase over inner max: 0.979
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.32%
mean, std amount infeasible at boundary: 0.00 +/- 0.08
max amount infeasible at boundary: 3.74

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.519
total grad norm: 22.123

==================== evaluation at iteration: 576 ====================
train total loss: 78.931%
train max loss: 1.416%, reg loss: 78.119%
time spent training so far: 7:38:09.578710
train attack total time: 37.414s
train attack init time: 28.612s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.303s


train attack loss increase over inner max: 1.932
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.064
total grad norm: 21.585

==================== evaluation at iteration: 577 ====================
train total loss: 79.192%
train max loss: 2.413%, reg loss: 78.008%
time spent training so far: 7:38:49.394037
train attack total time: 39.337s
train attack init time: 32.169s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.229s


train attack loss increase over inner max: 1.919
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.091
total grad norm: 52.418

==================== evaluation at iteration: 578 ====================
train total loss: 83.386%
train max loss: 11.420%, reg loss: 77.975%
time spent training so far: 7:39:42.642098
train attack total time: 52.778s
train attack init time: 44.568s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.278s


train attack loss increase over inner max: 9.886
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.891
total grad norm: 61.440

==================== evaluation at iteration: 579 ====================
train total loss: 81.278%
train max loss: 5.758%, reg loss: 77.868%
time spent training so far: 7:40:29.781712
train attack total time: 46.647s
train attack init time: 38.328s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.282s


train attack loss increase over inner max: -4.677
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.485
total grad norm: 39.008

==================== evaluation at iteration: 580 ====================
train total loss: 79.681%
train max loss: 4.657%, reg loss: 78.076%
time spent training so far: 7:41:32.498166
train attack total time: 62.176s
train attack init time: 46.785s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.604s


train attack loss increase over inner max: 2.435
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.121
total grad norm: 20.184

==================== evaluation at iteration: 581 ====================
train total loss: 78.829%
train max loss: 1.372%, reg loss: 77.973%
time spent training so far: 7:42:15.691467
train attack total time: 42.594s
train attack init time: 34.142s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.288s


train attack loss increase over inner max: 1.394
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.411
total grad norm: 51.731

==================== evaluation at iteration: 582 ====================
train total loss: 78.804%
train max loss: 1.054%, reg loss: 78.099%
time spent training so far: 7:43:07.037699
train attack total time: 50.828s
train attack init time: 42.374s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.284s


train attack loss increase over inner max: -0.569
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.629
total grad norm: 38.253

==================== evaluation at iteration: 583 ====================
train total loss: 78.341%
train max loss: 0.117%, reg loss: 78.224%
time spent training so far: 7:43:59.401056
train attack total time: 51.884s
train attack init time: 38.887s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.492s


train attack loss increase over inner max: 0.892
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1824]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1833]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1842]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1850]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1856]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1861]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1866]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1871]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1875]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1877]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1879]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1880]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1879]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1879]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1878]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1876]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1873]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1870]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1866]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1863]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1859]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1855]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1851]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1847]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1843]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1839]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1835]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1832]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1828]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1823]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1819]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1815]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1811]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1807]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1804]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1800]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1796]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1793]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1789]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1788]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1792]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1798]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1804]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1808]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1812]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 4.447
total grad norm: 57.245

==================== evaluation at iteration: 584 ====================
train total loss: 78.622%
train max loss: 0.593%, reg loss: 78.029%
time spent training so far: 7:44:42.462744
train attack total time: 42.580s
train attack init time: 34.126s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.286s


train attack loss increase over inner max: 2.858
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.151
total grad norm: 18.007

==================== evaluation at iteration: 585 ====================
train total loss: 78.455%
train max loss: 0.820%, reg loss: 77.974%
time spent training so far: 7:45:22.121072
train attack total time: 39.166s
train attack init time: 32.087s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.228s


train attack loss increase over inner max: 1.681
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.397
total grad norm: 23.088

==================== evaluation at iteration: 586 ====================
train total loss: 78.522%
train max loss: 0.831%, reg loss: 78.106%
time spent training so far: 7:46:07.771708
train attack total time: 45.101s
train attack init time: 37.085s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: 0.242
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 3.986
total grad norm: 24.649

==================== evaluation at iteration: 587 ====================
train total loss: 81.435%
train max loss: 10.008%, reg loss: 77.961%
time spent training so far: 7:46:56.345024
train attack total time: 48.019s
train attack init time: 38.993s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.310s


train attack loss increase over inner max: 4.459
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.384
total grad norm: 23.146

==================== evaluation at iteration: 588 ====================
train total loss: 81.842%
train max loss: 7.530%, reg loss: 77.977%
time spent training so far: 7:47:37.374454
train attack total time: 40.476s
train attack init time: 30.700s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.348s


train attack loss increase over inner max: -2.880
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.193
total grad norm: 33.656

==================== evaluation at iteration: 589 ====================
train total loss: 81.905%
train max loss: 8.423%, reg loss: 77.932%
time spent training so far: 7:48:08.592574
train attack total time: 30.710s
train attack init time: 22.588s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.273s


train attack loss increase over inner max: 0.330
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.861
total grad norm: 21.551

==================== evaluation at iteration: 590 ====================
train total loss: 82.941%
train max loss: 9.477%, reg loss: 78.227%
time spent training so far: 7:48:35.791644
train attack total time: 26.725s
train attack init time: 17.795s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.310s


train attack loss increase over inner max: 0.639
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.845
total grad norm: 27.806

==================== evaluation at iteration: 591 ====================
train total loss: 82.967%
train max loss: 10.039%, reg loss: 78.159%
time spent training so far: 7:49:00.352004
train attack total time: 23.938s
train attack init time: 16.006s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.262s


train attack loss increase over inner max: 1.470
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.875
total grad norm: 31.006

==================== evaluation at iteration: 592 ====================
train total loss: 82.933%
train max loss: 10.589%, reg loss: 78.135%
time spent training so far: 7:49:30.485937
train attack total time: 29.666s
train attack init time: 19.718s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.351s


train attack loss increase over inner max: 1.676
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.862
total grad norm: 45.062

==================== evaluation at iteration: 593 ====================
train total loss: 83.996%
train max loss: 10.942%, reg loss: 78.129%
time spent training so far: 7:50:02.277544
train attack total time: 31.335s
train attack init time: 23.629s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.255s


train attack loss increase over inner max: 2.258
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.515
total grad norm: 33.957

==================== evaluation at iteration: 594 ====================
train total loss: 82.422%
train max loss: 7.518%, reg loss: 78.090%
time spent training so far: 7:50:43.619984
train attack total time: 40.759s
train attack init time: 28.631s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.455s


train attack loss increase over inner max: 0.577
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.684
total grad norm: 48.265

==================== evaluation at iteration: 595 ====================
train total loss: 82.265%
train max loss: 6.192%, reg loss: 78.177%
time spent training so far: 7:51:29.491868
train attack total time: 45.377s
train attack init time: 34.675s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.392s


train attack loss increase over inner max: 0.027
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.459
total grad norm: 33.679

==================== evaluation at iteration: 596 ====================
train total loss: 80.261%
train max loss: 2.968%, reg loss: 78.221%
time spent training so far: 7:52:15.333603
train attack total time: 45.260s
train attack init time: 36.512s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.301s


train attack loss increase over inner max: -0.939
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.304
total grad norm: 26.155

==================== evaluation at iteration: 597 ====================
train total loss: 79.365%
train max loss: 1.784%, reg loss: 78.145%
time spent training so far: 7:53:05.380429
train attack total time: 49.543s
train attack init time: 41.382s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.274s


train attack loss increase over inner max: -1.565
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.385
total grad norm: 51.275

==================== evaluation at iteration: 598 ====================
train total loss: 78.277%
train max loss: 0.275%, reg loss: 78.134%
time spent training so far: 7:53:58.642929
train attack total time: 52.751s
train attack init time: 43.766s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.310s


train attack loss increase over inner max: -1.775
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.443
total grad norm: 39.648

==================== evaluation at iteration: 599 ====================
train total loss: 79.593%
train max loss: 2.340%, reg loss: 78.269%
time spent training so far: 7:54:50.907834
train attack total time: 51.779s
train attack init time: 44.242s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.250s


train attack loss increase over inner max: 2.649
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.977
total grad norm: 15.777

==================== evaluation at iteration: 600 ====================
train total loss: 79.311%
train max loss: 1.659%, reg loss: 78.427%
time spent training so far: 7:55:38.887170
train attack total time: 47.475s
train attack init time: 38.436s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.319s


train attack loss increase over inner max: -0.255
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.08%
mean, std amount infeasible at boundary: 0.00 +/- 0.02
max amount infeasible at boundary: 0.81

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.819
total grad norm: 35.667

==================== evaluation at iteration: 601 ====================
train total loss: 79.927%
train max loss: 1.692%, reg loss: 78.313%
time spent training so far: 8:33:27.755003
train attack total time: 52.966s
train attack init time: 45.096s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.262s


train attack loss increase over inner max: 1.706
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.315
total grad norm: 30.080

==================== evaluation at iteration: 602 ====================
train total loss: 79.117%
train max loss: 1.033%, reg loss: 78.257%
time spent training so far: 8:34:17.016030
train attack total time: 48.732s
train attack init time: 39.868s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.307s


train attack loss increase over inner max: 1.082
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.511
total grad norm: 36.422

==================== evaluation at iteration: 603 ====================
train total loss: 78.821%
train max loss: 0.670%, reg loss: 78.283%
time spent training so far: 8:35:08.339162
train attack total time: 50.828s
train attack init time: 35.231s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.614s


train attack loss increase over inner max: 1.297
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.603
total grad norm: 41.397

==================== evaluation at iteration: 604 ====================
train total loss: 79.502%
train max loss: 1.920%, reg loss: 78.251%
time spent training so far: 8:35:57.582950
train attack total time: 48.730s
train attack init time: 41.913s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.216s


train attack loss increase over inner max: 0.893
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.061
total grad norm: 23.195

==================== evaluation at iteration: 605 ====================
train total loss: 79.636%
train max loss: 2.219%, reg loss: 78.032%
time spent training so far: 8:36:36.920455
train attack total time: 38.850s
train attack init time: 29.060s
train attack avg grad step time: 0.098s
train attack avg reproj time: 0.344s


train attack loss increase over inner max: 0.130
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.883
total grad norm: 37.364

==================== evaluation at iteration: 606 ====================
train total loss: 81.888%
train max loss: 7.857%, reg loss: 78.295%
time spent training so far: 8:37:15.974475
train attack total time: 38.488s
train attack init time: 28.625s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.352s


train attack loss increase over inner max: 5.799
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.839
total grad norm: 33.378

==================== evaluation at iteration: 607 ====================
train total loss: 81.945%
train max loss: 7.027%, reg loss: 78.471%
time spent training so far: 8:37:52.226255
train attack total time: 35.792s
train attack init time: 28.624s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.228s


train attack loss increase over inner max: -0.365
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.301
total grad norm: 36.808

==================== evaluation at iteration: 608 ====================
train total loss: 81.567%
train max loss: 7.787%, reg loss: 78.138%
time spent training so far: 8:38:26.419005
train attack total time: 33.645s
train attack init time: 26.828s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.214s


train attack loss increase over inner max: 1.090
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.176
total grad norm: 105.692

==================== evaluation at iteration: 609 ====================
train total loss: 86.358%
train max loss: 15.228%, reg loss: 78.576%
time spent training so far: 8:39:04.809093
train attack total time: 37.889s
train attack init time: 29.254s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.294s


train attack loss increase over inner max: 10.644
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.213
total grad norm: 273.678

==================== evaluation at iteration: 610 ====================
train total loss: 84.276%
train max loss: 6.016%, reg loss: 78.261%
time spent training so far: 8:39:58.992136
train attack total time: 53.681s
train attack init time: 43.354s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.374s


train attack loss increase over inner max: 3.143
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.885
total grad norm: 46.979

==================== evaluation at iteration: 611 ====================
train total loss: 81.209%
train max loss: 5.485%, reg loss: 78.488%
time spent training so far: 8:40:48.926252
train attack total time: 49.321s
train attack init time: 38.790s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.381s


train attack loss increase over inner max: 5.929
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.055
total grad norm: 26.898

==================== evaluation at iteration: 612 ====================
train total loss: 81.497%
train max loss: 5.261%, reg loss: 78.534%
time spent training so far: 8:41:31.332651
train attack total time: 41.881s
train attack init time: 31.759s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.362s


train attack loss increase over inner max: 1.074
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.064
total grad norm: 39.503

==================== evaluation at iteration: 613 ====================
train total loss: 82.967%
train max loss: 10.642%, reg loss: 78.618%
time spent training so far: 8:42:04.774395
train attack total time: 33.006s
train attack init time: 17.976s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.587s


train attack loss increase over inner max: 5.971
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.473
total grad norm: 25.201

==================== evaluation at iteration: 614 ====================
train total loss: 83.617%
train max loss: 14.107%, reg loss: 78.308%
time spent training so far: 8:42:31.308594
train attack total time: 26.045s
train attack init time: 18.573s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.243s


train attack loss increase over inner max: 2.138
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.087
total grad norm: 45.022

==================== evaluation at iteration: 615 ====================
train total loss: 86.217%
train max loss: 14.170%, reg loss: 78.684%
time spent training so far: 8:43:00.439595
train attack total time: 28.638s
train attack init time: 18.206s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.376s


train attack loss increase over inner max: -0.374
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.313
total grad norm: 38.398

==================== evaluation at iteration: 616 ====================
train total loss: 83.790%
train max loss: 12.013%, reg loss: 78.405%
time spent training so far: 8:43:28.472293
train attack total time: 27.462s
train attack init time: 19.338s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.275s


train attack loss increase over inner max: -1.388
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.221
total grad norm: 38.545

==================== evaluation at iteration: 617 ====================
train total loss: 85.570%
train max loss: 11.914%, reg loss: 78.709%
time spent training so far: 8:44:01.024560
train attack total time: 32.081s
train attack init time: 20.501s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.428s


train attack loss increase over inner max: -1.111
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.135
total grad norm: 46.279

==================== evaluation at iteration: 618 ====================
train total loss: 82.373%
train max loss: 7.270%, reg loss: 78.606%
time spent training so far: 8:44:32.571384
train attack total time: 31.113s
train attack init time: 20.542s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.384s


train attack loss increase over inner max: -1.030
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.236
total grad norm: 35.851

==================== evaluation at iteration: 619 ====================
train total loss: 80.442%
train max loss: 2.715%, reg loss: 78.920%
time spent training so far: 8:45:26.735703
train attack total time: 53.678s
train attack init time: 44.076s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.341s


train attack loss increase over inner max: -1.810
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.833
total grad norm: 21.008

==================== evaluation at iteration: 620 ====================
train total loss: 82.854%
train max loss: 7.886%, reg loss: 79.132%
time spent training so far: 8:46:23.585311
train attack total time: 56.356s
train attack init time: 42.376s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.540s


train attack loss increase over inner max: 7.027
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.690
total grad norm: 44.768

==================== evaluation at iteration: 621 ====================
train total loss: 82.570%
train max loss: 6.681%, reg loss: 79.167%
time spent training so far: 8:47:11.019297
train attack total time: 46.812s
train attack init time: 27.943s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.763s


train attack loss increase over inner max: -0.393
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.940
total grad norm: 28.455

==================== evaluation at iteration: 622 ====================
train total loss: 80.102%
train max loss: 3.136%, reg loss: 78.751%
time spent training so far: 8:48:00.836738
train attack total time: 49.351s
train attack init time: 41.469s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.260s


train attack loss increase over inner max: -1.149
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.308
total grad norm: 5.308

==================== evaluation at iteration: 623 ====================
train total loss: 78.926%
train max loss: -0.079%, reg loss: 78.926%
time spent training so far: 8:48:46.284024
train attack total time: 44.933s
train attack init time: 25.911s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.768s


train attack loss increase over inner max: -1.136
OOM debug. Mem allocated and reserved: 1092096.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.213
total grad norm: 22.070

==================== evaluation at iteration: 624 ====================
train total loss: 79.407%
train max loss: 0.704%, reg loss: 78.969%
time spent training so far: 8:49:25.313811
train attack total time: 38.526s
train attack init time: 29.232s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.326s


train attack loss increase over inner max: 0.593
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.660
total grad norm: 24.716

==================== evaluation at iteration: 625 ====================
train total loss: 80.679%
train max loss: 3.657%, reg loss: 79.237%
time spent training so far: 8:50:02.978548
train attack total time: 37.160s
train attack init time: 29.545s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.249s


train attack loss increase over inner max: 2.972
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 1.20%
mean, std amount infeasible at boundary: 0.02 +/- 0.21
max amount infeasible at boundary: 4.53

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.798
total grad norm: 33.815

==================== evaluation at iteration: 626 ====================
train total loss: 81.709%
train max loss: 4.111%, reg loss: 78.814%
time spent training so far: 9:10:50.882657
train attack total time: 27.984s
train attack init time: 20.461s
train attack avg grad step time: 0.098s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 1.683
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.913
total grad norm: 22.947

==================== evaluation at iteration: 627 ====================
train total loss: 82.102%
train max loss: 8.318%, reg loss: 79.267%
time spent training so far: 9:11:14.962709
train attack total time: 23.545s
train attack init time: 17.696s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.173s


train attack loss increase over inner max: 4.312
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.246
total grad norm: 20.419

==================== evaluation at iteration: 628 ====================
train total loss: 83.025%
train max loss: 10.012%, reg loss: 78.879%
time spent training so far: 9:11:44.186947
train attack total time: 28.755s
train attack init time: 21.756s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.222s


train attack loss increase over inner max: 1.207
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.377
total grad norm: 26.044

==================== evaluation at iteration: 629 ====================
train total loss: 82.710%
train max loss: 9.302%, reg loss: 79.476%
time spent training so far: 9:12:08.095259
train attack total time: 23.378s
train attack init time: 14.509s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.306s


train attack loss increase over inner max: 1.381
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000

Max_n_steps: 20
Parameter containing:
tensor([[0.1814]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1816]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1817]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1818]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1819]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1821]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1823]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1825]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1828]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1831]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1835]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1838]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1841]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1843]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1844]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1844]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1844]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1843]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1842]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1840]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1838]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1836]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1835]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1835]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1835]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1837]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1845]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1853]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1860]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1867]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1873]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1878]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1883]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1887]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1891]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1895]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1899]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1901]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1903]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1905]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1905]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1906]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1907]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1908]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1908]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1908]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 5.031
total grad norm: 22.845

==================== evaluation at iteration: 630 ====================
train total loss: 82.622%
train max loss: 8.699%, reg loss: 78.917%
time spent training so far: 9:12:29.783869
train attack total time: 21.213s
train attack init time: 13.840s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.239s


train attack loss increase over inner max: 0.911
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.183
total grad norm: 33.160

==================== evaluation at iteration: 631 ====================
train total loss: 81.997%
train max loss: 6.715%, reg loss: 78.882%
time spent training so far: 9:12:55.813868
train attack total time: 25.435s
train attack init time: 12.615s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.486s


train attack loss increase over inner max: 0.472
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.312
total grad norm: 23.747

==================== evaluation at iteration: 632 ====================
train total loss: 83.536%
train max loss: 12.115%, reg loss: 79.018%
time spent training so far: 9:13:18.856839
train attack total time: 22.550s
train attack init time: 15.030s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.243s


train attack loss increase over inner max: 4.439
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.581
total grad norm: 20.740

==================== evaluation at iteration: 633 ====================
train total loss: 86.296%
train max loss: 12.656%, reg loss: 79.115%
time spent training so far: 9:13:38.169115
train attack total time: 18.835s
train attack init time: 12.548s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.187s


train attack loss increase over inner max: 1.225
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.925
total grad norm: 30.677

==================== evaluation at iteration: 634 ====================
train total loss: 84.873%
train max loss: 12.876%, reg loss: 79.242%
time spent training so far: 9:13:56.963052
train attack total time: 18.284s
train attack init time: 11.471s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: 0.278
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.549
total grad norm: 33.174

==================== evaluation at iteration: 635 ====================
train total loss: 84.855%
train max loss: 11.473%, reg loss: 79.474%
time spent training so far: 9:14:22.620694
train attack total time: 25.191s
train attack init time: 16.861s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.279s


train attack loss increase over inner max: -0.276
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.319
total grad norm: 37.832

==================== evaluation at iteration: 636 ====================
train total loss: 82.714%
train max loss: 8.990%, reg loss: 78.898%
time spent training so far: 9:14:47.637645
train attack total time: 24.460s
train attack init time: 17.022s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 0.017
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.761
total grad norm: 25.392

==================== evaluation at iteration: 637 ====================
train total loss: 81.951%
train max loss: 8.626%, reg loss: 78.716%
time spent training so far: 9:15:22.528275
train attack total time: 34.413s
train attack init time: 24.544s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.352s


train attack loss increase over inner max: 3.118
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.385
total grad norm: 21.455

==================== evaluation at iteration: 638 ====================
train total loss: 81.137%
train max loss: 5.462%, reg loss: 78.950%
time spent training so far: 9:15:56.390470
train attack total time: 33.339s
train attack init time: 25.716s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.247s


train attack loss increase over inner max: 0.190
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.578
total grad norm: 21.069

==================== evaluation at iteration: 639 ====================
train total loss: 80.670%
train max loss: 3.765%, reg loss: 79.076%
time spent training so far: 9:16:39.222356
train attack total time: 42.313s
train attack init time: 31.875s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.376s


train attack loss increase over inner max: -0.223
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.670
total grad norm: 23.221

==================== evaluation at iteration: 640 ====================
train total loss: 79.663%
train max loss: 1.023%, reg loss: 79.080%
time spent training so far: 9:17:24.178811
train attack total time: 44.487s
train attack init time: 33.785s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.388s


train attack loss increase over inner max: -0.860
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.849
total grad norm: 24.694

==================== evaluation at iteration: 641 ====================
train total loss: 80.305%
train max loss: 2.449%, reg loss: 78.793%
time spent training so far: 9:18:09.929136
train attack total time: 45.085s
train attack init time: 36.981s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.269s


train attack loss increase over inner max: 2.435
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.343
total grad norm: 27.378

==================== evaluation at iteration: 642 ====================
train total loss: 79.518%
train max loss: 0.505%, reg loss: 79.013%
time spent training so far: 9:19:16.280658
train attack total time: 65.881s
train attack init time: 37.918s
train attack avg grad step time: 0.093s
train attack avg reproj time: 1.174s


train attack loss increase over inner max: -0.973
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.399
total grad norm: 66.467

==================== evaluation at iteration: 643 ====================
train total loss: 79.316%
train max loss: 0.386%, reg loss: 78.930%
time spent training so far: 9:20:08.402752
train attack total time: 51.627s
train attack init time: 42.385s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: -0.058
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.677
total grad norm: 28.034

==================== evaluation at iteration: 644 ====================
train total loss: 79.753%
train max loss: 2.048%, reg loss: 78.666%
time spent training so far: 9:21:01.014184
train attack total time: 52.177s
train attack init time: 40.106s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.455s


train attack loss increase over inner max: 1.473
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.271
total grad norm: 28.309

==================== evaluation at iteration: 645 ====================
train total loss: 80.026%
train max loss: 1.278%, reg loss: 78.912%
time spent training so far: 9:21:54.712995
train attack total time: 53.233s
train attack init time: 42.783s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.381s


train attack loss increase over inner max: 0.574
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.789
total grad norm: 47.926

==================== evaluation at iteration: 646 ====================
train total loss: 78.855%
train max loss: 0.185%, reg loss: 78.670%
time spent training so far: 9:22:53.156142
train attack total time: 57.872s
train attack init time: 46.555s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.416s


train attack loss increase over inner max: -1.048
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.358
total grad norm: 62.850

==================== evaluation at iteration: 647 ====================
train total loss: 78.863%
train max loss: 0.082%, reg loss: 78.781%
time spent training so far: 9:23:42.067075
train attack total time: 48.416s
train attack init time: 37.584s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.396s


train attack loss increase over inner max: -0.276
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.771
total grad norm: 37.724

==================== evaluation at iteration: 648 ====================
train total loss: 79.086%
train max loss: 0.620%, reg loss: 78.490%
time spent training so far: 9:24:42.090356
train attack total time: 59.539s
train attack init time: 50.298s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: 1.113
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.072
total grad norm: 57.166

==================== evaluation at iteration: 649 ====================
train total loss: 85.567%
train max loss: 8.934%, reg loss: 78.652%
time spent training so far: 9:25:26.993012
train attack total time: 44.460s
train attack init time: 29.605s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.579s


train attack loss increase over inner max: 9.275
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.524
total grad norm: 43.542

==================== evaluation at iteration: 650 ====================
train total loss: 82.395%
train max loss: 6.611%, reg loss: 78.470%
time spent training so far: 9:26:12.347885
train attack total time: 44.909s
train attack init time: 30.709s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.553s


train attack loss increase over inner max: -2.657
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 1.28%
mean, std amount infeasible at boundary: 0.03 +/- 0.38
max amount infeasible at boundary: 8.03

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.914
total grad norm: 63.374

==================== evaluation at iteration: 651 ====================
train total loss: 85.971%
train max loss: 10.011%, reg loss: 78.626%
time spent training so far: 10:00:58.092900
train attack total time: 41.776s
train attack init time: 31.545s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.368s


train attack loss increase over inner max: 3.578
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.030
total grad norm: 115.716

==================== evaluation at iteration: 652 ====================
train total loss: 84.065%
train max loss: 7.638%, reg loss: 78.558%
time spent training so far: 10:01:41.935720
train attack total time: 43.382s
train attack init time: 33.302s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.360s


train attack loss increase over inner max: -0.040
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.105
total grad norm: 72.577

==================== evaluation at iteration: 653 ====================
train total loss: 79.634%
train max loss: 0.842%, reg loss: 78.792%
time spent training so far: 10:02:33.703349
train attack total time: 51.223s
train attack init time: 39.166s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.448s


train attack loss increase over inner max: -0.623
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.720
total grad norm: 4.720

==================== evaluation at iteration: 654 ====================
train total loss: 78.329%
train max loss: -0.799%, reg loss: 78.329%
time spent training so far: 10:03:40.884167
train attack total time: 66.742s
train attack init time: 54.393s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.466s


train attack loss increase over inner max: 2.834
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.862
total grad norm: 4.862

==================== evaluation at iteration: 655 ====================
train total loss: 78.538%
train max loss: -0.439%, reg loss: 78.538%
time spent training so far: 10:04:48.056028
train attack total time: 66.717s
train attack init time: 53.680s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.495s


train attack loss increase over inner max: -0.018
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.136
total grad norm: 5.136

==================== evaluation at iteration: 656 ====================
train total loss: 78.581%
train max loss: -0.479%, reg loss: 78.581%
time spent training so far: 10:06:07.217442
train attack total time: 78.602s
train attack init time: 66.093s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.473s


train attack loss increase over inner max: -0.517
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.990
total grad norm: 27.606

==================== evaluation at iteration: 657 ====================
train total loss: 78.781%
train max loss: 0.324%, reg loss: 78.456%
time spent training so far: 10:07:10.670957
train attack total time: 62.963s
train attack init time: 50.255s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.479s


train attack loss increase over inner max: 0.783
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.800
total grad norm: 41.577

==================== evaluation at iteration: 658 ====================
train total loss: 79.015%
train max loss: 0.631%, reg loss: 78.474%
time spent training so far: 10:08:10.394909
train attack total time: 59.197s
train attack init time: 50.120s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.317s


train attack loss increase over inner max: 1.046
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.614
total grad norm: 39.311

==================== evaluation at iteration: 659 ====================
train total loss: 79.605%
train max loss: 2.354%, reg loss: 78.677%
time spent training so far: 10:09:09.111984
train attack total time: 58.256s
train attack init time: 49.356s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.305s


train attack loss increase over inner max: 1.497
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.257
total grad norm: 33.000

==================== evaluation at iteration: 660 ====================
train total loss: 79.660%
train max loss: 2.477%, reg loss: 78.619%
time spent training so far: 10:10:02.127298
train attack total time: 52.554s
train attack init time: 42.592s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.356s


train attack loss increase over inner max: 1.056
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.155
total grad norm: 40.019

==================== evaluation at iteration: 661 ====================
train total loss: 80.984%
train max loss: 2.666%, reg loss: 78.474%
time spent training so far: 10:10:49.729929
train attack total time: 47.020s
train attack init time: 37.321s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.343s


train attack loss increase over inner max: -0.254
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.139
total grad norm: 25.897

==================== evaluation at iteration: 662 ====================
train total loss: 80.214%
train max loss: 3.142%, reg loss: 78.480%
time spent training so far: 10:11:38.253042
train attack total time: 47.983s
train attack init time: 38.523s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: 0.769
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.062
total grad norm: 41.387

==================== evaluation at iteration: 663 ====================
train total loss: 80.899%
train max loss: 5.934%, reg loss: 78.434%
time spent training so far: 10:12:26.646513
train attack total time: 47.884s
train attack init time: 38.682s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.323s


train attack loss increase over inner max: 2.253
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.060
total grad norm: 57.171

==================== evaluation at iteration: 664 ====================
train total loss: 81.465%
train max loss: 7.001%, reg loss: 78.372%
time spent training so far: 10:13:04.628730
train attack total time: 37.477s
train attack init time: 24.742s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.481s


train attack loss increase over inner max: 1.795
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.710
total grad norm: 45.199

==================== evaluation at iteration: 665 ====================
train total loss: 80.892%
train max loss: 7.299%, reg loss: 78.317%
time spent training so far: 10:13:41.238135
train attack total time: 36.132s
train attack init time: 29.701s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.196s


train attack loss increase over inner max: 3.588
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.283
total grad norm: 34.218

==================== evaluation at iteration: 666 ====================
train total loss: 81.464%
train max loss: 10.470%, reg loss: 78.398%
time spent training so far: 10:14:21.622398
train attack total time: 39.738s
train attack init time: 27.613s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.455s


train attack loss increase over inner max: 5.651
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.218
total grad norm: 38.110

==================== evaluation at iteration: 667 ====================
train total loss: 82.109%
train max loss: 9.940%, reg loss: 78.488%
time spent training so far: 10:15:03.771683
train attack total time: 41.624s
train attack init time: 33.859s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.257s


train attack loss increase over inner max: 3.213
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.199
total grad norm: 36.474

==================== evaluation at iteration: 668 ====================
train total loss: 81.647%
train max loss: 6.021%, reg loss: 78.491%
time spent training so far: 10:15:47.919838
train attack total time: 43.638s
train attack init time: 36.384s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.233s


train attack loss increase over inner max: 1.578
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.448
total grad norm: 57.549

==================== evaluation at iteration: 669 ====================
train total loss: 79.019%
train max loss: 1.255%, reg loss: 78.228%
time spent training so far: 10:16:38.897438
train attack total time: 50.536s
train attack init time: 41.257s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.326s


train attack loss increase over inner max: -2.562
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.606
total grad norm: 44.321

==================== evaluation at iteration: 670 ====================
train total loss: 79.261%
train max loss: 1.944%, reg loss: 78.201%
time spent training so far: 10:17:32.486401
train attack total time: 53.073s
train attack init time: 42.306s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.391s


train attack loss increase over inner max: 1.779
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.403
total grad norm: 70.909

==================== evaluation at iteration: 671 ====================
train total loss: 78.493%
train max loss: 0.373%, reg loss: 78.162%
time spent training so far: 10:18:38.949459
train attack total time: 65.905s
train attack init time: 57.209s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.301s


train attack loss increase over inner max: 0.479
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.688
total grad norm: 37.432

==================== evaluation at iteration: 672 ====================
train total loss: 78.785%
train max loss: 0.651%, reg loss: 78.294%
time spent training so far: 10:19:39.738504
train attack total time: 60.305s
train attack init time: 48.658s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.433s


train attack loss increase over inner max: 3.839
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.598
total grad norm: 13.521

==================== evaluation at iteration: 673 ====================
train total loss: 78.722%
train max loss: 1.114%, reg loss: 78.253%
time spent training so far: 10:20:36.730681
train attack total time: 56.511s
train attack init time: 46.977s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.338s


train attack loss increase over inner max: 0.388
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.386
total grad norm: 19.930

==================== evaluation at iteration: 674 ====================
train total loss: 78.846%
train max loss: 1.190%, reg loss: 78.096%
time spent training so far: 10:21:45.262347
train attack total time: 68.047s
train attack init time: 55.870s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.460s


train attack loss increase over inner max: 0.610
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1908]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1908]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1908]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1907]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1905]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1903]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1900]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1897]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1894]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1891]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1887]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1883]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1878]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1874]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1869]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1864]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1859]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1854]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1849]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1844]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1839]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1834]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1828]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1822]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1817]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1813]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1810]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1807]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1805]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1804]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1803]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1804]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1806]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1810]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1816]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1822]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1830]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1837]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1845]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1853]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1860]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1865]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1870]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1873]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1876]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 4.830
total grad norm: 41.944

==================== evaluation at iteration: 675 ====================
train total loss: 78.709%
train max loss: 0.428%, reg loss: 78.321%
time spent training so far: 10:22:53.577651
train attack total time: 67.827s
train attack init time: 50.720s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.681s


train attack loss increase over inner max: 0.119
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.04%
mean, std amount infeasible at boundary: 0.00 +/- 0.01
max amount infeasible at boundary: 0.49

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.829
total grad norm: 28.902

==================== evaluation at iteration: 676 ====================
train total loss: 78.495%
train max loss: 0.226%, reg loss: 78.362%
time spent training so far: 11:12:02.719642
train attack total time: 66.143s
train attack init time: 54.214s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.450s


train attack loss increase over inner max: 0.162
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.549
total grad norm: 51.477

==================== evaluation at iteration: 677 ====================
train total loss: 78.299%
train max loss: 0.115%, reg loss: 78.183%
time spent training so far: 11:13:01.351844
train attack total time: 58.133s
train attack init time: 47.292s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: 0.910
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.474
total grad norm: 60.341

==================== evaluation at iteration: 678 ====================
train total loss: 82.579%
train max loss: 8.098%, reg loss: 78.230%
time spent training so far: 11:14:14.926462
train attack total time: 73.094s
train attack init time: 62.383s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: 7.415
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.408
total grad norm: 48.415

==================== evaluation at iteration: 679 ====================
train total loss: 83.288%
train max loss: 10.107%, reg loss: 78.164%
time spent training so far: 11:14:59.245165
train attack total time: 43.834s
train attack init time: 35.208s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.295s


train attack loss increase over inner max: 2.274
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.821
total grad norm: 39.629

==================== evaluation at iteration: 680 ====================
train total loss: 83.838%
train max loss: 10.402%, reg loss: 78.219%
time spent training so far: 11:15:37.304917
train attack total time: 37.575s
train attack init time: 30.354s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.231s


train attack loss increase over inner max: 2.286
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.926
total grad norm: 30.772

==================== evaluation at iteration: 681 ====================
train total loss: 82.896%
train max loss: 10.583%, reg loss: 78.312%
time spent training so far: 11:16:09.389228
train attack total time: 31.517s
train attack init time: 24.483s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.224s


train attack loss increase over inner max: 2.695
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.537
total grad norm: 22.183

==================== evaluation at iteration: 682 ====================
train total loss: 82.710%
train max loss: 8.092%, reg loss: 78.043%
time spent training so far: 11:16:39.341110
train attack total time: 29.425s
train attack init time: 21.355s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.271s


train attack loss increase over inner max: 0.759
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.915
total grad norm: 28.910

==================== evaluation at iteration: 683 ====================
train total loss: 82.559%
train max loss: 10.629%, reg loss: 78.268%
time spent training so far: 11:17:05.839453
train attack total time: 25.986s
train attack init time: 19.111s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.214s


train attack loss increase over inner max: 3.708
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.167
total grad norm: 47.301

==================== evaluation at iteration: 684 ====================
train total loss: 85.894%
train max loss: 16.267%, reg loss: 78.347%
time spent training so far: 11:17:32.131681
train attack total time: 25.777s
train attack init time: 18.267s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.249s


train attack loss increase over inner max: 7.053
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.029
total grad norm: 28.181

==================== evaluation at iteration: 685 ====================
train total loss: 83.920%
train max loss: 10.385%, reg loss: 78.302%
time spent training so far: 11:18:03.517927
train attack total time: 30.866s
train attack init time: 23.916s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.220s


train attack loss increase over inner max: -4.980
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.167
total grad norm: 24.991

==================== evaluation at iteration: 686 ====================
train total loss: 82.338%
train max loss: 5.946%, reg loss: 78.381%
time spent training so far: 11:18:39.770624
train attack total time: 35.601s
train attack init time: 28.497s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.223s


train attack loss increase over inner max: -1.904
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.427
total grad norm: 46.909

==================== evaluation at iteration: 687 ====================
train total loss: 82.679%
train max loss: 6.772%, reg loss: 78.036%
time spent training so far: 11:19:13.700382
train attack total time: 33.413s
train attack init time: 26.101s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.236s


train attack loss increase over inner max: 1.976
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.675
total grad norm: 46.625

==================== evaluation at iteration: 688 ====================
train total loss: 81.041%
train max loss: 6.393%, reg loss: 78.259%
time spent training so far: 11:19:57.527153
train attack total time: 43.363s
train attack init time: 36.022s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.240s


train attack loss increase over inner max: 2.895
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.627
total grad norm: 47.997

==================== evaluation at iteration: 689 ====================
train total loss: 82.038%
train max loss: 7.847%, reg loss: 78.233%
time spent training so far: 11:20:37.828859
train attack total time: 39.807s
train attack init time: 30.592s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.321s


train attack loss increase over inner max: 5.398
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.913
total grad norm: 4.913

==================== evaluation at iteration: 690 ====================
train total loss: 78.331%
train max loss: -0.696%, reg loss: 78.331%
time spent training so far: 11:21:41.427136
train attack total time: 63.069s
train attack init time: 49.870s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.504s


train attack loss increase over inner max: -4.233
OOM debug. Mem allocated and reserved: 1092096.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.784
total grad norm: 31.932

==================== evaluation at iteration: 691 ====================
train total loss: 78.683%
train max loss: 0.574%, reg loss: 78.253%
time spent training so far: 11:22:26.875381
train attack total time: 44.872s
train attack init time: 36.160s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.297s


train attack loss increase over inner max: 0.566
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.803
total grad norm: 27.777

==================== evaluation at iteration: 692 ====================
train total loss: 79.159%
train max loss: 1.544%, reg loss: 78.246%
time spent training so far: 11:23:15.597501
train attack total time: 48.229s
train attack init time: 40.584s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.248s


train attack loss increase over inner max: 1.270
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.608
total grad norm: 26.166

==================== evaluation at iteration: 693 ====================
train total loss: 78.959%
train max loss: 1.209%, reg loss: 78.227%
time spent training so far: 11:24:23.733177
train attack total time: 67.614s
train attack init time: 58.629s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.314s


train attack loss increase over inner max: 0.073
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.213
total grad norm: 35.969

==================== evaluation at iteration: 694 ====================
train total loss: 78.983%
train max loss: 0.765%, reg loss: 78.391%
time spent training so far: 11:25:23.630121
train attack total time: 59.408s
train attack init time: 50.014s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.329s


train attack loss increase over inner max: -0.493
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.758
total grad norm: 33.144

==================== evaluation at iteration: 695 ====================
train total loss: 78.680%
train max loss: 0.621%, reg loss: 78.203%
time spent training so far: 11:26:26.764130
train attack total time: 62.660s
train attack init time: 51.758s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.403s


train attack loss increase over inner max: -0.064
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.669
total grad norm: 4.669

==================== evaluation at iteration: 696 ====================
train total loss: 78.154%
train max loss: -0.309%, reg loss: 78.154%
time spent training so far: 11:27:24.320055
train attack total time: 56.964s
train attack init time: 45.241s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.435s


train attack loss increase over inner max: 0.077
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.637
total grad norm: 234.301

==================== evaluation at iteration: 697 ====================
train total loss: 84.140%
train max loss: 6.029%, reg loss: 78.111%
time spent training so far: 11:28:43.695892
train attack total time: 78.902s
train attack init time: 59.477s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.789s


train attack loss increase over inner max: 2.074
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.637
total grad norm: 240.037

==================== evaluation at iteration: 698 ====================
train total loss: 80.484%
train max loss: 2.398%, reg loss: 78.086%
time spent training so far: 11:29:44.561715
train attack total time: 60.406s
train attack init time: 49.004s
train attack avg grad step time: 0.099s
train attack avg reproj time: 0.416s


train attack loss increase over inner max: 2.931
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.484
total grad norm: 54.488

==================== evaluation at iteration: 699 ====================
train total loss: 83.433%
train max loss: 8.269%, reg loss: 78.017%
time spent training so far: 11:30:42.476674
train attack total time: 57.404s
train attack init time: 48.484s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.308s


train attack loss increase over inner max: 10.202
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.832
total grad norm: 55.205

==================== evaluation at iteration: 700 ====================
train total loss: 83.112%
train max loss: 7.755%, reg loss: 78.153%
time spent training so far: 11:31:32.594019
train attack total time: 49.654s
train attack init time: 41.158s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.291s


train attack loss increase over inner max: 2.068
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.64%
mean, std amount infeasible at boundary: 0.02 +/- 0.34
max amount infeasible at boundary: 12.70

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.657
total grad norm: 54.477

==================== evaluation at iteration: 701 ====================
train total loss: 82.863%
train max loss: 9.797%, reg loss: 78.192%
time spent training so far: 12:10:42.701291
train attack total time: 52.393s
train attack init time: 39.797s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.474s


train attack loss increase over inner max: 1.244
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.667
total grad norm: 79.792

==================== evaluation at iteration: 702 ====================
train total loss: 91.963%
train max loss: 19.180%, reg loss: 78.139%
time spent training so far: 12:11:30.947765
train attack total time: 47.728s
train attack init time: 37.483s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.368s


train attack loss increase over inner max: 10.831
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.201
total grad norm: 107.791

==================== evaluation at iteration: 703 ====================
train total loss: 96.041%
train max loss: 20.184%, reg loss: 78.310%
time spent training so far: 12:12:20.868920
train attack total time: 49.403s
train attack init time: 40.416s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.316s


train attack loss increase over inner max: 4.061
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.268
total grad norm: 98.036

==================== evaluation at iteration: 704 ====================
train total loss: 83.873%
train max loss: 8.891%, reg loss: 78.506%
time spent training so far: 12:13:10.532657
train attack total time: 49.155s
train attack init time: 36.586s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.472s


train attack loss increase over inner max: -4.580
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.114
total grad norm: 46.337

==================== evaluation at iteration: 705 ====================
train total loss: 81.103%
train max loss: 5.828%, reg loss: 78.384%
time spent training so far: 12:13:56.453673
train attack total time: 45.434s
train attack init time: 30.685s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.573s


train attack loss increase over inner max: 2.548
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.661
total grad norm: 44.966

==================== evaluation at iteration: 706 ====================
train total loss: 83.051%
train max loss: 9.044%, reg loss: 78.744%
time spent training so far: 12:14:38.154496
train attack total time: 41.119s
train attack init time: 29.955s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.409s


train attack loss increase over inner max: 2.517
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.648
total grad norm: 43.108

==================== evaluation at iteration: 707 ====================
train total loss: 84.751%
train max loss: 13.940%, reg loss: 78.796%
time spent training so far: 12:15:24.291509
train attack total time: 45.629s
train attack init time: 31.101s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.563s


train attack loss increase over inner max: 7.443
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.860
total grad norm: 31.410

==================== evaluation at iteration: 708 ====================
train total loss: 83.051%
train max loss: 10.533%, reg loss: 79.047%
time spent training so far: 12:16:12.241377
train attack total time: 47.437s
train attack init time: 36.207s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.413s


train attack loss increase over inner max: 0.915
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.016
total grad norm: 37.758

==================== evaluation at iteration: 709 ====================
train total loss: 82.725%
train max loss: 9.193%, reg loss: 78.825%
time spent training so far: 12:16:53.227938
train attack total time: 40.543s
train attack init time: 30.069s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.378s


train attack loss increase over inner max: -1.708
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.569
total grad norm: 62.222

==================== evaluation at iteration: 710 ====================
train total loss: 79.331%
train max loss: 0.413%, reg loss: 78.918%
time spent training so far: 12:17:52.129640
train attack total time: 58.434s
train attack init time: 30.521s
train attack avg grad step time: 0.095s
train attack avg reproj time: 1.170s


train attack loss increase over inner max: -2.354
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.111
total grad norm: 27.592

==================== evaluation at iteration: 711 ====================
train total loss: 80.625%
train max loss: 2.464%, reg loss: 79.248%
time spent training so far: 12:18:38.909254
train attack total time: 46.179s
train attack init time: 36.343s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.348s


train attack loss increase over inner max: 4.479
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.436
total grad norm: 30.432

==================== evaluation at iteration: 712 ====================
train total loss: 80.446%
train max loss: 2.007%, reg loss: 78.957%
time spent training so far: 12:19:19.740082
train attack total time: 40.343s
train attack init time: 30.615s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.343s


train attack loss increase over inner max: 0.699
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.664
total grad norm: 26.595

==================== evaluation at iteration: 713 ====================
train total loss: 79.833%
train max loss: 0.770%, reg loss: 79.146%
time spent training so far: 12:20:28.061761
train attack total time: 67.826s
train attack init time: 53.773s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.539s


train attack loss increase over inner max: 0.248
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.451
total grad norm: 5.451

==================== evaluation at iteration: 714 ====================
train total loss: 79.044%
train max loss: -0.021%, reg loss: 79.044%
time spent training so far: 12:21:24.052758
train attack total time: 55.485s
train attack init time: 44.274s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.414s


train attack loss increase over inner max: 0.586
OOM debug. Mem allocated and reserved: 1092096.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.570
total grad norm: 27.411

==================== evaluation at iteration: 715 ====================
train total loss: 82.988%
train max loss: 5.600%, reg loss: 79.150%
time spent training so far: 12:22:15.822719
train attack total time: 51.290s
train attack init time: 37.927s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.509s


train attack loss increase over inner max: 6.263
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.418
total grad norm: 14.895

==================== evaluation at iteration: 716 ====================
train total loss: 81.913%
train max loss: 6.655%, reg loss: 79.018%
time spent training so far: 12:23:06.495582
train attack total time: 50.058s
train attack init time: 39.572s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.378s


train attack loss increase over inner max: 1.509
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.290
total grad norm: 50.895

==================== evaluation at iteration: 717 ====================
train total loss: 82.972%
train max loss: 5.977%, reg loss: 79.394%
time spent training so far: 12:24:05.701802
train attack total time: 58.729s
train attack init time: 45.940s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.486s


train attack loss increase over inner max: -0.485
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.058
total grad norm: 131.072

==================== evaluation at iteration: 718 ====================
train total loss: 80.311%
train max loss: 0.840%, reg loss: 79.471%
time spent training so far: 12:25:02.005140
train attack total time: 55.792s
train attack init time: 43.315s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.468s


train attack loss increase over inner max: -2.169
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.509
total grad norm: 24.998

==================== evaluation at iteration: 719 ====================
train total loss: 79.470%
train max loss: 0.510%, reg loss: 79.121%
time spent training so far: 12:25:57.558965
train attack total time: 55.075s
train attack init time: 40.225s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.579s


train attack loss increase over inner max: 2.415
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.952
total grad norm: 5.952

==================== evaluation at iteration: 720 ====================
train total loss: 79.319%
train max loss: -0.476%, reg loss: 79.319%
time spent training so far: 12:26:50.381850
train attack total time: 52.323s
train attack init time: 34.300s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.722s


train attack loss increase over inner max: -1.215
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000

Max_n_steps: 20
Parameter containing:
tensor([[0.1878]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1879]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1881]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1885]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1890]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1897]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1904]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1911]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1918]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1926]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1935]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1942]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1951]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1958]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1964]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1970]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1974]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1978]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1981]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1982]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1983]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.1984]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.1990]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2001]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2014]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2028]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2044]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2062]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2082]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2103]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2121]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2136]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2149]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2160]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2168]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2175]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2181]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2184]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2187]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2189]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2191]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2191]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2190]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2189]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2188]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2187]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 5.495
total grad norm: 48.857

==================== evaluation at iteration: 721 ====================
train total loss: 79.713%
train max loss: 0.663%, reg loss: 79.050%
time spent training so far: 12:27:51.511418
train attack total time: 60.553s
train attack init time: 48.141s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.466s


train attack loss increase over inner max: 0.583
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 7.093
total grad norm: 33.304

==================== evaluation at iteration: 722 ====================
train total loss: 80.551%
train max loss: 1.263%, reg loss: 79.797%
time spent training so far: 12:28:41.447155
train attack total time: 49.421s
train attack init time: 38.551s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.399s


train attack loss increase over inner max: 1.703
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.104
total grad norm: 48.616

==================== evaluation at iteration: 723 ====================
train total loss: 80.118%
train max loss: 1.313%, reg loss: 79.327%
time spent training so far: 12:29:26.495805
train attack total time: 44.522s
train attack init time: 33.129s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.420s


train attack loss increase over inner max: 2.205
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.396
total grad norm: 26.157

==================== evaluation at iteration: 724 ====================
train total loss: 80.852%
train max loss: 4.858%, reg loss: 79.014%
time spent training so far: 12:30:03.586769
train attack total time: 36.560s
train attack init time: 28.651s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.261s


train attack loss increase over inner max: 3.826
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.179
total grad norm: 22.089

==================== evaluation at iteration: 725 ====================
train total loss: 82.136%
train max loss: 5.988%, reg loss: 79.262%
time spent training so far: 12:30:31.713224
train attack total time: 27.651s
train attack init time: 19.651s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: 1.161
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 1.28%
mean, std amount infeasible at boundary: 0.02 +/- 0.22
max amount infeasible at boundary: 5.09

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 6.194
total grad norm: 28.158

==================== evaluation at iteration: 726 ====================
train total loss: 82.400%
train max loss: 5.523%, reg loss: 79.351%
time spent training so far: 12:51:17.966073
train attack total time: 29.759s
train attack init time: 21.987s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.260s


train attack loss increase over inner max: 0.831
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.462
total grad norm: 21.323

==================== evaluation at iteration: 727 ====================
train total loss: 82.567%
train max loss: 7.384%, reg loss: 79.597%
time spent training so far: 12:51:48.634459
train attack total time: 30.136s
train attack init time: 23.194s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.216s


train attack loss increase over inner max: 1.554
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.149
total grad norm: 30.301

==================== evaluation at iteration: 728 ====================
train total loss: 82.922%
train max loss: 9.277%, reg loss: 78.938%
time spent training so far: 12:52:14.392578
train attack total time: 25.204s
train attack init time: 17.734s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.242s


train attack loss increase over inner max: 2.586
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.694
total grad norm: 30.459

==================== evaluation at iteration: 729 ====================
train total loss: 82.719%
train max loss: 6.622%, reg loss: 79.085%
time spent training so far: 12:52:47.225467
train attack total time: 32.314s
train attack init time: 22.689s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.339s


train attack loss increase over inner max: -0.377
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.992
total grad norm: 39.913

==================== evaluation at iteration: 730 ====================
train total loss: 81.627%
train max loss: 5.384%, reg loss: 79.162%
time spent training so far: 12:53:16.958889
train attack total time: 29.269s
train attack init time: 20.341s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.310s


train attack loss increase over inner max: 0.028
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.278
total grad norm: 24.702

==================== evaluation at iteration: 731 ====================
train total loss: 81.157%
train max loss: 3.479%, reg loss: 79.397%
time spent training so far: 12:53:55.031761
train attack total time: 37.512s
train attack init time: 26.534s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.406s


train attack loss increase over inner max: 0.115
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.631
total grad norm: 35.626

==================== evaluation at iteration: 732 ====================
train total loss: 81.392%
train max loss: 4.488%, reg loss: 79.060%
time spent training so far: 12:54:31.257349
train attack total time: 35.787s
train attack init time: 24.083s
train attack avg grad step time: 0.098s
train attack avg reproj time: 0.430s


train attack loss increase over inner max: 0.513
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.459
total grad norm: 24.124

==================== evaluation at iteration: 733 ====================
train total loss: 80.368%
train max loss: 3.059%, reg loss: 78.972%
time spent training so far: 12:55:08.622436
train attack total time: 36.850s
train attack init time: 27.573s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.325s


train attack loss increase over inner max: -0.653
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.197
total grad norm: 27.862

==================== evaluation at iteration: 734 ====================
train total loss: 80.113%
train max loss: 1.337%, reg loss: 79.380%
time spent training so far: 12:55:55.789522
train attack total time: 46.664s
train attack init time: 35.327s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.416s


train attack loss increase over inner max: 0.050
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.388
total grad norm: 35.761

==================== evaluation at iteration: 735 ====================
train total loss: 80.245%
train max loss: 3.521%, reg loss: 78.968%
time spent training so far: 12:57:04.709934
train attack total time: 68.417s
train attack init time: 58.609s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.350s


train attack loss increase over inner max: 3.280
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.332
total grad norm: 43.955

==================== evaluation at iteration: 736 ====================
train total loss: 79.898%
train max loss: 1.799%, reg loss: 78.898%
time spent training so far: 12:58:06.975162
train attack total time: 61.653s
train attack init time: 48.670s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.490s


train attack loss increase over inner max: -2.048
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.180
total grad norm: 58.789

==================== evaluation at iteration: 737 ====================
train total loss: 79.436%
train max loss: 0.710%, reg loss: 78.726%
time spent training so far: 12:59:08.334845
train attack total time: 60.840s
train attack init time: 45.870s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.586s


train attack loss increase over inner max: -1.013
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.175
total grad norm: 5.175

==================== evaluation at iteration: 738 ====================
train total loss: 78.819%
train max loss: -0.546%, reg loss: 78.819%
time spent training so far: 13:00:16.486168
train attack total time: 67.635s
train attack init time: 53.858s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.533s


train attack loss increase over inner max: 0.727
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.024
total grad norm: 35.926

==================== evaluation at iteration: 739 ====================
train total loss: 78.978%
train max loss: 0.780%, reg loss: 78.668%
time spent training so far: 13:01:26.399177
train attack total time: 69.387s
train attack init time: 58.869s
train attack avg grad step time: 0.100s
train attack avg reproj time: 0.375s


train attack loss increase over inner max: 0.747
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.531
total grad norm: 27.883

==================== evaluation at iteration: 740 ====================
train total loss: 79.653%
train max loss: 1.711%, reg loss: 78.833%
time spent training so far: 13:02:34.956642
train attack total time: 68.097s
train attack init time: 57.162s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.396s


train attack loss increase over inner max: 1.421
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.189
total grad norm: 55.344

==================== evaluation at iteration: 741 ====================
train total loss: 79.288%
train max loss: 0.505%, reg loss: 78.784%
time spent training so far: 13:03:46.924848
train attack total time: 71.352s
train attack init time: 53.645s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.708s


train attack loss increase over inner max: -0.661
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.117
total grad norm: 10.724

==================== evaluation at iteration: 742 ====================
train total loss: 78.895%
train max loss: 0.823%, reg loss: 78.529%
time spent training so far: 13:04:57.312288
train attack total time: 69.872s
train attack init time: 56.415s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.511s


train attack loss increase over inner max: 1.393
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.228
total grad norm: 30.121

==================== evaluation at iteration: 743 ====================
train total loss: 79.658%
train max loss: 1.619%, reg loss: 78.641%
time spent training so far: 13:06:04.667641
train attack total time: 66.839s
train attack init time: 55.960s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.400s


train attack loss increase over inner max: 1.128
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.998
total grad norm: 23.546

==================== evaluation at iteration: 744 ====================
train total loss: 79.074%
train max loss: 0.824%, reg loss: 78.610%
time spent training so far: 13:07:08.472903
train attack total time: 63.302s
train attack init time: 54.336s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.310s


train attack loss increase over inner max: 1.176
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.183
total grad norm: 22.179

==================== evaluation at iteration: 745 ====================
train total loss: 78.907%
train max loss: 0.779%, reg loss: 78.530%
time spent training so far: 13:08:09.022115
train attack total time: 60.007s
train attack init time: 50.825s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.322s


train attack loss increase over inner max: 0.376
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.887
total grad norm: 30.229

==================== evaluation at iteration: 746 ====================
train total loss: 79.268%
train max loss: 0.754%, reg loss: 78.544%
time spent training so far: 13:09:06.063255
train attack total time: 56.373s
train attack init time: 44.736s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.432s


train attack loss increase over inner max: 0.212
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.698
total grad norm: 7.081

==================== evaluation at iteration: 747 ====================
train total loss: 78.544%
train max loss: 0.150%, reg loss: 78.394%
time spent training so far: 13:10:32.859621
train attack total time: 86.317s
train attack init time: 65.768s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.839s


train attack loss increase over inner max: -0.555
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.131
total grad norm: 21.131

==================== evaluation at iteration: 748 ====================
train total loss: 78.159%
train max loss: 0.100%, reg loss: 78.059%
time spent training so far: 13:11:45.889185
train attack total time: 72.555s
train attack init time: 60.186s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.465s


train attack loss increase over inner max: 0.245
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.773
total grad norm: 29.984

==================== evaluation at iteration: 749 ====================
train total loss: 78.921%
train max loss: 0.876%, reg loss: 78.276%
time spent training so far: 13:12:50.710186
train attack total time: 64.335s
train attack init time: 48.923s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.605s


train attack loss increase over inner max: 0.262
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.114
total grad norm: 48.089

==================== evaluation at iteration: 750 ====================
train total loss: 79.183%
train max loss: 1.227%, reg loss: 78.489%
time spent training so far: 13:14:21.026402
train attack total time: 89.808s
train attack init time: 76.495s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.510s


train attack loss increase over inner max: 1.334
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.28%
mean, std amount infeasible at boundary: 0.01 +/- 0.21
max amount infeasible at boundary: 8.83

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.158
total grad norm: 33.244

==================== evaluation at iteration: 751 ====================
train total loss: 79.283%
train max loss: 1.254%, reg loss: 78.030%
time spent training so far: 13:59:35.389285
train attack total time: 61.317s
train attack init time: 52.644s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.301s


train attack loss increase over inner max: 0.612
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.711
total grad norm: 29.606

==================== evaluation at iteration: 752 ====================
train total loss: 78.777%
train max loss: 0.791%, reg loss: 78.252%
time spent training so far: 14:00:31.897568
train attack total time: 56.052s
train attack init time: 41.282s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.576s


train attack loss increase over inner max: -0.336
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.448
total grad norm: 25.980

==================== evaluation at iteration: 753 ====================
train total loss: 78.673%
train max loss: 0.964%, reg loss: 78.020%
time spent training so far: 14:01:42.932803
train attack total time: 70.497s
train attack init time: 43.878s
train attack avg grad step time: 0.091s
train attack avg reproj time: 1.115s


train attack loss increase over inner max: 0.543
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.062
total grad norm: 42.561

==================== evaluation at iteration: 754 ====================
train total loss: 78.763%
train max loss: 0.936%, reg loss: 78.111%
time spent training so far: 14:02:50.056810
train attack total time: 66.608s
train attack init time: 53.431s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.508s


train attack loss increase over inner max: 0.186
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.730
total grad norm: 33.218

==================== evaluation at iteration: 755 ====================
train total loss: 78.843%
train max loss: 0.798%, reg loss: 78.090%
time spent training so far: 14:03:40.037646
train attack total time: 49.456s
train attack init time: 40.014s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.331s


train attack loss increase over inner max: 0.171
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.090
total grad norm: 38.711

==================== evaluation at iteration: 756 ====================
train total loss: 81.462%
train max loss: 5.468%, reg loss: 78.175%
time spent training so far: 14:04:35.141130
train attack total time: 54.468s
train attack init time: 42.415s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.451s


train attack loss increase over inner max: 5.002
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.271
total grad norm: 43.213

==================== evaluation at iteration: 757 ====================
train total loss: 82.921%
train max loss: 9.694%, reg loss: 77.893%
time spent training so far: 14:05:39.647778
train attack total time: 64.078s
train attack init time: 53.558s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.381s


train attack loss increase over inner max: 4.409
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.336
total grad norm: 114.611

==================== evaluation at iteration: 758 ====================
train total loss: 82.377%
train max loss: 4.339%, reg loss: 78.039%
time spent training so far: 14:06:46.240341
train attack total time: 66.102s
train attack init time: 51.723s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.558s


train attack loss increase over inner max: -3.513
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.180
total grad norm: 70.843

==================== evaluation at iteration: 759 ====================
train total loss: 78.082%
train max loss: 0.070%, reg loss: 78.013%
time spent training so far: 14:08:03.576851
train attack total time: 76.841s
train attack init time: 68.783s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.272s


train attack loss increase over inner max: 0.423
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.336
total grad norm: 53.447

==================== evaluation at iteration: 760 ====================
train total loss: 78.690%
train max loss: 1.162%, reg loss: 77.951%
time spent training so far: 14:09:12.115938
train attack total time: 67.992s
train attack init time: 57.507s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.379s


train attack loss increase over inner max: 1.855
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.701
total grad norm: 102.637

==================== evaluation at iteration: 761 ====================
train total loss: 79.905%
train max loss: 2.958%, reg loss: 77.974%
time spent training so far: 14:10:37.957675
train attack total time: 85.252s
train attack init time: 63.041s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.912s


train attack loss increase over inner max: 2.652
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.464
total grad norm: 72.441

==================== evaluation at iteration: 762 ====================
train total loss: 83.691%
train max loss: 10.541%, reg loss: 77.878%
time spent training so far: 14:11:52.192144
train attack total time: 73.774s
train attack init time: 59.071s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.573s


train attack loss increase over inner max: 10.139
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.450
total grad norm: 112.512

==================== evaluation at iteration: 763 ====================
train total loss: 82.930%
train max loss: 7.909%, reg loss: 77.831%
time spent training so far: 14:13:09.940070
train attack total time: 77.241s
train attack init time: 60.376s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.670s


train attack loss increase over inner max: -5.399
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.506
total grad norm: 123.369

==================== evaluation at iteration: 764 ====================
train total loss: 84.399%
train max loss: 10.697%, reg loss: 77.996%
time spent training so far: 14:14:12.016716
train attack total time: 61.591s
train attack init time: 49.323s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.462s


train attack loss increase over inner max: 7.606
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.395
total grad norm: 58.347

==================== evaluation at iteration: 765 ====================
train total loss: 83.767%
train max loss: 13.174%, reg loss: 77.835%
time spent training so far: 14:15:08.954655
train attack total time: 56.467s
train attack init time: 47.755s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.300s


train attack loss increase over inner max: 12.457
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2185]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2184]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2182]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2179]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2175]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2171]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2166]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2162]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2157]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2152]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2147]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2141]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2137]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2132]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2127]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2122]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2116]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2112]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2107]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2103]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2098]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2093]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2088]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2083]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2078]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2073]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2069]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2064]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2060]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2055]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2050]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2045]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Attack: reprojection exited on timeout, max dist from =0 boundary:  0.10225965082645416
Parameter containing:
tensor([[0.2040]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2035]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2030]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2025]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2021]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2018]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2015]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2012]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2012]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2015]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2024]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2035]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2047]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 4.717
total grad norm: 79.385

==================== evaluation at iteration: 766 ====================
train total loss: 85.040%
train max loss: 9.573%, reg loss: 78.034%
time spent training so far: 14:16:01.656915
train attack total time: 52.065s
train attack init time: 39.748s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.461s


train attack loss increase over inner max: -2.240
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.702
total grad norm: 68.397

==================== evaluation at iteration: 767 ====================
train total loss: 83.527%
train max loss: 8.445%, reg loss: 78.008%
time spent training so far: 14:16:42.307150
train attack total time: 40.138s
train attack init time: 29.372s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: 3.403
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.921
total grad norm: 44.724

==================== evaluation at iteration: 768 ====================
train total loss: 83.629%
train max loss: 9.217%, reg loss: 78.133%
time spent training so far: 14:17:31.775589
train attack total time: 49.003s
train attack init time: 41.461s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.246s


train attack loss increase over inner max: 1.131
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.196
total grad norm: 42.589

==================== evaluation at iteration: 769 ====================
train total loss: 82.172%
train max loss: 8.804%, reg loss: 78.279%
time spent training so far: 14:18:14.785510
train attack total time: 42.574s
train attack init time: 32.236s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.371s


train attack loss increase over inner max: -0.762
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.796
total grad norm: 27.629

==================== evaluation at iteration: 770 ====================
train total loss: 82.026%
train max loss: 7.360%, reg loss: 78.200%
time spent training so far: 14:18:59.052880
train attack total time: 43.797s
train attack init time: 32.587s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.413s


train attack loss increase over inner max: 0.410
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.713
total grad norm: 43.842

==================== evaluation at iteration: 771 ====================
train total loss: 82.498%
train max loss: 6.529%, reg loss: 78.232%
time spent training so far: 14:19:54.654544
train attack total time: 55.014s
train attack init time: 43.963s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.406s


train attack loss increase over inner max: 0.921
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.382
total grad norm: 60.059

==================== evaluation at iteration: 772 ====================
train total loss: 80.656%
train max loss: 3.463%, reg loss: 78.553%
time spent training so far: 14:20:56.642472
train attack total time: 61.429s
train attack init time: 48.971s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.468s


train attack loss increase over inner max: -0.624
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.735
total grad norm: 4.735

==================== evaluation at iteration: 773 ====================
train total loss: 78.214%
train max loss: -0.319%, reg loss: 78.214%
time spent training so far: 14:21:52.176670
train attack total time: 55.015s
train attack init time: 42.351s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.475s


train attack loss increase over inner max: 0.150
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.198
total grad norm: 28.548

==================== evaluation at iteration: 774 ====================
train total loss: 78.528%
train max loss: 0.162%, reg loss: 78.366%
time spent training so far: 14:22:51.362928
train attack total time: 58.657s
train attack init time: 48.297s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.376s


train attack loss increase over inner max: 1.053
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.034
total grad norm: 88.870

==================== evaluation at iteration: 775 ====================
train total loss: 78.562%
train max loss: 0.075%, reg loss: 78.487%
time spent training so far: 14:23:59.569713
train attack total time: 67.692s
train attack init time: 45.213s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.925s


train attack loss increase over inner max: 0.517
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.12%
mean, std amount infeasible at boundary: 0.00 +/- 0.03
max amount infeasible at boundary: 0.95

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 5.605
total grad norm: 39.931

==================== evaluation at iteration: 776 ====================
train total loss: 78.947%
train max loss: 0.330%, reg loss: 78.617%
time spent training so far: 15:08:23.039406
train attack total time: 72.444s
train attack init time: 57.895s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.562s


train attack loss increase over inner max: 0.823
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.302
total grad norm: 18.623

==================== evaluation at iteration: 777 ====================
train total loss: 79.312%
train max loss: 1.319%, reg loss: 78.461%
time spent training so far: 15:09:18.045004
train attack total time: 54.490s
train attack init time: 44.346s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.364s


train attack loss increase over inner max: 2.134
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.150
total grad norm: 46.490

==================== evaluation at iteration: 778 ====================
train total loss: 79.989%
train max loss: 1.816%, reg loss: 78.521%
time spent training so far: 15:10:04.700906
train attack total time: 46.136s
train attack init time: 33.124s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.497s


train attack loss increase over inner max: 0.738
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.209
total grad norm: 44.442

==================== evaluation at iteration: 779 ====================
train total loss: 78.857%
train max loss: 0.335%, reg loss: 78.523%
time spent training so far: 15:10:45.142366
train attack total time: 39.949s
train attack init time: 27.898s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.453s


train attack loss increase over inner max: -2.085
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.581
total grad norm: 54.843

==================== evaluation at iteration: 780 ====================
train total loss: 78.426%
train max loss: 0.092%, reg loss: 78.334%
time spent training so far: 15:11:44.273100
train attack total time: 58.613s
train attack init time: 44.117s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.565s


train attack loss increase over inner max: 0.249
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.039
total grad norm: 45.821

==================== evaluation at iteration: 781 ====================
train total loss: 78.796%
train max loss: 0.323%, reg loss: 78.502%
time spent training so far: 15:12:38.025934
train attack total time: 53.106s
train attack init time: 42.172s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.404s


train attack loss increase over inner max: 1.373
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.875
total grad norm: 5.875

==================== evaluation at iteration: 782 ====================
train total loss: 78.726%
train max loss: -0.094%, reg loss: 78.726%
time spent training so far: 15:13:30.493615
train attack total time: 51.936s
train attack init time: 41.210s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.393s


train attack loss increase over inner max: 1.345
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.384
total grad norm: 28.190

==================== evaluation at iteration: 783 ====================
train total loss: 82.644%
train max loss: 7.241%, reg loss: 78.650%
time spent training so far: 15:14:16.118197
train attack total time: 45.169s
train attack init time: 36.709s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.289s


train attack loss increase over inner max: 5.422
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.253
total grad norm: 32.299

==================== evaluation at iteration: 784 ====================
train total loss: 83.706%
train max loss: 10.847%, reg loss: 78.452%
time spent training so far: 15:14:57.579459
train attack total time: 40.985s
train attack init time: 27.920s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.501s


train attack loss increase over inner max: 3.156
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.595
total grad norm: 32.575

==================== evaluation at iteration: 785 ====================
train total loss: 86.355%
train max loss: 14.719%, reg loss: 78.686%
time spent training so far: 15:15:25.968105
train attack total time: 27.868s
train attack init time: 20.701s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.232s


train attack loss increase over inner max: 2.798
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.083
total grad norm: 23.988

==================== evaluation at iteration: 786 ====================
train total loss: 84.025%
train max loss: 12.923%, reg loss: 78.444%
time spent training so far: 15:16:08.029222
train attack total time: 41.396s
train attack init time: 18.208s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.960s


train attack loss increase over inner max: -1.968
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.369
total grad norm: 30.384

==================== evaluation at iteration: 787 ====================
train total loss: 85.466%
train max loss: 13.600%, reg loss: 78.438%
time spent training so far: 15:16:46.372537
train attack total time: 37.802s
train attack init time: 27.612s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.368s


train attack loss increase over inner max: 1.128
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.234
total grad norm: 33.077

==================== evaluation at iteration: 788 ====================
train total loss: 87.298%
train max loss: 11.667%, reg loss: 78.511%
time spent training so far: 15:17:24.756218
train attack total time: 37.919s
train attack init time: 27.049s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.399s


train attack loss increase over inner max: -0.756
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.274
total grad norm: 38.297

==================== evaluation at iteration: 789 ====================
train total loss: 84.689%
train max loss: 10.279%, reg loss: 78.503%
time spent training so far: 15:17:56.350418
train attack total time: 31.159s
train attack init time: 23.866s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.234s


train attack loss increase over inner max: -0.249
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.434
total grad norm: 46.244

==================== evaluation at iteration: 790 ====================
train total loss: 84.961%
train max loss: 10.494%, reg loss: 78.583%
time spent training so far: 15:18:33.968431
train attack total time: 37.105s
train attack init time: 28.128s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.314s


train attack loss increase over inner max: 2.494
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.246
total grad norm: 36.613

==================== evaluation at iteration: 791 ====================
train total loss: 83.028%
train max loss: 8.101%, reg loss: 78.457%
time spent training so far: 15:19:24.091907
train attack total time: 49.557s
train attack init time: 39.392s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.361s


train attack loss increase over inner max: 0.523
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.179
total grad norm: 48.374

==================== evaluation at iteration: 792 ====================
train total loss: 83.946%
train max loss: 8.047%, reg loss: 78.504%
time spent training so far: 15:20:19.500772
train attack total time: 54.847s
train attack init time: 42.403s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.471s


train attack loss increase over inner max: 4.434
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.327
total grad norm: 87.409

==================== evaluation at iteration: 793 ====================
train total loss: 81.140%
train max loss: 2.729%, reg loss: 78.459%
time spent training so far: 15:21:18.762990
train attack total time: 58.748s
train attack init time: 44.883s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.536s


train attack loss increase over inner max: -2.143
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.413
total grad norm: 5.413

==================== evaluation at iteration: 794 ====================
train total loss: 78.596%
train max loss: -0.133%, reg loss: 78.596%
time spent training so far: 15:22:23.873329
train attack total time: 64.613s
train attack init time: 43.329s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.873s


train attack loss increase over inner max: 1.101
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.015
total grad norm: 50.644

==================== evaluation at iteration: 795 ====================
train total loss: 79.312%
train max loss: 1.410%, reg loss: 78.465%
time spent training so far: 15:23:22.548597
train attack total time: 58.185s
train attack init time: 46.515s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.436s


train attack loss increase over inner max: 1.391
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.815
total grad norm: 57.266

==================== evaluation at iteration: 796 ====================
train total loss: 78.721%
train max loss: 0.563%, reg loss: 78.355%
time spent training so far: 15:24:28.252535
train attack total time: 65.067s
train attack init time: 52.126s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.487s


train attack loss increase over inner max: 0.655
OOM debug. Mem allocated and reserved: 1094144.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.626
total grad norm: 36.656

==================== evaluation at iteration: 797 ====================
train total loss: 80.093%
train max loss: 2.543%, reg loss: 78.753%
time spent training so far: 15:25:30.229809
train attack total time: 61.473s
train attack init time: 51.857s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.340s


train attack loss increase over inner max: 2.224
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.955
total grad norm: 43.383

==================== evaluation at iteration: 798 ====================
train total loss: 79.536%
train max loss: 2.750%, reg loss: 78.458%
time spent training so far: 15:26:40.446184
train attack total time: 69.743s
train attack init time: 55.060s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.571s


train attack loss increase over inner max: 0.841
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.326
total grad norm: 68.645

==================== evaluation at iteration: 799 ====================
train total loss: 78.993%
train max loss: 0.349%, reg loss: 78.773%
time spent training so far: 15:27:55.588832
train attack total time: 74.671s
train attack init time: 62.684s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.450s


train attack loss increase over inner max: -1.496
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.494
total grad norm: 70.646

==================== evaluation at iteration: 800 ====================
train total loss: 79.342%
train max loss: 1.168%, reg loss: 78.704%
time spent training so far: 15:28:55.034732
train attack total time: 58.984s
train attack init time: 44.201s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.573s


train attack loss increase over inner max: 1.851
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.040% of volume
percentage infeasible at boundary: 0.04%
mean, std amount infeasible at boundary: 0.00 +/- 0.00
max amount infeasible at boundary: 0.00

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 5.233
total grad norm: 5.233

==================== evaluation at iteration: 801 ====================
train total loss: 78.597%
train max loss: -0.205%, reg loss: 78.597%
time spent training so far: 16:22:34.604159
train attack total time: 81.512s
train attack init time: 67.536s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.539s


train attack loss increase over inner max: 1.524
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.110
total grad norm: 66.628

==================== evaluation at iteration: 802 ====================
train total loss: 78.659%
train max loss: 0.103%, reg loss: 78.555%
time spent training so far: 16:24:01.770869
train attack total time: 86.684s
train attack init time: 66.967s
train attack avg grad step time: 0.088s
train attack avg reproj time: 0.805s


train attack loss increase over inner max: 0.797
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.444
total grad norm: 5.444

==================== evaluation at iteration: 803 ====================
train total loss: 78.752%
train max loss: -0.240%, reg loss: 78.752%
time spent training so far: 16:25:06.681965
train attack total time: 64.443s
train attack init time: 49.681s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.573s


train attack loss increase over inner max: 1.320
OOM debug. Mem allocated and reserved: 1092096.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.698
total grad norm: 64.965

==================== evaluation at iteration: 804 ====================
train total loss: 79.298%
train max loss: 0.427%, reg loss: 78.871%
time spent training so far: 16:26:28.279687
train attack total time: 81.143s
train attack init time: 55.907s
train attack avg grad step time: 0.093s
train attack avg reproj time: 1.051s


train attack loss increase over inner max: 1.296
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.843
total grad norm: 4.843

==================== evaluation at iteration: 805 ====================
train total loss: 78.425%
train max loss: -0.012%, reg loss: 78.425%
time spent training so far: 16:27:38.739789
train attack total time: 69.998s
train attack init time: 55.998s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.543s


train attack loss increase over inner max: 1.106
OOM debug. Mem allocated and reserved: 1092096.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.580
total grad norm: 46.125

==================== evaluation at iteration: 806 ====================
train total loss: 78.840%
train max loss: 0.193%, reg loss: 78.647%
time spent training so far: 16:28:31.461929
train attack total time: 52.135s
train attack init time: 39.127s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.493s


train attack loss increase over inner max: 0.545
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.716
total grad norm: 58.137

==================== evaluation at iteration: 807 ====================
train total loss: 80.151%
train max loss: 1.821%, reg loss: 78.657%
time spent training so far: 16:29:28.065622
train attack total time: 56.131s
train attack init time: 43.965s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.459s


train attack loss increase over inner max: 2.215
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.859
total grad norm: 73.643

==================== evaluation at iteration: 808 ====================
train total loss: 82.172%
train max loss: 6.563%, reg loss: 78.325%
time spent training so far: 16:30:20.408982
train attack total time: 51.826s
train attack init time: 38.249s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.519s


train attack loss increase over inner max: 5.294
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.757
total grad norm: 44.966

==================== evaluation at iteration: 809 ====================
train total loss: 82.812%
train max loss: 11.633%, reg loss: 78.367%
time spent training so far: 16:30:58.863975
train attack total time: 37.964s
train attack init time: 29.337s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.293s


train attack loss increase over inner max: 7.935
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.248
total grad norm: 41.189

==================== evaluation at iteration: 810 ====================
train total loss: 84.829%
train max loss: 10.470%, reg loss: 78.426%
time spent training so far: 16:31:32.533034
train attack total time: 33.197s
train attack init time: 25.859s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.237s


train attack loss increase over inner max: 1.031
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000

Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2060]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2073]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2084]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2095]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2104]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2112]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2119]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2126]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2131]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2135]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2139]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2141]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2142]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2142]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2143]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2142]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2142]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2142]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2143]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2142]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2142]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2140]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2139]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2138]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2135]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2132]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2132]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2133]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2134]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2134]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2135]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2134]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2133]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2132]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2130]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2128]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2127]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2125]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2124]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2123]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2121]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2120]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2120]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2120]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2121]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)============================ end of evaluation ============================

Reg grad norm: 5.560
total grad norm: 38.983

==================== evaluation at iteration: 811 ====================
train total loss: 83.771%
train max loss: 10.663%, reg loss: 78.584%
time spent training so far: 16:32:05.186919
train attack total time: 32.018s
train attack init time: 21.384s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.384s


train attack loss increase over inner max: 0.445
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.621
total grad norm: 27.837

==================== evaluation at iteration: 812 ====================
train total loss: 83.754%
train max loss: 11.188%, reg loss: 78.635%
time spent training so far: 16:32:30.441990
train attack total time: 24.812s
train attack init time: 17.422s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.241s


train attack loss increase over inner max: 0.802
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.391
total grad norm: 52.343

==================== evaluation at iteration: 813 ====================
train total loss: 86.120%
train max loss: 14.315%, reg loss: 78.537%
time spent training so far: 16:33:01.985453
train attack total time: 31.050s
train attack init time: 23.074s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.266s


train attack loss increase over inner max: 4.719
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.953
total grad norm: 40.528

==================== evaluation at iteration: 814 ====================
train total loss: 82.845%
train max loss: 10.722%, reg loss: 78.415%
time spent training so far: 16:33:35.422941
train attack total time: 32.979s
train attack init time: 23.047s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.354s


train attack loss increase over inner max: -2.109
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.831
total grad norm: 35.928

==================== evaluation at iteration: 815 ====================
train total loss: 83.614%
train max loss: 11.743%, reg loss: 78.436%
time spent training so far: 16:34:10.127267
train attack total time: 34.254s
train attack init time: 26.149s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.267s


train attack loss increase over inner max: 3.811
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.417
total grad norm: 39.743

==================== evaluation at iteration: 816 ====================
train total loss: 82.911%
train max loss: 6.358%, reg loss: 78.606%
time spent training so far: 16:34:56.496507
train attack total time: 45.716s
train attack init time: 32.575s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.501s


train attack loss increase over inner max: -3.015
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.451
total grad norm: 41.173

==================== evaluation at iteration: 817 ====================
train total loss: 81.164%
train max loss: 5.447%, reg loss: 78.451%
time spent training so far: 16:35:46.614560
train attack total time: 49.666s
train attack init time: 38.908s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.393s


train attack loss increase over inner max: 1.002
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.287
total grad norm: 61.066

==================== evaluation at iteration: 818 ====================
train total loss: 79.497%
train max loss: 0.913%, reg loss: 78.584%
time spent training so far: 16:36:46.794502
train attack total time: 59.661s
train attack init time: 47.318s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.467s


train attack loss increase over inner max: -3.143
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.194
total grad norm: 46.561

==================== evaluation at iteration: 819 ====================
train total loss: 79.245%
train max loss: 1.078%, reg loss: 78.520%
time spent training so far: 16:37:55.398985
train attack total time: 68.060s
train attack init time: 56.653s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.420s


train attack loss increase over inner max: 2.168
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.164
total grad norm: 40.304

==================== evaluation at iteration: 820 ====================
train total loss: 78.790%
train max loss: 0.347%, reg loss: 78.639%
time spent training so far: 16:39:06.049365
train attack total time: 70.179s
train attack init time: 53.365s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.667s


train attack loss increase over inner max: 0.924
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.894
total grad norm: 60.766

==================== evaluation at iteration: 821 ====================
train total loss: 78.576%
train max loss: 0.235%, reg loss: 78.341%
time spent training so far: 16:40:14.292586
train attack total time: 67.615s
train attack init time: 55.188s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.467s


train attack loss increase over inner max: 1.342
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.113
total grad norm: 44.524

==================== evaluation at iteration: 822 ====================
train total loss: 78.863%
train max loss: 0.633%, reg loss: 78.429%
time spent training so far: 16:41:29.063347
train attack total time: 74.285s
train attack init time: 63.871s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.376s


train attack loss increase over inner max: 1.526
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.823
total grad norm: 4.823

==================== evaluation at iteration: 823 ====================
train total loss: 78.477%
train max loss: -0.049%, reg loss: 78.477%
time spent training so far: 16:42:49.618009
train attack total time: 80.079s
train attack init time: 50.943s
train attack avg grad step time: 0.095s
train attack avg reproj time: 1.226s


train attack loss increase over inner max: 0.178
OOM debug. Mem allocated and reserved: 1092096.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.152
total grad norm: 60.252

==================== evaluation at iteration: 824 ====================
train total loss: 79.186%
train max loss: 0.639%, reg loss: 78.547%
time spent training so far: 16:44:06.139357
train attack total time: 76.037s
train attack init time: 64.518s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.429s


train attack loss increase over inner max: 0.685
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.546
total grad norm: 58.239

==================== evaluation at iteration: 825 ====================
train total loss: 78.919%
train max loss: 0.187%, reg loss: 78.732%
time spent training so far: 16:45:13.353095
train attack total time: 66.717s
train attack init time: 56.421s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: -0.326
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.040% of volume
percentage infeasible at boundary: 0.20%
mean, std amount infeasible at boundary: 0.00 +/- 0.04
max amount infeasible at boundary: 1.24

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 4.880
total grad norm: 32.050

==================== evaluation at iteration: 826 ====================
train total loss: 79.338%
train max loss: 0.931%, reg loss: 78.415%
time spent training so far: 17:26:52.364538
train attack total time: 63.561s
train attack init time: 51.585s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.449s


train attack loss increase over inner max: 0.778
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.469
total grad norm: 57.044

==================== evaluation at iteration: 827 ====================
train total loss: 83.173%
train max loss: 9.346%, reg loss: 78.715%
time spent training so far: 17:27:43.221763
train attack total time: 50.329s
train attack init time: 40.698s
train attack avg grad step time: 0.098s
train attack avg reproj time: 0.337s


train attack loss increase over inner max: 8.265
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.497
total grad norm: 50.888

==================== evaluation at iteration: 828 ====================
train total loss: 80.266%
train max loss: 2.779%, reg loss: 78.671%
time spent training so far: 17:28:30.788588
train attack total time: 47.123s
train attack init time: 35.067s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.450s


train attack loss increase over inner max: -4.203
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 6.135
total grad norm: 44.241

==================== evaluation at iteration: 829 ====================
train total loss: 83.766%
train max loss: 9.204%, reg loss: 78.961%
time spent training so far: 17:29:06.996104
train attack total time: 35.707s
train attack init time: 28.224s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.242s


train attack loss increase over inner max: 6.451
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.401
total grad norm: 50.578

==================== evaluation at iteration: 830 ====================
train total loss: 83.853%
train max loss: 8.684%, reg loss: 78.742%
time spent training so far: 17:29:49.938161
train attack total time: 42.461s
train attack init time: 30.646s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.439s


train attack loss increase over inner max: -0.235
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.539
total grad norm: 46.158

==================== evaluation at iteration: 831 ====================
train total loss: 83.981%
train max loss: 7.630%, reg loss: 78.780%
time spent training so far: 17:30:29.976366
train attack total time: 39.432s
train attack init time: 30.522s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.306s


train attack loss increase over inner max: 0.428
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.252
total grad norm: 39.772

==================== evaluation at iteration: 832 ====================
train total loss: 82.311%
train max loss: 6.659%, reg loss: 78.541%
time spent training so far: 17:31:10.714312
train attack total time: 40.203s
train attack init time: 31.680s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.290s


train attack loss increase over inner max: 0.724
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.474
total grad norm: 74.034

==================== evaluation at iteration: 833 ====================
train total loss: 82.411%
train max loss: 4.973%, reg loss: 78.737%
time spent training so far: 17:31:58.448708
train attack total time: 47.245s
train attack init time: 33.376s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.533s


train attack loss increase over inner max: 0.035
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.242
total grad norm: 67.095

==================== evaluation at iteration: 834 ====================
train total loss: 79.561%
train max loss: 1.346%, reg loss: 78.727%
time spent training so far: 17:32:42.529533
train attack total time: 43.576s
train attack init time: 30.605s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.494s


train attack loss increase over inner max: -1.621
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.037
total grad norm: 5.037

==================== evaluation at iteration: 835 ====================
train total loss: 78.678%
train max loss: -0.666%, reg loss: 78.678%
time spent training so far: 17:33:50.935570
train attack total time: 67.936s
train attack init time: 50.653s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.687s


train attack loss increase over inner max: -0.216
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.045
total grad norm: 35.270

==================== evaluation at iteration: 836 ====================
train total loss: 79.273%
train max loss: 0.762%, reg loss: 78.678%
time spent training so far: 17:34:53.152084
train attack total time: 61.609s
train attack init time: 50.752s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.400s


train attack loss increase over inner max: 0.223
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.298
total grad norm: 58.562

==================== evaluation at iteration: 837 ====================
train total loss: 80.778%
train max loss: 2.044%, reg loss: 78.892%
time spent training so far: 17:35:54.237479
train attack total time: 60.593s
train attack init time: 43.265s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.695s


train attack loss increase over inner max: 0.872
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.088
total grad norm: 11.333

==================== evaluation at iteration: 838 ====================
train total loss: 79.827%
train max loss: 1.027%, reg loss: 78.800%
time spent training so far: 17:37:17.581120
train attack total time: 82.885s
train attack init time: 66.935s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.629s


train attack loss increase over inner max: -0.909
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.712
total grad norm: 5.712

==================== evaluation at iteration: 839 ====================
train total loss: 78.918%
train max loss: -0.001%, reg loss: 78.918%
time spent training so far: 17:38:47.739718
train attack total time: 89.682s
train attack init time: 76.294s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.512s


train attack loss increase over inner max: -0.351
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.400
total grad norm: 20.441

==================== evaluation at iteration: 840 ====================
train total loss: 79.436%
train max loss: 0.691%, reg loss: 78.947%
time spent training so far: 17:39:54.607671
train attack total time: 66.382s
train attack init time: 56.933s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.336s


train attack loss increase over inner max: 1.068
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.478
total grad norm: 12.270

==================== evaluation at iteration: 841 ====================
train total loss: 79.383%
train max loss: 0.432%, reg loss: 78.951%
time spent training so far: 17:41:10.896086
train attack total time: 75.664s
train attack init time: 64.304s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.421s


train attack loss increase over inner max: -0.542
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.765
total grad norm: 4.765

==================== evaluation at iteration: 842 ====================
train total loss: 78.613%
train max loss: -0.266%, reg loss: 78.613%
time spent training so far: 17:42:34.729215
train attack total time: 83.338s
train attack init time: 69.230s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.540s


train attack loss increase over inner max: -0.219
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.104
total grad norm: 5.104

==================== evaluation at iteration: 843 ====================
train total loss: 78.870%
train max loss: -0.434%, reg loss: 78.870%
time spent training so far: 17:44:00.522741
train attack total time: 85.272s
train attack init time: 70.764s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.560s


train attack loss increase over inner max: -0.329
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.050
total grad norm: 15.831

==================== evaluation at iteration: 844 ====================
train total loss: 79.330%
train max loss: 1.033%, reg loss: 78.705%
time spent training so far: 17:45:17.766365
train attack total time: 76.751s
train attack init time: 64.559s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.460s


train attack loss increase over inner max: 1.471
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.448
total grad norm: 33.958

==================== evaluation at iteration: 845 ====================
train total loss: 79.489%
train max loss: 0.860%, reg loss: 78.860%
time spent training so far: 17:46:31.924732
train attack total time: 73.690s
train attack init time: 62.769s
train attack avg grad step time: 0.099s
train attack avg reproj time: 0.394s


train attack loss increase over inner max: 0.585
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.050
total grad norm: 5.050

==================== evaluation at iteration: 846 ====================
train total loss: 78.573%
train max loss: -0.276%, reg loss: 78.573%
time spent training so far: 17:47:50.219594
train attack total time: 77.671s
train attack init time: 64.577s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.500s


train attack loss increase over inner max: -1.020
OOM debug. Mem allocated and reserved: 1092096.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.897
total grad norm: 5.897

==================== evaluation at iteration: 847 ====================
train total loss: 79.034%
train max loss: -1.730%, reg loss: 79.034%
time spent training so far: 17:49:12.605230
train attack total time: 81.885s
train attack init time: 58.218s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.980s


train attack loss increase over inner max: -1.633
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.312
total grad norm: 5.312

==================== evaluation at iteration: 848 ====================
train total loss: 78.763%
train max loss: -0.723%, reg loss: 78.763%
time spent training so far: 17:50:34.898807
train attack total time: 81.805s
train attack init time: 66.207s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.614s


train attack loss increase over inner max: -0.736
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.789
total grad norm: 4.789

==================== evaluation at iteration: 849 ====================
train total loss: 78.495%
train max loss: -0.153%, reg loss: 78.495%
time spent training so far: 17:51:37.193008
train attack total time: 61.743s
train attack init time: 48.769s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.495s


train attack loss increase over inner max: 0.130
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.926
total grad norm: 4.926

==================== evaluation at iteration: 850 ====================
train total loss: 78.595%
train max loss: -1.370%, reg loss: 78.595%
time spent training so far: 17:53:13.171248
train attack total time: 95.496s
train attack init time: 68.832s
train attack avg grad step time: 0.095s
train attack avg reproj time: 1.114s


train attack loss increase over inner max: -1.298
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.16%
mean, std amount infeasible at boundary: 0.00 +/- 0.05
max amount infeasible at boundary: 2.63

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 5.498
total grad norm: 64.128

==================== evaluation at iteration: 851 ====================
train total loss: 78.780%
train max loss: 0.088%, reg loss: 78.692%
time spent training so far: 18:44:28.747149
train attack total time: 60.985s
train attack init time: 48.972s
train attack avg grad step time: 0.087s
train attack avg reproj time: 0.455s


train attack loss increase over inner max: 1.432
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.698
total grad norm: 47.992

==================== evaluation at iteration: 852 ====================
train total loss: 79.674%
train max loss: 1.439%, reg loss: 78.762%
time spent training so far: 18:45:39.138266
train attack total time: 69.905s
train attack init time: 55.809s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.545s


train attack loss increase over inner max: 0.888
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.379
total grad norm: 94.414

==================== evaluation at iteration: 853 ====================
train total loss: 79.144%
train max loss: 0.514%, reg loss: 78.630%
time spent training so far: 18:46:43.606154
train attack total time: 63.959s
train attack init time: 49.608s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.559s


train attack loss increase over inner max: -0.288
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.091
total grad norm: 67.792

==================== evaluation at iteration: 854 ====================
train total loss: 80.925%
train max loss: 6.040%, reg loss: 78.490%
time spent training so far: 18:48:00.261020
train attack total time: 76.141s
train attack init time: 66.023s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.364s


train attack loss increase over inner max: 6.395
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.313
total grad norm: 18.608

==================== evaluation at iteration: 855 ====================
train total loss: 79.034%
train max loss: 0.881%, reg loss: 78.551%
time spent training so far: 18:49:07.030668
train attack total time: 66.278s
train attack init time: 54.000s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.463s


train attack loss increase over inner max: -1.146
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================


Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2123]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2125]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2127]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2129]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2130]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2131]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2130]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2129]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2127]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2126]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2124]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2122]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2120]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2118]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2116]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2114]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2113]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2112]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2112]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2112]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2113]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2114]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2115]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2117]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2119]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2120]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2120]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2119]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2119]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2119]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2118]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2118]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2118]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2118]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2117]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2117]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2117]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2117]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2118]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2119]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pkl
Max_n_steps: 20
Parameter containing:
tensor([[0.2120]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2122]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2126]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2131]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Max_n_steps: 20
Parameter containing:
tensor([[0.2135]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[0.]], device='cuda:0', requires_grad=True)
Saving at:  log/flying_inv_pend_repro_after_round1_edits/data.pklReg grad norm: 4.693
total grad norm: 44.505

==================== evaluation at iteration: 856 ====================
train total loss: 82.225%
train max loss: 7.402%, reg loss: 78.256%
time spent training so far: 18:50:00.120700
train attack total time: 52.452s
train attack init time: 42.469s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.357s


train attack loss increase over inner max: 7.079
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.168
total grad norm: 52.262

==================== evaluation at iteration: 857 ====================
train total loss: 81.231%
train max loss: 5.622%, reg loss: 78.446%
time spent training so far: 18:50:51.954199
train attack total time: 51.369s
train attack init time: 43.188s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.276s


train attack loss increase over inner max: 0.424
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.734
total grad norm: 38.888

==================== evaluation at iteration: 858 ====================
train total loss: 83.318%
train max loss: 7.277%, reg loss: 78.654%
time spent training so far: 18:51:31.115414
train attack total time: 38.663s
train attack init time: 31.947s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.207s


train attack loss increase over inner max: 2.852
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.333
total grad norm: 26.031

==================== evaluation at iteration: 859 ====================
train total loss: 83.338%
train max loss: 11.955%, reg loss: 78.445%
time spent training so far: 18:52:15.445171
train attack total time: 43.826s
train attack init time: 29.217s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.570s


train attack loss increase over inner max: 4.635
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.828
total grad norm: 29.848

==================== evaluation at iteration: 860 ====================
train total loss: 84.290%
train max loss: 12.557%, reg loss: 78.245%
time spent training so far: 18:52:52.830358
train attack total time: 36.879s
train attack init time: 29.861s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.223s


train attack loss increase over inner max: 2.337
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.563
total grad norm: 27.255

==================== evaluation at iteration: 861 ====================
train total loss: 83.093%
train max loss: 11.873%, reg loss: 78.515%
time spent training so far: 18:53:24.984797
train attack total time: 31.539s
train attack init time: 22.147s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.331s


train attack loss increase over inner max: -1.735
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.314
total grad norm: 34.908

==================== evaluation at iteration: 862 ====================
train total loss: 82.834%
train max loss: 11.180%, reg loss: 78.310%
time spent training so far: 18:53:51.637501
train attack total time: 26.186s
train attack init time: 19.410s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.215s


train attack loss increase over inner max: -1.537
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.907
total grad norm: 34.493

==================== evaluation at iteration: 863 ====================
train total loss: 83.322%
train max loss: 10.772%, reg loss: 78.259%
time spent training so far: 18:54:34.831276
train attack total time: 42.626s
train attack init time: 33.749s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.310s


train attack loss increase over inner max: 1.807
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.395
total grad norm: 52.301

==================== evaluation at iteration: 864 ====================
train total loss: 83.136%
train max loss: 7.002%, reg loss: 78.504%
time spent training so far: 18:55:21.912447
train attack total time: 46.579s
train attack init time: 37.272s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.327s


train attack loss increase over inner max: -1.109
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.297
total grad norm: 50.165

==================== evaluation at iteration: 865 ====================
train total loss: 79.312%
train max loss: 0.859%, reg loss: 78.453%
time spent training so far: 18:56:11.488921
train attack total time: 49.061s
train attack init time: 32.669s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.645s


train attack loss increase over inner max: -1.936
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.726
total grad norm: 5.726

==================== evaluation at iteration: 866 ====================
train total loss: 78.613%
train max loss: -0.447%, reg loss: 78.613%
time spent training so far: 18:57:19.692699
train attack total time: 67.596s
train attack init time: 54.428s
train attack avg grad step time: 0.089s
train attack avg reproj time: 0.507s


train attack loss increase over inner max: -0.998
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.538
total grad norm: 34.097

==================== evaluation at iteration: 867 ====================
train total loss: 78.961%
train max loss: 0.344%, reg loss: 78.617%
time spent training so far: 18:58:25.823116
train attack total time: 65.618s
train attack init time: 54.227s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.420s


train attack loss increase over inner max: 1.053
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.801
total grad norm: 26.028

==================== evaluation at iteration: 868 ====================
train total loss: 78.864%
train max loss: 0.892%, reg loss: 78.305%
time spent training so far: 18:59:20.928729
train attack total time: 54.588s
train attack init time: 46.105s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.289s


train attack loss increase over inner max: 1.696
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.292
total grad norm: 47.446

==================== evaluation at iteration: 869 ====================
train total loss: 80.050%
train max loss: 1.471%, reg loss: 78.578%
time spent training so far: 19:00:33.272985
train attack total time: 71.783s
train attack init time: 56.948s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.574s


train attack loss increase over inner max: 1.108
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.783
total grad norm: 43.371

==================== evaluation at iteration: 870 ====================
train total loss: 79.984%
train max loss: 1.673%, reg loss: 78.311%
time spent training so far: 19:01:34.081320
train attack total time: 60.294s
train attack init time: 48.425s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.443s


train attack loss increase over inner max: 0.240
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.271
total grad norm: 24.495

==================== evaluation at iteration: 871 ====================
train total loss: 78.610%
train max loss: 0.280%, reg loss: 78.468%
time spent training so far: 19:02:42.640755
train attack total time: 67.890s
train attack init time: 55.522s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.464s


train attack loss increase over inner max: 0.131
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.724
total grad norm: 4.724

==================== evaluation at iteration: 872 ====================
train total loss: 78.231%
train max loss: -0.052%, reg loss: 78.231%
time spent training so far: 19:03:58.770872
train attack total time: 75.642s
train attack init time: 64.812s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.396s


train attack loss increase over inner max: -1.319
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.082
total grad norm: 5.082

==================== evaluation at iteration: 873 ====================
train total loss: 78.388%
train max loss: -0.757%, reg loss: 78.388%
time spent training so far: 19:04:50.797978
train attack total time: 51.575s
train attack init time: 39.514s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.452s


train attack loss increase over inner max: -0.910
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.316
total grad norm: 5.316

==================== evaluation at iteration: 874 ====================
train total loss: 78.431%
train max loss: -0.238%, reg loss: 78.431%
time spent training so far: 19:05:48.795862
train attack total time: 57.501s
train attack init time: 45.054s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.470s


train attack loss increase over inner max: 0.419
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.828
total grad norm: 4.828

==================== evaluation at iteration: 875 ====================
train total loss: 78.249%
train max loss: -0.292%, reg loss: 78.249%
time spent training so far: 19:06:54.552406
train attack total time: 65.250s
train attack init time: 53.800s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.425s


train attack loss increase over inner max: -0.309
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================


++++++++++++++++++++ computing test stats ++++++++++++++++++++
v approx: 0.000% of volume
percentage infeasible at boundary: 0.12%
mean, std amount infeasible at boundary: 0.00 +/- 0.02
max amount infeasible at boundary: 0.78

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Reg grad norm: 5.312
total grad norm: 48.096

==================== evaluation at iteration: 876 ====================
train total loss: 78.910%
train max loss: 0.499%, reg loss: 78.411%
time spent training so far: 19:49:40.718636
train attack total time: 55.030s
train attack init time: 44.822s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.366s


train attack loss increase over inner max: 0.171
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.033
total grad norm: 31.855

==================== evaluation at iteration: 877 ====================
train total loss: 79.174%
train max loss: 1.392%, reg loss: 78.297%
time spent training so far: 19:50:37.947213
train attack total time: 56.760s
train attack init time: 48.724s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.272s


train attack loss increase over inner max: 2.719
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.050
total grad norm: 46.112

==================== evaluation at iteration: 878 ====================
train total loss: 79.144%
train max loss: 1.242%, reg loss: 78.213%
time spent training so far: 19:51:40.467418
train attack total time: 62.041s
train attack init time: 44.626s
train attack avg grad step time: 0.091s
train attack avg reproj time: 0.697s


train attack loss increase over inner max: 0.288
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.806
total grad norm: 67.845

==================== evaluation at iteration: 879 ====================
train total loss: 78.280%
train max loss: 0.194%, reg loss: 78.086%
time spent training so far: 19:52:38.471468
train attack total time: 57.485s
train attack init time: 47.314s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.365s


train attack loss increase over inner max: 0.147
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.783
total grad norm: 31.641

==================== evaluation at iteration: 880 ====================
train total loss: 79.230%
train max loss: 1.557%, reg loss: 78.115%
time spent training so far: 19:53:26.317522
train attack total time: 47.379s
train attack init time: 35.074s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.461s


train attack loss increase over inner max: 2.405
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.517
total grad norm: 28.633

==================== evaluation at iteration: 881 ====================
train total loss: 79.486%
train max loss: 1.989%, reg loss: 78.335%
time spent training so far: 19:54:15.651786
train attack total time: 48.637s
train attack init time: 39.508s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.318s


train attack loss increase over inner max: 0.290
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.873
total grad norm: 20.343

==================== evaluation at iteration: 882 ====================
train total loss: 79.932%
train max loss: 3.183%, reg loss: 78.066%
time spent training so far: 19:54:57.513223
train attack total time: 41.386s
train attack init time: 31.166s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.368s


train attack loss increase over inner max: 1.343
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.845
total grad norm: 30.793

==================== evaluation at iteration: 883 ====================
train total loss: 81.910%
train max loss: 9.013%, reg loss: 78.109%
time spent training so far: 19:55:37.401869
train attack total time: 39.429s
train attack init time: 31.095s
train attack avg grad step time: 0.097s
train attack avg reproj time: 0.279s


train attack loss increase over inner max: 6.079
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.980
total grad norm: 41.421

==================== evaluation at iteration: 884 ====================
train total loss: 83.295%
train max loss: 10.703%, reg loss: 78.030%
time spent training so far: 19:56:09.943240
train attack total time: 31.986s
train attack init time: 23.864s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.271s


train attack loss increase over inner max: 1.039
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.151
total grad norm: 29.496

==================== evaluation at iteration: 885 ====================
train total loss: 83.680%
train max loss: 10.644%, reg loss: 78.190%
time spent training so far: 19:56:37.054490
train attack total time: 26.572s
train attack init time: 18.611s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.265s


train attack loss increase over inner max: -0.700
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.565
total grad norm: 33.021

==================== evaluation at iteration: 886 ====================
train total loss: 83.677%
train max loss: 10.610%, reg loss: 78.217%
time spent training so far: 19:57:03.826922
train attack total time: 26.148s
train attack init time: 16.793s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.330s


train attack loss increase over inner max: 1.910
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.543
total grad norm: 35.709

==================== evaluation at iteration: 887 ====================
train total loss: 85.008%
train max loss: 13.650%, reg loss: 78.321%
time spent training so far: 19:57:35.145171
train attack total time: 30.862s
train attack init time: 22.884s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.264s


train attack loss increase over inner max: 5.657
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.789
total grad norm: 32.782

==================== evaluation at iteration: 888 ====================
train total loss: 82.503%
train max loss: 13.055%, reg loss: 77.993%
time spent training so far: 19:58:04.694430
train attack total time: 29.089s
train attack init time: 22.428s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.206s


train attack loss increase over inner max: 1.372
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.668
total grad norm: 49.658

==================== evaluation at iteration: 889 ====================
train total loss: 83.765%
train max loss: 11.162%, reg loss: 77.917%
time spent training so far: 19:58:39.018408
train attack total time: 33.823s
train attack init time: 27.439s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.193s


train attack loss increase over inner max: 0.824
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.891
total grad norm: 33.803

==================== evaluation at iteration: 890 ====================
train total loss: 83.046%
train max loss: 11.677%, reg loss: 78.066%
time spent training so far: 19:59:25.183507
train attack total time: 45.667s
train attack init time: 35.413s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.373s


train attack loss increase over inner max: 3.242
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.091
total grad norm: 64.440

==================== evaluation at iteration: 891 ====================
train total loss: 86.724%
train max loss: 8.673%, reg loss: 78.052%
time spent training so far: 20:00:17.418062
train attack total time: 51.615s
train attack init time: 35.211s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.649s


train attack loss increase over inner max: -2.072
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.321
total grad norm: 73.610

==================== evaluation at iteration: 892 ====================
train total loss: 78.586%
train max loss: 0.437%, reg loss: 78.149%
time spent training so far: 20:01:15.699607
train attack total time: 57.790s
train attack init time: 44.885s
train attack avg grad step time: 0.094s
train attack avg reproj time: 0.489s


train attack loss increase over inner max: -4.892
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.289
total grad norm: 57.412

==================== evaluation at iteration: 893 ====================
train total loss: 78.163%
train max loss: 0.039%, reg loss: 78.124%
time spent training so far: 20:02:03.973598
train attack total time: 47.764s
train attack init time: 37.536s
train attack avg grad step time: 0.096s
train attack avg reproj time: 0.366s


train attack loss increase over inner max: 1.470
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.115
total grad norm: 48.050

==================== evaluation at iteration: 894 ====================
train total loss: 79.149%
train max loss: 1.960%, reg loss: 78.295%
time spent training so far: 20:02:53.987225
train attack total time: 49.538s
train attack init time: 39.043s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.381s


train attack loss increase over inner max: 2.291
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.061
total grad norm: 105.158

==================== evaluation at iteration: 895 ====================
train total loss: 79.448%
train max loss: 1.185%, reg loss: 78.263%
time spent training so far: 20:03:53.619922
train attack total time: 59.143s
train attack init time: 44.686s
train attack avg grad step time: 0.090s
train attack avg reproj time: 0.563s


train attack loss increase over inner max: 2.429
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.818
total grad norm: 4.818

==================== evaluation at iteration: 896 ====================
train total loss: 78.187%
train max loss: -0.056%, reg loss: 78.187%
time spent training so far: 20:04:52.699693
train attack total time: 58.455s
train attack init time: 39.316s
train attack avg grad step time: 0.095s
train attack avg reproj time: 0.771s


train attack loss increase over inner max: 4.377
OOM debug. Mem allocated and reserved: 1091584.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.985
total grad norm: 24.949

==================== evaluation at iteration: 897 ====================
train total loss: 79.065%
train max loss: 1.949%, reg loss: 78.146%
time spent training so far: 20:05:54.771609
train attack total time: 61.584s
train attack init time: 52.264s
train attack avg grad step time: 0.098s
train attack avg reproj time: 0.322s


train attack loss increase over inner max: 2.186
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 4.899
total grad norm: 50.601

==================== evaluation at iteration: 898 ====================
train total loss: 79.409%
train max loss: 2.286%, reg loss: 78.240%
time spent training so far: 20:06:45.328219
train attack total time: 50.095s
train attack init time: 41.692s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.286s


train attack loss increase over inner max: 1.642
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.226
total grad norm: 69.820

==================== evaluation at iteration: 899 ====================
train total loss: 79.060%
train max loss: 1.260%, reg loss: 78.346%
time spent training so far: 20:07:40.914638
train attack total time: 55.074s
train attack init time: 43.562s
train attack avg grad step time: 0.093s
train attack avg reproj time: 0.426s


train attack loss increase over inner max: -2.123
OOM debug. Mem allocated and reserved: 1093632.000000, 33554432.000000
============================ end of evaluation ============================

Reg grad norm: 5.311
total grad norm: 20.645

==================== evaluation at iteration: 900 ====================
train total loss: 79.337%
train max loss: 1.038%, reg loss: 78.393%
time spent training so far: 20:08:37.468496
train attack total time: 56.044s
train attack init time: 42.254s
train attack avg grad step time: 0.092s
train attack avg reproj time: 0.531s


train attack loss increase over inner max: 0.116
OOM debug. Mem allocated and reserved: 1093120.000000, 33554432.000000
============================ end of evaluation ============================

